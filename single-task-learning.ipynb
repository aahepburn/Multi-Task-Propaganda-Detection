{
    "cells": [
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "b618418e",
                "execution_start": 1743798246507,
                "execution_millis": 0,
                "execution_context_id": "c91e604b-a20e-43ce-9ac0-2bc6123e3d8b",
                "cell_id": "45b39c51560c4df5aef0095d8d79b60a",
                "deepnote_cell_type": "code"
            },
            "source": "import os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DebertaV2Tokenizer, AutoModel, RobertaTokenizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score, classification_report\nimport numpy as np\nimport shap\nfrom captum.attr import IntegratedGradients\nfrom transformers import AutoTokenizer\nimport torch.nn.functional as F\nimport hf_xet",
            "block_group": "05320c4fb99b445dba66cf205049fdf9",
            "execution_count": 46,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "093172d7275a4670884b0b95732e3bf2",
                "deepnote_cell_type": "markdown"
            },
            "source": "# Single-Task Learning\n\nBelow is the current pipeline for single-task learning. The code chunk bellow allows to switch between the two tasks: \"narrative_classification\" and \"entity_framing\".\n\nWe can also specify the training and test domains.",
            "block_group": "b415a08f22c9471c815c4e27d7d8e2be"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "75291c4c",
                "execution_start": 1743798617576,
                "execution_millis": 0,
                "execution_context_id": "c91e604b-a20e-43ce-9ac0-2bc6123e3d8b",
                "cell_id": "7324c81338a4473f88788141bd0b3060",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# CONTROL PANEL\n# ==========================\n\n# choose a task for the pipeline below: \"narrative_classification\" or \"entity_framing\"\nTASK = \"narrative_classification\"\n\n# select domains for training and testing: \"UA\"; \"CC\"; \"UA\", \"CC\";\nTRAIN_DOMAIN = [\"CC\"]\nTEST_DOMAIN = [\"UA\", \"CC\"] # The test data comes from a separate dataset. \n# The test data is always the same regardless of the domain we choose to train on. This is for consistency.\n\n\"\"\"\nNote that all articles are now in English, but if we wanted to control for e.g. certain cultural variations of a specific language,\nwe could exclude articles that were originally written in that language.\n\nNot to use the functionality, 'ALL' should be selected.\n\n\"\"\"\n# select languages for training and testing: \"ALL\";\"EN\";\"HI\";\"BG\";\"RU\";\"PT\"\nTRAIN_LANGUAGES = [\"ALL\"] \nTEST_LANGUAGES = [\"ALL\"]\n\n# debug mode -- reduced samples\nDEBUG_MODE = False\n\n# change the training hyperparameters here\nMODEL_NAME = \"roberta-base\" # OR \"deberta-v3-base\"\nMAX_LEN = 512\nBATCH_SIZE = 8\nEPOCHS = 3\nLEARNING_RATE = 2e-5\nMODEL_PATH = f\"{TASK}_STL_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\" # -- to save the model later\n",
            "block_group": "dffc3ccb17bc445e96e81bb3c0e11639",
            "execution_count": 60,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "cell_id": "5670c41a353649f486be16f4968fc6ce",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# LOAD AND MERGE DATA\n# ==========================\n\narticles = pd.read_csv(\"train-all-articles.csv\")\ns1 = pd.read_csv(\"train-S1-labels.csv\")\ns2 = pd.read_csv(\"train-S2-labels.csv\")\n\ntest_s1_articles = pd.read_csv(\"test-S1-articles.csv\")\ntest_s1_labels = pd.read_csv(\"test-S1-labels.csv\")\ntest_s2_articles = pd.read_csv(\"test-S2-articles.csv\")\ntest_s2_labels = pd.read_csv(\"test-S2-labels.csv\")\n\n# ==========================\n# STANDARDISE TEST SET COLUMNS\n# ==========================\n\nif TASK == \"entity_framing\":\n    test_s1_labels.rename(columns={\"Translated_Entity\": \"Entity\"}, inplace=True)\nelif TASK == \"narrative_classification\":\n    test_s2_labels.columns = [\"Filename\", \"Narrative\", \"Subnarrative\"]\n\n# ==========================\n# FILTER + SPLIT TRAIN/VAL\n# ==========================\n\n# Filter domains/languages for shared train/val\nfiltered_articles = articles[articles[\"Domain\"].isin(TRAIN_DOMAIN)]\nif \"ALL\" not in TRAIN_LANGUAGES:\n    filtered_articles = filtered_articles[filtered_articles[\"Language\"].isin(TRAIN_LANGUAGES)]\n\n# 80/20 train/val split\nfiltered_articles = filtered_articles.sample(frac=1, random_state=42).reset_index(drop=True)\nsplit_idx = int(0.8 * len(filtered_articles))\ntrain_articles = filtered_articles.iloc[:split_idx].copy()\nval_articles = filtered_articles.iloc[split_idx:].copy()\n\n# debug subsampling if needed -- off by default\nif DEBUG_MODE:\n    train_articles = train_articles.sample(100)\n    val_articles = val_articles.sample(100)\n    test_s1_articles = test_s1_articles.sample(100)\n    test_s2_articles = test_s2_articles.sample(100)\n",
            "block_group": "f1dd3339e778493fa7f5cc455f638f45",
            "execution_count": null,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "5f5ad0f5",
                "execution_start": 1743798619432,
                "execution_millis": 698,
                "execution_context_id": "c91e604b-a20e-43ce-9ac0-2bc6123e3d8b",
                "cell_id": "881a3e6ae539492f91e6a0afac0b946f",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# TASK-SPECIFIC MERGE + PROCESSING\n# ==========================\n\nif TASK == \"narrative_classification\":\n    # Merge articles with S2 labels\n    df_train = pd.merge(train_articles, s2, on=\"Filename\")\n    df_val   = pd.merge(val_articles, s2, on=\"Filename\")\n    df_test  = pd.merge(test_s2_articles, test_s2_labels, on=\"Filename\")\n\n    TEXT_COL = \"Translated_Text\"\n    LABEL_COL = \"Narrative\"\n\n    for df in [df_train, df_val, df_test]:\n        df.dropna(subset=[TEXT_COL, LABEL_COL], inplace=True)\n        df[LABEL_COL] = df[LABEL_COL].apply(\n            lambda x: [s.strip() for s in str(x).split(\";\") if s.strip().lower() != \"nan\"]\n        )\n\n    # Create shared label space from all available narrative data (zero-shot setup)\n    full_set = pd.concat([df_train, df_val, df_test])\n    mlb = MultiLabelBinarizer()\n    mlb.fit(full_set[LABEL_COL])\n\n    y_train = mlb.transform(df_train[LABEL_COL])\n    y_val   = mlb.transform(df_val[LABEL_COL])\n    y_test  = mlb.transform(df_test[LABEL_COL])\n    num_classes = len(mlb.classes_)\n\nelif TASK == \"entity_framing\":\n    # Merge entity labels with articles\n    df_train = pd.merge(s1, train_articles, on=\"Filename\")\n    df_val   = pd.merge(s1, val_articles, on=\"Filename\")\n    df_test  = pd.merge(test_s1_labels, test_s1_articles, on=\"Filename\")\n\n    TEXT_COL = \"Translated_Text\"\n    LABEL_COL = \"Label\"\n\n    def insert_entity_marker(text, start, end):\n        try:\n            start, end = int(start), int(end)\n            return text[:start] + \"[ENTITY]\" + text[start:end] + \"[/ENTITY]\" + text[end:]\n        except:\n            return text\n\n    for df in [df_train, df_val, df_test]:\n        df.dropna(subset=[TEXT_COL, \"Entity\", LABEL_COL, \"Start\", \"End\"], inplace=True)\n        df[\"Start\"] = df[\"Start\"].astype(int)\n        df[\"End\"] = df[\"End\"].astype(int)\n        df[\"Input_Text\"] = df.apply(lambda row: insert_entity_marker(row[TEXT_COL], row[\"Start\"], row[\"End\"]), axis=1)\n        df[LABEL_COL] = df[LABEL_COL].apply(lambda x: [s.strip() for s in str(x).split(\",\") if s.strip().lower() != \"nan\"])\n\n    # For entity framing, create a separate label binarizer\n    mlb = MultiLabelBinarizer()\n    mlb.fit(df_train[LABEL_COL] + df_val[LABEL_COL] + df_test[LABEL_COL])  # full entity label space\n\n    y_train = mlb.transform(df_train[LABEL_COL])\n    y_val   = mlb.transform(df_val[LABEL_COL])\n    y_test  = mlb.transform(df_test[LABEL_COL])\n    num_classes = len(mlb.classes_)\n\nelse:\n    raise ValueError(\"Unknown TASK specified.\")\n",
            "block_group": "b35900c3aeb64991a6d1123c456e6404",
            "execution_count": 62,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "d192a57",
                "execution_start": 1743798620306,
                "execution_millis": 214,
                "execution_context_id": "c91e604b-a20e-43ce-9ac0-2bc6123e3d8b",
                "cell_id": "e6f1d2f4d7a64f2e95aad2b5d0f8dcbe",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# TOKENISATION and DATASET CLASS\n# ==========================\n\ntokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n#tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n\nclass MultiLabelDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float)\n        }\n\ntrain_dataset = MultiLabelDataset(df_train[TEXT_COL].tolist(), y_train, tokenizer, MAX_LEN)\nval_dataset   = MultiLabelDataset(df_val[TEXT_COL].tolist(), y_val, tokenizer, MAX_LEN)\ntest_dataset  = MultiLabelDataset(df_test[TEXT_COL].tolist(), y_test, tokenizer, MAX_LEN)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE)\ntest_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
            "block_group": "e393f42d1f384c04983bf5ead6b2dcfe",
            "execution_count": 64,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "b0ac4adc",
                "execution_start": 1743798620576,
                "execution_millis": 1,
                "execution_context_id": "c91e604b-a20e-43ce-9ac0-2bc6123e3d8b",
                "cell_id": "229e126177bc4ffc8c8198ff3e1e35bc",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# MODEL CLASS\n# ==========================\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, model_name, num_classes):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(0.3)\n        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0]  # CLS token\n        pooled_output = self.dropout(pooled_output)\n        return self.classifier(pooled_output)\n",
            "block_group": "6aaa0cfc62de4dca9e058c7a16dfa66a",
            "execution_count": 65,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "cell_id": "a5a87c4942b84a8c8b53fa5141a210d9",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# TRAINING UTILS\n# ==========================\n\ndef predict_proba(model, loader, device):\n    model.eval()\n    probs = []\n    with torch.no_grad():\n        for batch in loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            outputs = model(input_ids, attention_mask)\n            probs.extend(torch.sigmoid(outputs).cpu().numpy())\n    return np.array(probs)\n\ndef evaluate_threshold_sweep(y_true, y_pred, thresholds=np.arange(0.1, 0.9, 0.05)):\n    best_thresh = 0.5\n    best_f1 = 0\n    results = []\n\n    for thresh in thresholds:\n        y_pred_bin = (y_pred > thresh).astype(int)\n        macro = f1_score(y_true, y_pred_bin, average='macro', zero_division=0)\n        micro = f1_score(y_true, y_pred_bin, average='micro', zero_division=0)\n        exact = (y_pred_bin == y_true).all(axis=1).mean()\n\n        results.append((thresh, macro, micro, exact))\n        if macro > best_f1:\n            best_f1 = macro\n            best_thresh = thresh\n\n    print(\"Threshold sweep results:\")\n    for t, macro, micro, exact in results:\n        print(f\"Thresh {t:.2f} | Macro F1: {macro:.3f} | Micro F1: {micro:.3f} | Exact Match: {exact:.3f}\")\n\n    print(f\"\\n Best threshold = {best_thresh:.2f} with Macro F1 = {best_f1:.3f}\")\n    return best_thresh\n",
            "block_group": "2b4954ddc86847acba53c843cb1eeb6c",
            "execution_count": null,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "9ada25d1",
                "execution_start": 1743798622589,
                "execution_millis": 75538,
                "execution_context_id": "c91e604b-a20e-43ce-9ac0-2bc6123e3d8b",
                "cell_id": "e7a491996a1b403eb5f26dd3c967f3e7",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# TRAINING LOOP\n# ==========================\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = TransformerClassifier(MODEL_NAME, num_classes).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.BCEWithLogitsLoss()\nbest_macro_f1 = 0.0 \n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"\\nEpoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}\")\n\n    # validation\n    val_probs = predict_proba(model, val_loader, device)\n    threshold = evaluate_threshold_sweep(y_val, val_probs)\n    y_val_pred = (val_probs > threshold).astype(int)\n    macro_f1 = f1_score(y_val, y_val_pred, average=\"macro\", zero_division=0)\n    print(f\"Validation Macro F1 (Epoch {epoch+1}): {macro_f1:.4f}\")\n\n    if macro_f1 > best_macro_f1:\n        best_macro_f1 = macro_f1\n        torch.save(model.state_dict(), MODEL_PATH)\n        print(f\"Saved best model (Epoch {epoch+1}) to {MODEL_PATH}\")\n",
            "block_group": "6088e635ebcb4dd69a28c65254265f5a",
            "execution_count": 67,
            "outputs": [
                {
                    "name": "stderr",
                    "text": "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 54/54 [00:18<00:00,  2.88it/s]\n\nEpoch 1: Loss = 0.3425\nThreshold sweep results:\nThresh 0.10 | Macro F1: 0.098 | Micro F1: 0.317 | Exact Match: 0.000\nThresh 0.15 | Macro F1: 0.074 | Micro F1: 0.393 | Exact Match: 0.000\nThresh 0.20 | Macro F1: 0.058 | Micro F1: 0.414 | Exact Match: 0.047\nThresh 0.25 | Macro F1: 0.049 | Micro F1: 0.411 | Exact Match: 0.112\nThresh 0.30 | Macro F1: 0.052 | Micro F1: 0.420 | Exact Match: 0.327\nThresh 0.35 | Macro F1: 0.032 | Micro F1: 0.356 | Exact Match: 0.318\nThresh 0.40 | Macro F1: 0.039 | Micro F1: 0.398 | Exact Match: 0.318\nThresh 0.45 | Macro F1: 0.028 | Micro F1: 0.226 | Exact Match: 0.168\nThresh 0.50 | Macro F1: 0.008 | Micro F1: 0.056 | Exact Match: 0.037\nThresh 0.55 | Macro F1: 0.000 | Micro F1: 0.000 | Exact Match: 0.000\nThresh 0.60 | Macro F1: 0.000 | Micro F1: 0.000 | Exact Match: 0.000\nThresh 0.65 | Macro F1: 0.000 | Micro F1: 0.000 | Exact Match: 0.000\nThresh 0.70 | Macro F1: 0.000 | Micro F1: 0.000 | Exact Match: 0.000\nThresh 0.75 | Macro F1: 0.000 | Micro F1: 0.000 | Exact Match: 0.000\nThresh 0.80 | Macro F1: 0.000 | Micro F1: 0.000 | Exact Match: 0.000\nThresh 0.85 | Macro F1: 0.000 | Micro F1: 0.000 | Exact Match: 0.000\n\n Best threshold = 0.10 with Macro F1 = 0.098\nValidation Macro F1 (Epoch 1): 0.0983\nSaved best model (Epoch 1) to narrative_classification_STL_CC_to_UA-CC.pt\nEpoch 2: 100%|██████████| 54/54 [00:18<00:00,  2.90it/s]\n\nEpoch 2: Loss = 0.2050\nThreshold sweep results:\nThresh 0.10 | Macro F1: 0.107 | Micro F1: 0.358 | Exact Match: 0.000\nThresh 0.15 | Macro F1: 0.131 | Micro F1: 0.431 | Exact Match: 0.000\nThresh 0.20 | Macro F1: 0.128 | Micro F1: 0.485 | Exact Match: 0.243\nThresh 0.25 | Macro F1: 0.118 | Micro F1: 0.508 | Exact Match: 0.308\nThresh 0.30 | Macro F1: 0.120 | Micro F1: 0.536 | Exact Match: 0.346\nThresh 0.35 | Macro F1: 0.082 | Micro F1: 0.505 | Exact Match: 0.393\nThresh 0.40 | Macro F1: 0.068 | Micro F1: 0.496 | Exact Match: 0.393\nThresh 0.45 | Macro F1: 0.065 | Micro F1: 0.478 | Exact Match: 0.374\nThresh 0.50 | Macro F1: 0.063 | Micro F1: 0.456 | Exact Match: 0.336\nThresh 0.55 | Macro F1: 0.050 | Micro F1: 0.386 | Exact Match: 0.290\nThresh 0.60 | Macro F1: 0.038 | Micro F1: 0.319 | Exact Match: 0.243\nThresh 0.65 | Macro F1: 0.033 | Micro F1: 0.287 | Exact Match: 0.234\nThresh 0.70 | Macro F1: 0.030 | Micro F1: 0.253 | Exact Match: 0.196\nThresh 0.75 | Macro F1: 0.022 | Micro F1: 0.170 | Exact Match: 0.140\nThresh 0.80 | Macro F1: 0.004 | Micro F1: 0.023 | Exact Match: 0.019\nThresh 0.85 | Macro F1: 0.000 | Micro F1: 0.000 | Exact Match: 0.000\n\n Best threshold = 0.15 with Macro F1 = 0.131\nValidation Macro F1 (Epoch 2): 0.1311\nSaved best model (Epoch 2) to narrative_classification_STL_CC_to_UA-CC.pt\nEpoch 3: 100%|██████████| 54/54 [00:18<00:00,  2.88it/s]\n\nEpoch 3: Loss = 0.1735\nThreshold sweep results:\nThresh 0.10 | Macro F1: 0.185 | Micro F1: 0.497 | Exact Match: 0.140\nThresh 0.15 | Macro F1: 0.166 | Micro F1: 0.523 | Exact Match: 0.252\nThresh 0.20 | Macro F1: 0.149 | Micro F1: 0.553 | Exact Match: 0.290\nThresh 0.25 | Macro F1: 0.136 | Micro F1: 0.569 | Exact Match: 0.336\nThresh 0.30 | Macro F1: 0.139 | Micro F1: 0.579 | Exact Match: 0.374\nThresh 0.35 | Macro F1: 0.147 | Micro F1: 0.602 | Exact Match: 0.458\nThresh 0.40 | Macro F1: 0.138 | Micro F1: 0.580 | Exact Match: 0.486\nThresh 0.45 | Macro F1: 0.112 | Micro F1: 0.549 | Exact Match: 0.467\nThresh 0.50 | Macro F1: 0.109 | Micro F1: 0.539 | Exact Match: 0.449\nThresh 0.55 | Macro F1: 0.094 | Micro F1: 0.508 | Exact Match: 0.421\nThresh 0.60 | Macro F1: 0.070 | Micro F1: 0.460 | Exact Match: 0.346\nThresh 0.65 | Macro F1: 0.056 | Micro F1: 0.429 | Exact Match: 0.318\nThresh 0.70 | Macro F1: 0.042 | Micro F1: 0.394 | Exact Match: 0.308\nThresh 0.75 | Macro F1: 0.041 | Micro F1: 0.383 | Exact Match: 0.290\nThresh 0.80 | Macro F1: 0.038 | Micro F1: 0.344 | Exact Match: 0.262\nThresh 0.85 | Macro F1: 0.035 | Micro F1: 0.304 | Exact Match: 0.243\n\n Best threshold = 0.10 with Macro F1 = 0.185\nValidation Macro F1 (Epoch 3): 0.1847\nSaved best model (Epoch 3) to narrative_classification_STL_CC_to_UA-CC.pt\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "s3:deepnote-cell-outputs-production/0165e740-b7cb-494b-9e29-aa965f1a02cc",
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "is_code_hidden": false,
                "deepnote_to_be_reexecuted": true,
                "deepnote_app_is_code_hidden": true,
                "cell_id": "8968163ea1b74925be9da1fdbd8afcdf",
                "deepnote_cell_type": "code"
            },
            "source": "torch.cuda.empty_cache()",
            "block_group": "5c86f24778cd445194873f12829216a4",
            "execution_count": null,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "8c1bbf70",
                "execution_start": 1743798744496,
                "execution_millis": 0,
                "execution_context_id": "c91e604b-a20e-43ce-9ac0-2bc6123e3d8b",
                "cell_id": "1fcf5c74c61e49d2ad7c402616547439",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# FIXED THRESHOLD EVALUATION\n# ==========================\n\ndef evaluate(loader, df_source, mlb, label=\"TEST\", threshold=0.25): \n\n    \"\"\"\n    Evaluates a multi-label classification model using a fixed probability threshold.\n\n    Args:\n        loader (DataLoader): A PyTorch DataLoader yielding batches of tokenised input data\n        df_source (pd.DataFrame): Source dataframe containing metadata for each example, including domain info.\n        mlb (MultiLabelBinarizer): The fitted multi-label binarizer used for encoding and decoding labels.\n        label (str, optional): Label for the dataset (e.g., 'TEST', 'VALIDATION'). Used for logging. Defaults to \"TEST\".\n        threshold (float, optional): Probability threshold to convert predicted probabilities into binary labels. Defaults to 0.25.\n\n    Returns:\n        dict: A dictionary containing overall macro F1, micro F1, exact match score, \n              the threshold used, and the list of labels used after filtering.\n              Also prints per-domain breakdowns of these metrics.\n\n    Notes:\n        - Filters out labels that are completely unseen in both predictions and ground truths \n          to avoid skewed metric calculations.\n        - Performs evaluation on the entire dataset as well as broken down by domain.\n    \"\"\"\n    model.eval()\n    y_true, y_pred, domains = [], [], []\n\n    with torch.no_grad():\n        for i, batch in enumerate(tqdm(loader, desc=f\"Evaluating {label}\")):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].cpu().numpy()\n\n            outputs = model(input_ids, attention_mask)\n            probs = torch.sigmoid(outputs).cpu().numpy()\n\n            y_pred.extend(probs)\n            y_true.extend(labels)\n\n            start = i * loader.batch_size\n            end = start + len(labels)\n            domains.extend(df_source[\"Domain\"].iloc[start:end].tolist())\n\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    domains = np.array(domains)\n\n    y_pred_bin = (y_pred > threshold).astype(int)\n\n    # filter columns where y_true or y_pred has no samples (i.e., unseen label)\n    mask = (y_true.sum(axis=0) + y_pred_bin.sum(axis=0)) > 0\n    y_true = y_true[:, mask]\n    y_pred_bin = y_pred_bin[:, mask]\n    filtered_labels = np.array(mlb.classes_)[mask]\n\n    macro = f1_score(y_true, y_pred_bin, average=\"macro\", zero_division=0)\n    micro = f1_score(y_true, y_pred_bin, average=\"micro\", zero_division=0)\n    exact = (y_pred_bin == y_true).all(axis=1).mean()\n\n    print(f\"\\n {label} (Fixed Threshold={threshold:.2f}):\")\n    print(f\"Macro F1: {macro:.3f}\")\n    print(f\"Micro F1: {micro:.3f}\")\n    print(f\"Exact Match: {exact:.3f}\")\n\n    print(\"\\n----------------------------\")\n    print(\"Per-Domain Breakdown\")\n    print(\"----------------------------\")\n    for domain in np.unique(domains):\n        idx = np.where(domains == domain)[0]\n        y_true_d = y_true[idx]\n        y_pred_d = y_pred_bin[idx]\n\n        macro_d = f1_score(y_true_d, y_pred_d, average=\"macro\", zero_division=0)\n        micro_d = f1_score(y_true_d, y_pred_d, average=\"micro\", zero_division=0)\n        exact_d = (y_pred_d == y_true_d).all(axis=1).mean()\n\n        print(f\"\\n Domain: {domain}\")\n        print(f\"Macro F1: {macro_d:.3f}\")\n        print(f\"Micro F1: {micro_d:.3f}\")\n        print(f\"Exact Match: {exact_d:.3f}\")\n\n    return {\n        \"macro\": macro,\n        \"micro\": micro,\n        \"exact\": exact,\n        \"threshold\": threshold,\n        \"labels_used\": filtered_labels.tolist()\n    }\n\n\ndef evaluate_and_compare_fixed_thresh(val_loader, df_val, test_loader, df_test, mlb, threshold=0.25):\n    print(\"\\n=========================\")\n    print(\"Validation (Fixed Threshold)\")\n    print(\"=========================\")\n    val_results = evaluate(val_loader, df_val.reset_index(drop=True), mlb, label=\"VALIDATION\", threshold=threshold)\n\n    print(\"\\n=========================\")\n    print(\"Test (Fixed Threshold)\")\n    print(\"=========================\")\n    test_results = evaluate(test_loader, df_test.reset_index(drop=True), mlb, label=\"TEST\", threshold=threshold)\n\n    print(\"\\n=========================\")\n    print(\"OOD Generalization (Fixed Threshold)\")\n    print(\"=========================\")\n    macro_drop = val_results[\"macro\"] - test_results[\"macro\"]\n    print(f\"Δ Macro F1 (val - test): {macro_drop:.3f}\")\n\n    return {\n        \"val\": val_results,\n        \"test\": test_results,\n        \"ood_gap_macro\": macro_drop\n    }\n\n",
            "block_group": "4534ee0a07354a7b9f2d3059ed80f04c",
            "execution_count": 69,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "4624cbaa",
                "execution_start": 1743798745982,
                "execution_millis": 4207,
                "execution_context_id": "c91e604b-a20e-43ce-9ac0-2bc6123e3d8b",
                "cell_id": "1b851481c1b042be861911bb65ce2451",
                "deepnote_cell_type": "code"
            },
            "source": "results = evaluate_and_compare_fixed_thresh(\n    val_loader, df_val,\n    test_loader, df_test,\n    mlb\n)",
            "block_group": "8da600d15a9a4975941b526e11598804",
            "execution_count": 71,
            "outputs": [
                {
                    "name": "stdout",
                    "text": "\n=========================\nValidation (Fixed Threshold)\n=========================\nEvaluating VALIDATION: 100%|██████████| 14/14 [00:01<00:00,  9.04it/s]\n\n VALIDATION (Fixed Threshold=0.25):\nMacro F1: 0.176\nMicro F1: 0.569\nExact Match: 0.336\n\n----------------------------\nPer-Domain Breakdown\n----------------------------\n\n Domain: CC\nMacro F1: 0.176\nMicro F1: 0.569\nExact Match: 0.336\n\n=========================\nTest (Fixed Threshold)\n=========================\nEvaluating TEST: 100%|██████████| 23/23 [00:02<00:00,  8.74it/s]\n TEST (Fixed Threshold=0.25):\nMacro F1: 0.070\nMicro F1: 0.227\nExact Match: 0.135\n\n----------------------------\nPer-Domain Breakdown\n----------------------------\n\n Domain: CC\nMacro F1: 0.128\nMicro F1: 0.569\nExact Match: 0.329\n\n Domain: UA\nMacro F1: 0.009\nMicro F1: 0.040\nExact Match: 0.000\n\n=========================\nOOD Generalization (Fixed Threshold)\n=========================\nΔ Macro F1 (val - test): 0.106\n\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "s3:deepnote-cell-outputs-production/c4a6e6fe-a3e8-4191-bb3b-117b2c191033",
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "b021856af2d042089ce8f6b51cf37df4",
                "deepnote_cell_type": "markdown"
            },
            "source": "# Post Hoc Interpretation -- Not Working -- Never mind for now",
            "block_group": "390ce735aab046978b9e054c87930e2a"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "af889093",
                "execution_start": 1743778243462,
                "execution_millis": 1056,
                "execution_context_id": "0584160d-43be-413a-acf4-fe6858cdad7d",
                "cell_id": "ab4f14d492694ca28564e5e718d72368",
                "deepnote_cell_type": "code"
            },
            "source": "import shap\nfrom captum.attr import IntegratedGradients\nfrom transformers import AutoTokenizer\nimport torch.nn.functional as F\n\n",
            "block_group": "4ca955de68ce46fd9e86c0f15f64d299",
            "execution_count": 22,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "a8ecc548",
                "execution_start": 1743778246873,
                "execution_millis": 1987,
                "execution_context_id": "0584160d-43be-413a-acf4-fe6858cdad7d",
                "cell_id": "5dd8cc85d112431e942a05c38efb88ce",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# LOAD BEST MODEL\n# ==========================\n\nmodel = TransformerClassifier(MODEL_NAME, num_classes)\nmodel.load_state_dict(torch.load(MODEL_PATH))\nmodel.to(device)\nmodel.eval()\n",
            "block_group": "41262b58401b40be8ffa779d5beea6fc",
            "execution_count": 24,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 24,
                    "data": {
                        "text/plain": "TransformerClassifier(\n  (encoder): DebertaV2Model(\n    (embeddings): DebertaV2Embeddings(\n      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaV2Encoder(\n      (layer): ModuleList(\n        (0-11): 12 x DebertaV2Layer(\n          (attention): DebertaV2Attention(\n            (self): DisentangledSelfAttention(\n              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaV2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaV2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaV2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (classifier): Linear(in_features=768, out_features=25, bias=True)\n)"
                    },
                    "metadata": {}
                }
            ],
            "outputs_reference": "s3:deepnote-cell-outputs-production/bf2e1be2-c90e-47a7-9037-26d092f48ae8",
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "4c9b4da9",
                "execution_start": 1743778255694,
                "execution_millis": 3,
                "execution_context_id": "0584160d-43be-413a-acf4-fe6858cdad7d",
                "cell_id": "60cfe961294e4cec907f41287fe25a39",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# SHAP WRAPPER + EXPLAINER\n# ==========================\n\nimport shap\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclass TextClassifierWrapper:\n    def __init__(self, model, tokenizer, device):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n\n    def __call__(self, texts):\n        texts = [str(t) for t in texts]\n        encodings = self.tokenizer(\n            texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=MAX_LEN\n        )\n        input_ids = encodings[\"input_ids\"].to(self.device)\n        attention_mask = encodings[\"attention_mask\"].to(self.device)\n        with torch.no_grad():\n            logits = self.model(input_ids=input_ids, attention_mask=attention_mask)\n            probs = torch.sigmoid(logits)\n        return probs.cpu().numpy()\n\nwrapped_model = TextClassifierWrapper(model, tokenizer, device)\nmasker = shap.maskers.Text(tokenizer)\nexplainer = shap.Explainer(wrapped_model, masker)\n",
            "block_group": "e93807d0ee714b57a436764e010ce57d",
            "execution_count": 28,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "98c26a3d",
                "execution_start": 1743778258578,
                "execution_millis": 2,
                "execution_context_id": "0584160d-43be-413a-acf4-fe6858cdad7d",
                "cell_id": "96b05f847a754edf935139abaa8fbb33",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# GLOBAL SHAP SUMMARY PLOT\n# ==========================\n\ndef shap_global_summary(texts, filename=\"shap_global_summary.png\", top_n=20):\n    shap_values = explainer(texts)\n\n    all_scores = np.abs(shap_values.values)  # (samples, tokens, outputs)\n    all_tokens = shap_values.data\n\n    if all_scores.ndim == 3:\n        all_scores = all_scores[:, :, 0]  # collapse output label dim\n\n    token_contributions = {}\n    for doc_tokens, doc_scores in zip(all_tokens, all_scores):\n        for token, score in zip(doc_tokens, doc_scores):\n            token = str(token)\n            token_contributions[token] = token_contributions.get(token, []) + [abs(score)]\n\n    token_avg_scores = {tok: np.mean(vals) for tok, vals in token_contributions.items()}\n    top_items = sorted(token_avg_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n    top_tokens, top_scores = zip(*top_items)\n\n    plt.figure(figsize=(10, 6))\n    y_pos = np.arange(len(top_tokens))\n    plt.barh(y_pos, top_scores, align='center')\n    plt.yticks(y_pos, top_tokens)\n    plt.xlabel('Mean |SHAP Value|')\n    plt.title(f'Top {top_n} Most Influential Tokens Globally')\n    plt.gca().invert_yaxis()\n    plt.tight_layout()\n    plt.savefig(filename)\n    plt.close()\n",
            "block_group": "adb86556b9f94adab723c7264bcbab97",
            "execution_count": 30,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "a71a2263",
                "execution_start": 1743778262268,
                "execution_millis": 218215,
                "execution_context_id": "0584160d-43be-413a-acf4-fe6858cdad7d",
                "cell_id": "c10213618d5242b1ae207baa2ad421e2",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# RUN GLOBAL ANALYSIS\n# ==========================\n\nsample_texts = df_test[TEXT_COL].tolist()[:10]\nshap_global_summary(sample_texts, filename=\"shap_global_summary.png\", top_n=25)\n",
            "block_group": "a7edf35e522f4432a3acde04c987b32c",
            "execution_count": 32,
            "outputs": [
                {
                    "name": "stderr",
                    "text": "PartitionExplainer explainer:  10%|█         | 1/10 [00:00<?, ?it/s]\n  0%|          | 0/498 [00:00<?, ?it/s]\u001b[A\n 62%|██████▏   | 308/498 [00:00<00:00, 2946.00it/s]\u001b[A\nPartitionExplainer explainer:  30%|███       | 3/10 [00:23<00:30,  4.34s/it]\n  0%|          | 0/498 [00:00<?, ?it/s]\u001b[A\n 24%|██▍       | 122/498 [00:00<00:00, 433.03it/s]\u001b[A\n 34%|███▍      | 170/498 [00:02<00:06, 54.17it/s] \u001b[A\n 39%|███▉      | 194/498 [00:03<00:07, 41.11it/s]\u001b[A\n 41%|████▏     | 206/498 [00:04<00:07, 36.69it/s]\u001b[A\n 44%|████▍     | 218/498 [00:04<00:08, 32.98it/s]\u001b[A\n 45%|████▍     | 224/498 [00:05<00:08, 31.21it/s]\u001b[A\n 46%|████▌     | 230/498 [00:05<00:09, 29.49it/s]\u001b[A\n 47%|████▋     | 236/498 [00:05<00:09, 27.85it/s]\u001b[A\n 49%|████▊     | 242/498 [00:05<00:09, 26.32it/s]\u001b[A\n 50%|████▉     | 248/498 [00:06<00:09, 25.10it/s]\u001b[A\n 51%|█████     | 254/498 [00:06<00:10, 24.07it/s]\u001b[A\n 52%|█████▏    | 260/498 [00:06<00:10, 23.31it/s]\u001b[A\n 53%|█████▎    | 266/498 [00:07<00:10, 22.74it/s]\u001b[A\n 55%|█████▍    | 272/498 [00:07<00:10, 22.29it/s]\u001b[A\n 56%|█████▌    | 278/498 [00:07<00:10, 21.96it/s]\u001b[A\n 57%|█████▋    | 284/498 [00:07<00:09, 21.73it/s]\u001b[A\n 58%|█████▊    | 290/498 [00:08<00:09, 21.54it/s]\u001b[A\n 59%|█████▉    | 296/498 [00:08<00:09, 21.50it/s]\u001b[A\n 61%|██████    | 302/498 [00:08<00:09, 21.43it/s]\u001b[A\n 62%|██████▏   | 308/498 [00:09<00:08, 21.32it/s]\u001b[A\n 63%|██████▎   | 314/498 [00:09<00:08, 21.26it/s]\u001b[A\n 64%|██████▍   | 320/498 [00:09<00:08, 21.27it/s]\u001b[A\n 65%|██████▌   | 326/498 [00:09<00:08, 21.26it/s]\u001b[A\n 67%|██████▋   | 332/498 [00:10<00:07, 21.22it/s]\u001b[A\n 68%|██████▊   | 338/498 [00:10<00:07, 21.19it/s]\u001b[A\n 69%|██████▉   | 344/498 [00:10<00:07, 21.27it/s]\u001b[A\n 70%|███████   | 350/498 [00:11<00:06, 21.24it/s]\u001b[A\n 71%|███████▏  | 356/498 [00:11<00:06, 21.19it/s]\u001b[A\n 73%|███████▎  | 362/498 [00:11<00:06, 21.22it/s]\u001b[A\n 74%|███████▍  | 368/498 [00:11<00:06, 21.19it/s]\u001b[A\n 75%|███████▌  | 374/498 [00:12<00:05, 21.20it/s]\u001b[A\n 76%|███████▋  | 380/498 [00:12<00:05, 21.22it/s]\u001b[A\n 78%|███████▊  | 386/498 [00:12<00:05, 21.19it/s]\u001b[A\n 79%|███████▊  | 392/498 [00:13<00:04, 21.21it/s]\u001b[A\n 80%|███████▉  | 398/498 [00:13<00:04, 21.19it/s]\u001b[A\n 81%|████████  | 404/498 [00:13<00:04, 21.16it/s]\u001b[A\n 82%|████████▏ | 410/498 [00:13<00:04, 21.19it/s]\u001b[A\n 84%|████████▎ | 416/498 [00:14<00:03, 21.20it/s]\u001b[A\n 85%|████████▍ | 422/498 [00:14<00:03, 21.21it/s]\u001b[A\n 86%|████████▌ | 428/498 [00:14<00:03, 21.21it/s]\u001b[A\n 87%|████████▋ | 434/498 [00:15<00:03, 21.19it/s]\u001b[A\n 88%|████████▊ | 440/498 [00:15<00:02, 21.22it/s]\u001b[A\n 90%|████████▉ | 446/498 [00:15<00:02, 21.25it/s]\u001b[A\n 91%|█████████ | 452/498 [00:15<00:02, 20.96it/s]\u001b[A\n 92%|█████████▏| 458/498 [00:16<00:01, 20.95it/s]\u001b[A\n 93%|█████████▎| 464/498 [00:16<00:01, 20.89it/s]\u001b[A\n 94%|█████████▍| 470/498 [00:16<00:01, 21.03it/s]\u001b[A\n 96%|█████████▌| 476/498 [00:17<00:01, 21.06it/s]\u001b[A\n 97%|█████████▋| 482/498 [00:17<00:00, 21.12it/s]\u001b[A\n 98%|█████████▊| 488/498 [00:17<00:00, 21.14it/s]\u001b[A\n 99%|█████████▉| 494/498 [00:17<00:00, 21.10it/s]\u001b[A\n500it [00:18, 21.16it/s]                         \u001b[A\n504it [00:18, 21.07it/s]\u001b[A\nPartitionExplainer explainer:  40%|████      | 4/10 [00:47<01:15, 12.55s/it]\n  0%|          | 0/498 [00:00<?, ?it/s]\u001b[A\n 24%|██▍       | 122/498 [00:00<00:00, 434.04it/s]\u001b[A\n 34%|███▍      | 170/498 [00:02<00:06, 54.20it/s] \u001b[A\n 39%|███▉      | 194/498 [00:03<00:07, 41.10it/s]\u001b[A\n 41%|████▏     | 206/498 [00:04<00:07, 36.72it/s]\u001b[A\n 44%|████▍     | 218/498 [00:04<00:08, 32.96it/s]\u001b[A\n 45%|████▍     | 224/498 [00:05<00:08, 31.24it/s]\u001b[A\n 46%|████▌     | 230/498 [00:05<00:09, 29.53it/s]\u001b[A\n 47%|████▋     | 236/498 [00:05<00:09, 27.87it/s]\u001b[A\n 49%|████▊     | 242/498 [00:05<00:09, 26.44it/s]\u001b[A\n 50%|████▉     | 248/498 [00:06<00:09, 25.12it/s]\u001b[A\n 51%|█████     | 254/498 [00:06<00:10, 24.11it/s]\u001b[A\n 52%|█████▏    | 260/498 [00:06<00:10, 23.36it/s]\u001b[A\n 53%|█████▎    | 266/498 [00:07<00:10, 22.75it/s]\u001b[A\n 55%|█████▍    | 272/498 [00:07<00:10, 22.30it/s]\u001b[A\n 56%|█████▌    | 278/498 [00:07<00:10, 22.00it/s]\u001b[A\n 57%|█████▋    | 284/498 [00:07<00:09, 21.77it/s]\u001b[A\n 58%|█████▊    | 290/498 [00:08<00:09, 21.60it/s]\u001b[A\n 59%|█████▉    | 296/498 [00:08<00:09, 21.52it/s]\u001b[A\n 61%|██████    | 302/498 [00:08<00:09, 21.34it/s]\u001b[A\n 62%|██████▏   | 308/498 [00:09<00:08, 21.26it/s]\u001b[A\n 63%|██████▎   | 314/498 [00:09<00:08, 21.10it/s]\u001b[A\n 64%|██████▍   | 320/498 [00:09<00:08, 21.03it/s]\u001b[A\n 65%|██████▌   | 326/498 [00:09<00:08, 21.02it/s]\u001b[A\n 67%|██████▋   | 332/498 [00:10<00:07, 21.02it/s]\u001b[A\n 68%|██████▊   | 338/498 [00:10<00:07, 21.09it/s]\u001b[A\n 69%|██████▉   | 344/498 [00:10<00:07, 21.10it/s]\u001b[A\n 70%|███████   | 350/498 [00:11<00:07, 21.11it/s]\u001b[A\n 71%|███████▏  | 356/498 [00:11<00:06, 21.16it/s]\u001b[A\n 73%|███████▎  | 362/498 [00:11<00:06, 21.18it/s]\u001b[A\n 74%|███████▍  | 368/498 [00:11<00:06, 21.20it/s]\u001b[A\n 75%|███████▌  | 374/498 [00:12<00:05, 21.17it/s]\u001b[A\n 76%|███████▋  | 380/498 [00:12<00:05, 21.22it/s]\u001b[A\n 78%|███████▊  | 386/498 [00:12<00:05, 21.19it/s]\u001b[A\n 79%|███████▊  | 392/498 [00:13<00:05, 21.16it/s]\u001b[A\n 80%|███████▉  | 398/498 [00:13<00:04, 21.15it/s]\u001b[A\n 81%|████████  | 404/498 [00:13<00:04, 21.15it/s]\u001b[A\n 82%|████████▏ | 410/498 [00:13<00:04, 21.07it/s]\u001b[A\n 84%|████████▎ | 416/498 [00:14<00:03, 21.09it/s]\u001b[A\n 85%|████████▍ | 422/498 [00:14<00:03, 20.96it/s]\u001b[A\n 86%|████████▌ | 428/498 [00:14<00:03, 20.98it/s]\u001b[A\n 87%|████████▋ | 434/498 [00:15<00:03, 20.96it/s]\u001b[A\n 88%|████████▊ | 440/498 [00:15<00:02, 21.01it/s]\u001b[A\n 90%|████████▉ | 446/498 [00:15<00:02, 21.02it/s]\u001b[A\n 91%|█████████ | 452/498 [00:15<00:02, 21.06it/s]\u001b[A\n 92%|█████████▏| 458/498 [00:16<00:01, 21.15it/s]\u001b[A\n 93%|█████████▎| 464/498 [00:16<00:01, 21.18it/s]\u001b[A\n 94%|█████████▍| 470/498 [00:16<00:01, 21.13it/s]\u001b[A\n 96%|█████████▌| 476/498 [00:17<00:01, 21.13it/s]\u001b[A\n 97%|█████████▋| 482/498 [00:17<00:00, 21.18it/s]\u001b[A\n 98%|█████████▊| 488/498 [00:17<00:00, 21.22it/s]\u001b[A\n 99%|█████████▉| 494/498 [00:17<00:00, 21.18it/s]\u001b[A\n500it [00:18, 21.14it/s]                         \u001b[A\n504it [00:18, 21.05it/s]\u001b[A\nPartitionExplainer explainer:  50%|█████     | 5/10 [01:11<01:24, 16.85s/it]\n  0%|          | 0/498 [00:00<?, ?it/s]\u001b[A\n 24%|██▍       | 122/498 [00:00<00:00, 430.68it/s]\u001b[A\n 34%|███▍      | 170/498 [00:02<00:06, 54.15it/s] \u001b[A\n 39%|███▉      | 194/498 [00:03<00:07, 41.07it/s]\u001b[A\n 41%|████▏     | 206/498 [00:04<00:07, 36.66it/s]\u001b[A\n 44%|████▍     | 218/498 [00:04<00:08, 32.88it/s]\u001b[A\n 45%|████▍     | 224/498 [00:05<00:08, 31.16it/s]\u001b[A\n 46%|████▌     | 230/498 [00:05<00:09, 29.42it/s]\u001b[A\n 47%|████▋     | 236/498 [00:05<00:09, 27.78it/s]\u001b[A\n 49%|████▊     | 242/498 [00:05<00:09, 26.21it/s]\u001b[A\n 50%|████▉     | 248/498 [00:06<00:10, 24.94it/s]\u001b[A\n 51%|█████     | 254/498 [00:06<00:10, 23.93it/s]\u001b[A\n 52%|█████▏    | 260/498 [00:06<00:10, 23.13it/s]\u001b[A\n 53%|█████▎    | 266/498 [00:07<00:10, 22.59it/s]\u001b[A\n 55%|█████▍    | 272/498 [00:07<00:10, 22.17it/s]\u001b[A\n 56%|█████▌    | 278/498 [00:07<00:10, 21.89it/s]\u001b[A\n 57%|█████▋    | 284/498 [00:07<00:09, 21.73it/s]\u001b[A\n 58%|█████▊    | 290/498 [00:08<00:09, 21.51it/s]\u001b[A\n 59%|█████▉    | 296/498 [00:08<00:09, 21.34it/s]\u001b[A\n 61%|██████    | 302/498 [00:08<00:09, 21.20it/s]\u001b[A\n 62%|██████▏   | 308/498 [00:09<00:08, 21.13it/s]\u001b[A\n 63%|██████▎   | 314/498 [00:09<00:08, 21.06it/s]\u001b[A\n 64%|██████▍   | 320/498 [00:09<00:08, 21.13it/s]\u001b[A\n 65%|██████▌   | 326/498 [00:09<00:08, 21.08it/s]\u001b[A\n 67%|██████▋   | 332/498 [00:10<00:07, 21.09it/s]\u001b[A\n 68%|██████▊   | 338/498 [00:10<00:07, 21.10it/s]\u001b[A\n 69%|██████▉   | 344/498 [00:10<00:07, 21.13it/s]\u001b[A\n 70%|███████   | 350/498 [00:11<00:07, 21.12it/s]\u001b[A\n 71%|███████▏  | 356/498 [00:11<00:06, 21.05it/s]\u001b[A\n 73%|███████▎  | 362/498 [00:11<00:06, 21.03it/s]\u001b[A\n 74%|███████▍  | 368/498 [00:11<00:06, 21.00it/s]\u001b[A\n 75%|███████▌  | 374/498 [00:12<00:05, 21.01it/s]\u001b[A\n 76%|███████▋  | 380/498 [00:12<00:05, 20.96it/s]\u001b[A\n 78%|███████▊  | 386/498 [00:12<00:05, 21.02it/s]\u001b[A\n 79%|███████▊  | 392/498 [00:13<00:05, 21.05it/s]\u001b[A\n 80%|███████▉  | 398/498 [00:13<00:04, 21.03it/s]\u001b[A\n 81%|████████  | 404/498 [00:13<00:04, 21.01it/s]\u001b[A\n 82%|████████▏ | 410/498 [00:13<00:04, 20.96it/s]\u001b[A\n 84%|████████▎ | 416/498 [00:14<00:03, 20.95it/s]\u001b[A\n 85%|████████▍ | 422/498 [00:14<00:03, 20.98it/s]\u001b[A\n 86%|████████▌ | 428/498 [00:14<00:03, 20.99it/s]\u001b[A\n 87%|████████▋ | 434/498 [00:15<00:03, 20.98it/s]\u001b[A\n 88%|████████▊ | 440/498 [00:15<00:02, 20.95it/s]\u001b[A\n 90%|████████▉ | 446/498 [00:15<00:02, 20.92it/s]\u001b[A\n 91%|█████████ | 452/498 [00:15<00:02, 20.96it/s]\u001b[A\n 92%|█████████▏| 458/498 [00:16<00:01, 20.99it/s]\u001b[A\n 93%|█████████▎| 464/498 [00:16<00:01, 21.05it/s]\u001b[A\n 94%|█████████▍| 470/498 [00:16<00:01, 21.08it/s]\u001b[A\n 96%|█████████▌| 476/498 [00:17<00:01, 21.11it/s]\u001b[A\n 97%|█████████▋| 482/498 [00:17<00:00, 21.00it/s]\u001b[A\n 98%|█████████▊| 488/498 [00:17<00:00, 20.94it/s]\u001b[A\n 99%|█████████▉| 494/498 [00:17<00:00, 20.94it/s]\u001b[A\n500it [00:18, 20.88it/s]                         \u001b[A\n504it [00:18, 20.83it/s]\u001b[A\nPartitionExplainer explainer:  60%|██████    | 6/10 [01:35<01:17, 19.39s/it]\n  0%|          | 0/498 [00:00<?, ?it/s]\u001b[A\n 24%|██▍       | 122/498 [00:00<00:00, 433.45it/s]\u001b[A\n 34%|███▍      | 170/498 [00:02<00:06, 54.24it/s] \u001b[A\n 39%|███▉      | 194/498 [00:03<00:07, 41.12it/s]\u001b[A\n 41%|████▏     | 206/498 [00:04<00:07, 36.73it/s]\u001b[A\n 44%|████▍     | 218/498 [00:04<00:08, 32.98it/s]\u001b[A\n 45%|████▍     | 224/498 [00:05<00:08, 31.24it/s]\u001b[A\n 46%|████▌     | 230/498 [00:05<00:09, 29.30it/s]\u001b[A\n 47%|████▋     | 236/498 [00:05<00:09, 27.62it/s]\u001b[A\n 49%|████▊     | 242/498 [00:05<00:09, 26.14it/s]\u001b[A\n 50%|████▉     | 248/498 [00:06<00:10, 24.88it/s]\u001b[A\n 51%|█████     | 254/498 [00:06<00:10, 23.76it/s]\u001b[A\n 52%|█████▏    | 260/498 [00:06<00:10, 22.98it/s]\u001b[A\n 53%|█████▎    | 266/498 [00:07<00:10, 22.44it/s]\u001b[A\n 55%|█████▍    | 272/498 [00:07<00:10, 21.97it/s]\u001b[A\n 56%|█████▌    | 278/498 [00:07<00:10, 21.63it/s]\u001b[A\n 57%|█████▋    | 284/498 [00:07<00:09, 21.42it/s]\u001b[A\n 58%|█████▊    | 290/498 [00:08<00:09, 21.29it/s]\u001b[A\n 59%|█████▉    | 296/498 [00:08<00:09, 21.15it/s]\u001b[A\n 61%|██████    | 302/498 [00:08<00:09, 21.06it/s]\u001b[A\n 62%|██████▏   | 308/498 [00:09<00:09, 21.02it/s]\u001b[A\n 63%|██████▎   | 314/498 [00:09<00:08, 21.05it/s]\u001b[A\n 64%|██████▍   | 320/498 [00:09<00:08, 21.04it/s]\u001b[A\n 65%|██████▌   | 326/498 [00:09<00:08, 21.04it/s]\u001b[A\n 67%|██████▋   | 332/498 [00:10<00:07, 21.05it/s]\u001b[A\n 68%|██████▊   | 338/498 [00:10<00:07, 21.05it/s]\u001b[A\n 69%|██████▉   | 344/498 [00:10<00:07, 20.99it/s]\u001b[A\n 70%|███████   | 350/498 [00:11<00:07, 20.95it/s]\u001b[A\n 71%|███████▏  | 356/498 [00:11<00:06, 21.00it/s]\u001b[A\n 73%|███████▎  | 362/498 [00:11<00:06, 20.97it/s]\u001b[A\n 74%|███████▍  | 368/498 [00:11<00:06, 21.03it/s]\u001b[A\n 75%|███████▌  | 374/498 [00:12<00:05, 21.10it/s]\u001b[A\n 76%|███████▋  | 380/498 [00:12<00:05, 21.09it/s]\u001b[A\n 78%|███████▊  | 386/498 [00:12<00:05, 21.08it/s]\u001b[A\n 79%|███████▊  | 392/498 [00:13<00:05, 21.15it/s]\u001b[A\n 80%|███████▉  | 398/498 [00:13<00:04, 21.17it/s]\u001b[A\n 81%|████████  | 404/498 [00:13<00:04, 21.12it/s]\u001b[A\n 82%|████████▏ | 410/498 [00:13<00:04, 21.09it/s]\u001b[A\n 84%|████████▎ | 416/498 [00:14<00:03, 21.04it/s]\u001b[A\n 85%|████████▍ | 422/498 [00:14<00:03, 21.01it/s]\u001b[A\n 86%|████████▌ | 428/498 [00:14<00:03, 20.98it/s]\u001b[A\n 87%|████████▋ | 434/498 [00:15<00:03, 20.99it/s]\u001b[A\n 88%|████████▊ | 440/498 [00:15<00:02, 21.02it/s]\u001b[A\n 90%|████████▉ | 446/498 [00:15<00:02, 20.94it/s]\u001b[A\n 91%|█████████ | 452/498 [00:15<00:02, 20.93it/s]\u001b[A\n 92%|█████████▏| 458/498 [00:16<00:01, 20.91it/s]\u001b[A\n 93%|█████████▎| 464/498 [00:16<00:01, 20.93it/s]\u001b[A\n 94%|█████████▍| 470/498 [00:16<00:01, 20.96it/s]\u001b[A\n 96%|█████████▌| 476/498 [00:17<00:01, 21.03it/s]\u001b[A\n 97%|█████████▋| 482/498 [00:17<00:00, 21.06it/s]\u001b[A\n 98%|█████████▊| 488/498 [00:17<00:00, 21.04it/s]\u001b[A\n 99%|█████████▉| 494/498 [00:17<00:00, 21.03it/s]\u001b[A\n500it [00:18, 21.06it/s]                         \u001b[A\n504it [00:18, 21.01it/s]\u001b[A\nPartitionExplainer explainer:  70%|███████   | 7/10 [01:59<01:02, 20.98s/it]\n  0%|          | 0/498 [00:00<?, ?it/s]\u001b[A\n 24%|██▍       | 122/498 [00:00<00:00, 431.31it/s]\u001b[A\n 34%|███▍      | 170/498 [00:02<00:06, 54.23it/s] \u001b[A\n 39%|███▉      | 194/498 [00:03<00:07, 40.94it/s]\u001b[A\n 41%|████▏     | 206/498 [00:04<00:07, 36.54it/s]\u001b[A\n 44%|████▍     | 218/498 [00:04<00:08, 32.82it/s]\u001b[A\n 45%|████▍     | 224/498 [00:05<00:08, 31.12it/s]\u001b[A\n 46%|████▌     | 230/498 [00:05<00:09, 29.41it/s]\u001b[A\n 47%|████▋     | 236/498 [00:05<00:09, 27.76it/s]\u001b[A\n 49%|████▊     | 242/498 [00:05<00:09, 26.26it/s]\u001b[A\n 50%|████▉     | 248/498 [00:06<00:10, 24.96it/s]\u001b[A\n 51%|█████     | 254/498 [00:06<00:10, 23.91it/s]\u001b[A\n 52%|█████▏    | 260/498 [00:06<00:10, 23.12it/s]\u001b[A\n 53%|█████▎    | 266/498 [00:07<00:10, 22.50it/s]\u001b[A\n 55%|█████▍    | 272/498 [00:07<00:10, 22.14it/s]\u001b[A\n 56%|█████▌    | 278/498 [00:07<00:10, 21.91it/s]\u001b[A\n 57%|█████▋    | 284/498 [00:07<00:09, 21.61it/s]\u001b[A\n 58%|█████▊    | 290/498 [00:08<00:09, 21.51it/s]\u001b[A\n 59%|█████▉    | 296/498 [00:08<00:09, 21.37it/s]\u001b[A\n 61%|██████    | 302/498 [00:08<00:09, 21.27it/s]\u001b[A\n 62%|██████▏   | 308/498 [00:09<00:08, 21.17it/s]\u001b[A\n 63%|██████▎   | 314/498 [00:09<00:08, 21.11it/s]\u001b[A\n 64%|██████▍   | 320/498 [00:09<00:08, 21.00it/s]\u001b[A\n 65%|██████▌   | 326/498 [00:09<00:08, 20.87it/s]\u001b[A\n 67%|██████▋   | 332/498 [00:10<00:07, 20.95it/s]\u001b[A\n 68%|██████▊   | 338/498 [00:10<00:07, 20.94it/s]\u001b[A\n 69%|██████▉   | 344/498 [00:10<00:07, 20.93it/s]\u001b[A\n 70%|███████   | 350/498 [00:11<00:07, 20.93it/s]\u001b[A\n 71%|███████▏  | 356/498 [00:11<00:06, 20.94it/s]\u001b[A\n 73%|███████▎  | 362/498 [00:11<00:06, 20.97it/s]\u001b[A\n 74%|███████▍  | 368/498 [00:11<00:06, 20.99it/s]\u001b[A\n 75%|███████▌  | 374/498 [00:12<00:05, 21.05it/s]\u001b[A\n 76%|███████▋  | 380/498 [00:12<00:05, 21.05it/s]\u001b[A\n 78%|███████▊  | 386/498 [00:12<00:05, 21.07it/s]\u001b[A\n 79%|███████▊  | 392/498 [00:13<00:05, 21.00it/s]\u001b[A\n 80%|███████▉  | 398/498 [00:13<00:04, 20.97it/s]\u001b[A\n 81%|████████  | 404/498 [00:13<00:04, 20.97it/s]\u001b[A\n 82%|████████▏ | 410/498 [00:13<00:04, 21.00it/s]\u001b[A\n 84%|████████▎ | 416/498 [00:14<00:03, 21.03it/s]\u001b[A\n 85%|████████▍ | 422/498 [00:14<00:03, 21.03it/s]\u001b[A\n 86%|████████▌ | 428/498 [00:14<00:03, 21.09it/s]\u001b[A\n 87%|████████▋ | 434/498 [00:15<00:03, 21.15it/s]\u001b[A\n 88%|████████▊ | 440/498 [00:15<00:02, 21.14it/s]\u001b[A\n 90%|████████▉ | 446/498 [00:15<00:02, 21.15it/s]\u001b[A\n 91%|█████████ | 452/498 [00:15<00:02, 21.17it/s]\u001b[A\n 92%|█████████▏| 458/498 [00:16<00:01, 21.12it/s]\u001b[A\n 93%|█████████▎| 464/498 [00:16<00:01, 21.11it/s]\u001b[A\n 94%|█████████▍| 470/498 [00:16<00:01, 21.15it/s]\u001b[A\n 96%|█████████▌| 476/498 [00:17<00:01, 21.13it/s]\u001b[A\n 97%|█████████▋| 482/498 [00:17<00:00, 21.14it/s]\u001b[A\n 98%|█████████▊| 488/498 [00:17<00:00, 21.09it/s]\u001b[A\n 99%|█████████▉| 494/498 [00:17<00:00, 21.12it/s]\u001b[A\n500it [00:18, 21.10it/s]                         \u001b[A\n504it [00:18, 21.03it/s]\u001b[A\nPartitionExplainer explainer:  80%|████████  | 8/10 [02:23<00:43, 21.99s/it]\n  0%|          | 0/498 [00:00<?, ?it/s]\u001b[A\n 24%|██▍       | 122/498 [00:00<00:00, 427.26it/s]\u001b[A\n 34%|███▍      | 170/498 [00:02<00:06, 53.32it/s] \u001b[A\n 39%|███▉      | 194/498 [00:03<00:07, 40.50it/s]\u001b[A\n 41%|████▏     | 206/498 [00:04<00:08, 36.21it/s]\u001b[A\n 44%|████▍     | 218/498 [00:04<00:08, 32.58it/s]\u001b[A\n 45%|████▍     | 224/498 [00:05<00:08, 30.79it/s]\u001b[A\n 46%|████▌     | 230/498 [00:05<00:09, 29.05it/s]\u001b[A\n 47%|████▋     | 236/498 [00:05<00:09, 27.36it/s]\u001b[A\n 49%|████▊     | 242/498 [00:06<00:09, 25.86it/s]\u001b[A\n 50%|████▉     | 248/498 [00:06<00:10, 24.53it/s]\u001b[A\n 51%|█████     | 254/498 [00:06<00:10, 23.53it/s]\u001b[A\n 52%|█████▏    | 260/498 [00:06<00:10, 22.86it/s]\u001b[A\n 53%|█████▎    | 266/498 [00:07<00:10, 22.29it/s]\u001b[A\n 55%|█████▍    | 272/498 [00:07<00:10, 21.78it/s]\u001b[A\n 56%|█████▌    | 278/498 [00:07<00:10, 21.53it/s]\u001b[A\n 57%|█████▋    | 284/498 [00:08<00:10, 21.18it/s]\u001b[A\n 58%|█████▊    | 290/498 [00:08<00:09, 21.02it/s]\u001b[A\n 59%|█████▉    | 296/498 [00:08<00:09, 20.99it/s]\u001b[A\n 61%|██████    | 302/498 [00:08<00:09, 20.90it/s]\u001b[A\n 62%|██████▏   | 308/498 [00:09<00:09, 20.82it/s]\u001b[A\n 63%|██████▎   | 314/498 [00:09<00:08, 20.85it/s]\u001b[A\n 64%|██████▍   | 320/498 [00:09<00:08, 20.62it/s]\u001b[A\n 65%|██████▌   | 326/498 [00:10<00:08, 20.68it/s]\u001b[A\n 67%|██████▋   | 332/498 [00:10<00:08, 20.66it/s]\u001b[A\n 68%|██████▊   | 338/498 [00:10<00:07, 20.66it/s]\u001b[A\n 69%|██████▉   | 344/498 [00:10<00:07, 20.75it/s]\u001b[A\n 70%|███████   | 350/498 [00:11<00:07, 20.70it/s]\u001b[A\n 71%|███████▏  | 356/498 [00:11<00:06, 20.66it/s]\u001b[A\n 73%|███████▎  | 362/498 [00:11<00:06, 20.76it/s]\u001b[A\n 74%|███████▍  | 368/498 [00:12<00:06, 20.75it/s]\u001b[A\n 75%|███████▌  | 374/498 [00:12<00:05, 20.67it/s]\u001b[A\n 76%|███████▋  | 380/498 [00:12<00:05, 20.75it/s]\u001b[A\n 78%|███████▊  | 386/498 [00:13<00:05, 20.76it/s]\u001b[A\n 79%|███████▊  | 392/498 [00:13<00:05, 20.68it/s]\u001b[A\n 80%|███████▉  | 398/498 [00:13<00:04, 20.72it/s]\u001b[A\n 81%|████████  | 404/498 [00:13<00:04, 20.74it/s]\u001b[A\n 82%|████████▏ | 410/498 [00:14<00:04, 20.67it/s]\u001b[A\n 84%|████████▎ | 416/498 [00:14<00:03, 20.70it/s]\u001b[A\n 85%|████████▍ | 422/498 [00:14<00:03, 20.67it/s]\u001b[A\n 86%|████████▌ | 428/498 [00:15<00:03, 20.68it/s]\u001b[A\n 87%|████████▋ | 434/498 [00:15<00:03, 20.78it/s]\u001b[A\n 88%|████████▊ | 440/498 [00:15<00:02, 20.64it/s]\u001b[A\n 90%|████████▉ | 446/498 [00:15<00:02, 20.68it/s]\u001b[A\n 91%|█████████ | 452/498 [00:16<00:02, 20.68it/s]\u001b[A\n 92%|█████████▏| 458/498 [00:16<00:01, 20.69it/s]\u001b[A\n 93%|█████████▎| 464/498 [00:16<00:01, 20.72it/s]\u001b[A\n 94%|█████████▍| 470/498 [00:17<00:01, 20.76it/s]\u001b[A\n 96%|█████████▌| 476/498 [00:17<00:01, 20.75it/s]\u001b[A\n 97%|█████████▋| 482/498 [00:17<00:00, 20.69it/s]\u001b[A\n 98%|█████████▊| 488/498 [00:17<00:00, 20.76it/s]\u001b[A\n 99%|█████████▉| 494/498 [00:18<00:00, 20.84it/s]\u001b[A\n500it [00:18, 20.85it/s]                         \u001b[A\n504it [00:18, 20.79it/s]\u001b[A\nPartitionExplainer explainer:  90%|█████████ | 9/10 [02:48<00:22, 22.81s/it]\n  0%|          | 0/498 [00:00<?, ?it/s]\u001b[A\n 24%|██▍       | 122/498 [00:00<00:00, 410.10it/s]\u001b[A\n 33%|███▎      | 164/498 [00:02<00:05, 57.62it/s] \u001b[A\n 38%|███▊      | 188/498 [00:03<00:07, 42.00it/s]\u001b[A\n 40%|████      | 200/498 [00:04<00:08, 37.07it/s]\u001b[A\n 43%|████▎     | 212/498 [00:04<00:08, 32.98it/s]\u001b[A\n 44%|████▍     | 218/498 [00:04<00:09, 31.11it/s]\u001b[A\n 45%|████▍     | 224/498 [00:05<00:09, 29.26it/s]\u001b[A\n 46%|████▌     | 230/498 [00:05<00:09, 27.53it/s]\u001b[A\n 47%|████▋     | 236/498 [00:05<00:10, 26.00it/s]\u001b[A\n 49%|████▊     | 242/498 [00:06<00:10, 24.80it/s]\u001b[A\n 50%|████▉     | 248/498 [00:06<00:10, 23.72it/s]\u001b[A\n 51%|█████     | 254/498 [00:06<00:10, 22.87it/s]\u001b[A\n 52%|█████▏    | 260/498 [00:06<00:10, 22.33it/s]\u001b[A\n 53%|█████▎    | 266/498 [00:07<00:10, 21.97it/s]\u001b[A\n 55%|█████▍    | 272/498 [00:07<00:10, 21.46it/s]\u001b[A\n 56%|█████▌    | 278/498 [00:07<00:10, 21.31it/s]\u001b[A\n 57%|█████▋    | 284/498 [00:08<00:10, 21.12it/s]\u001b[A\n 58%|█████▊    | 290/498 [00:08<00:09, 20.99it/s]\u001b[A\n 59%|█████▉    | 296/498 [00:08<00:09, 20.92it/s]\u001b[A\n 61%|██████    | 302/498 [00:08<00:09, 20.84it/s]\u001b[A\n 62%|██████▏   | 308/498 [00:09<00:09, 20.81it/s]\u001b[A\n 63%|██████▎   | 314/498 [00:09<00:08, 20.76it/s]\u001b[A\n 64%|██████▍   | 320/498 [00:09<00:08, 20.72it/s]\u001b[A\n 65%|██████▌   | 326/498 [00:10<00:08, 20.74it/s]\u001b[A\n 67%|██████▋   | 332/498 [00:10<00:07, 20.80it/s]\u001b[A\n 68%|██████▊   | 338/498 [00:10<00:07, 20.73it/s]\u001b[A\n 69%|██████▉   | 344/498 [00:10<00:07, 20.68it/s]\u001b[A\n 70%|███████   | 350/498 [00:11<00:07, 20.63it/s]\u001b[A\n 71%|███████▏  | 356/498 [00:11<00:06, 20.59it/s]\u001b[A\n 73%|███████▎  | 362/498 [00:11<00:06, 20.66it/s]\u001b[A\n 74%|███████▍  | 368/498 [00:12<00:06, 20.69it/s]\u001b[A\n 75%|███████▌  | 374/498 [00:12<00:05, 20.67it/s]\u001b[A\n 76%|███████▋  | 380/498 [00:12<00:05, 20.71it/s]\u001b[A\n 78%|███████▊  | 386/498 [00:13<00:05, 20.62it/s]\u001b[A\n 79%|███████▊  | 392/498 [00:13<00:05, 20.68it/s]\u001b[A\n 80%|███████▉  | 398/498 [00:13<00:04, 20.69it/s]\u001b[A\n 81%|████████  | 404/498 [00:13<00:04, 20.67it/s]\u001b[A\n 82%|████████▏ | 410/498 [00:14<00:04, 20.70it/s]\u001b[A\n 84%|████████▎ | 416/498 [00:14<00:03, 20.79it/s]\u001b[A\n 85%|████████▍ | 422/498 [00:14<00:03, 20.75it/s]\u001b[A\n 86%|████████▌ | 428/498 [00:15<00:03, 20.70it/s]\u001b[A\n 87%|████████▋ | 434/498 [00:15<00:03, 20.77it/s]\u001b[A\n 88%|████████▊ | 440/498 [00:15<00:02, 20.71it/s]\u001b[A\n 90%|████████▉ | 446/498 [00:15<00:02, 20.71it/s]\u001b[A\n 91%|█████████ | 452/498 [00:16<00:02, 20.79it/s]\u001b[A\n 92%|█████████▏| 458/498 [00:16<00:01, 20.83it/s]\u001b[A\n 93%|█████████▎| 464/498 [00:16<00:01, 20.69it/s]\u001b[A\n 94%|█████████▍| 470/498 [00:17<00:01, 20.75it/s]\u001b[A\n 96%|█████████▌| 476/498 [00:17<00:01, 20.79it/s]\u001b[A\n 97%|█████████▋| 482/498 [00:17<00:00, 20.74it/s]\u001b[A\n 98%|█████████▊| 488/498 [00:17<00:00, 20.68it/s]\u001b[A\n 99%|█████████▉| 494/498 [00:18<00:00, 20.72it/s]\u001b[A\n500it [00:18, 20.75it/s]                         \u001b[A\n504it [00:18, 20.67it/s]\u001b[A\nPartitionExplainer explainer: 100%|██████████| 10/10 [03:12<00:00, 23.35s/it]\n  0%|          | 0/498 [00:00<?, ?it/s]\u001b[A\n 24%|██▍       | 122/498 [00:00<00:00, 421.56it/s]\u001b[A\n 34%|███▍      | 170/498 [00:02<00:06, 53.38it/s] \u001b[A\n 39%|███▉      | 194/498 [00:03<00:07, 40.36it/s]\u001b[A\n 41%|████▏     | 206/498 [00:04<00:08, 36.08it/s]\u001b[A\n 44%|████▍     | 218/498 [00:04<00:08, 32.51it/s]\u001b[A\n 45%|████▍     | 224/498 [00:05<00:08, 30.75it/s]\u001b[A\n 46%|████▌     | 230/498 [00:05<00:09, 28.99it/s]\u001b[A\n 47%|████▋     | 236/498 [00:05<00:09, 27.37it/s]\u001b[A\n 49%|████▊     | 242/498 [00:06<00:09, 25.88it/s]\u001b[A\n 50%|████▉     | 248/498 [00:06<00:10, 24.62it/s]\u001b[A\n 51%|█████     | 254/498 [00:06<00:10, 23.61it/s]\u001b[A\n 52%|█████▏    | 260/498 [00:06<00:10, 22.65it/s]\u001b[A\n 53%|█████▎    | 266/498 [00:07<00:10, 22.15it/s]\u001b[A\n 55%|█████▍    | 272/498 [00:07<00:10, 21.67it/s]\u001b[A\n 56%|█████▌    | 278/498 [00:07<00:10, 21.40it/s]\u001b[A\n 57%|█████▋    | 284/498 [00:08<00:10, 21.16it/s]\u001b[A\n 58%|█████▊    | 290/498 [00:08<00:09, 20.96it/s]\u001b[A\n 59%|█████▉    | 296/498 [00:08<00:09, 20.95it/s]\u001b[A\n 61%|██████    | 302/498 [00:08<00:09, 20.79it/s]\u001b[A\n 62%|██████▏   | 308/498 [00:09<00:09, 20.76it/s]\u001b[A\n 63%|██████▎   | 314/498 [00:09<00:08, 20.77it/s]\u001b[A\n 64%|██████▍   | 320/498 [00:09<00:08, 20.72it/s]\u001b[A\n 65%|██████▌   | 326/498 [00:10<00:08, 20.71it/s]\u001b[A\n 67%|██████▋   | 332/498 [00:10<00:07, 20.76it/s]\u001b[A\n 68%|██████▊   | 338/498 [00:10<00:07, 20.72it/s]\u001b[A\n 69%|██████▉   | 344/498 [00:10<00:07, 20.69it/s]\u001b[A\n 70%|███████   | 350/498 [00:11<00:07, 20.69it/s]\u001b[A\n 71%|███████▏  | 356/498 [00:11<00:06, 20.68it/s]\u001b[A\n 73%|███████▎  | 362/498 [00:11<00:06, 20.65it/s]\u001b[A\n 74%|███████▍  | 368/498 [00:12<00:06, 20.67it/s]\u001b[A\n 75%|███████▌  | 374/498 [00:12<00:05, 20.67it/s]\u001b[A\n 76%|███████▋  | 380/498 [00:12<00:05, 20.63it/s]\u001b[A\n 78%|███████▊  | 386/498 [00:13<00:05, 20.70it/s]\u001b[A\n 79%|███████▊  | 392/498 [00:13<00:05, 20.67it/s]\u001b[A\n 80%|███████▉  | 398/498 [00:13<00:04, 20.71it/s]\u001b[A\n 81%|████████  | 404/498 [00:13<00:04, 20.76it/s]\u001b[A\n 82%|████████▏ | 410/498 [00:14<00:04, 20.83it/s]\u001b[A\n 84%|████████▎ | 416/498 [00:14<00:03, 20.83it/s]\u001b[A\n 85%|████████▍ | 422/498 [00:14<00:03, 20.74it/s]\u001b[A\n 86%|████████▌ | 428/498 [00:15<00:03, 20.70it/s]\u001b[A\n 87%|████████▋ | 434/498 [00:15<00:03, 20.70it/s]\u001b[A\n 88%|████████▊ | 440/498 [00:15<00:02, 20.67it/s]\u001b[A\n 90%|████████▉ | 446/498 [00:15<00:02, 20.70it/s]\u001b[A\n 91%|█████████ | 452/498 [00:16<00:02, 20.66it/s]\u001b[A\n 92%|█████████▏| 458/498 [00:16<00:01, 20.62it/s]\u001b[A\n 93%|█████████▎| 464/498 [00:16<00:01, 20.72it/s]\u001b[A\n 94%|█████████▍| 470/498 [00:17<00:01, 20.70it/s]\u001b[A\n 96%|█████████▌| 476/498 [00:17<00:01, 20.70it/s]\u001b[A\n 97%|█████████▋| 482/498 [00:17<00:00, 20.76it/s]\u001b[A\n 98%|█████████▊| 488/498 [00:17<00:00, 20.78it/s]\u001b[A\n 99%|█████████▉| 494/498 [00:18<00:00, 20.76it/s]\u001b[A\n500it [00:18, 20.74it/s]                         \u001b[A\n504it [00:18, 20.72it/s]\u001b[A\nPartitionExplainer explainer: 11it [03:37, 21.76s/it]\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "s3:deepnote-cell-outputs-production/8b18b10e-e00c-4728-a2bf-1b2c9457b68a",
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=49d39932-ba1f-4621-a036-ab99ade88496' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
            "metadata": {
                "created_in_deepnote_cell": true,
                "deepnote_cell_type": "markdown"
            }
        }
    ],
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "deepnote_notebook_id": "8ccec4a61d4a48a9970e32794a5a560d"
    }
}