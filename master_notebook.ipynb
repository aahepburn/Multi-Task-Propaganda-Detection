{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Master Notebook"
      ],
      "metadata": {
        "id": "dm0YrOSxd4dA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkHFBELYsQio",
        "outputId": "f11862fb-2363-46e2-ab80-29134d4a2b48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQL193B0sbiR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Thesis Repository\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcdqzp0csryX",
        "outputId": "e3e69e74-ba8e-4f60-9e81-c6240b4c911f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: hf_xet in /usr/local/lib/python3.11/dist-packages (1.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-multilearn hf_xet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZ4ZrK2Cr3gF"
      },
      "outputs": [],
      "source": [
        "# RUN THESE IMPORTS FIRST\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DebertaV2Tokenizer, AutoModel, RobertaTokenizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "import numpy as np\n",
        "import shap\n",
        "#from captum.attr import IntegratedGradients\n",
        "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
        "import torch.nn.functional as F\n",
        "import hf_xet\n",
        "import itertools\n",
        "#import optuna\n",
        "import sys\n",
        "import importlib # !pip install importlib\n",
        "sys.path.append('.')\n",
        "\n",
        "\n",
        "### Custom built modules ###\n",
        "import importlib\n",
        "\n",
        "import data_loader_STL\n",
        "importlib.reload(data_loader_STL)\n",
        "from data_loader_STL import prepare_data_STL_fine, prepare_data_STL_hierarchical, prepare_data_STL_coarse\n",
        "\n",
        "import single_task\n",
        "importlib.reload(single_task)\n",
        "from single_task import TransformerClassifier, MultiLabelDataset, train_single_task_model, train_hierarchical_classifier\n",
        "\n",
        "import multi_task\n",
        "importlib.reload(multi_task)\n",
        "from multi_task import MultiTaskTransformer, train_mtl_flat, train_mtl_hierarchical, apply_hierarchical_constraints_mtl, hierarchical_loss_mtl, AdapterMultiTaskTransformer\n",
        "\n",
        "import data_loader_MTL\n",
        "importlib.reload(data_loader_MTL)\n",
        "from data_loader_MTL import prepare_data_MTL_fine_flat, prepare_data_MTL_hierarchical, prepare_data_MTL_coarse, MultiTaskDataset, prepare_data_MTL_mixed\n",
        "\n",
        "import evaluation_utils as eval_util\n",
        "importlib.reload(eval_util)\n",
        "from evaluation_utils import evaluate_flat, evaluate_hierarchy, evaluate_mtl_all_tasks, evaluate_mtl_task, evaluate_per_class_flat, evaluate_per_domain_flat, predict_proba, evaluate_threshold_sweep, evaluate_mtl_hierarchical_task, evaluate_mtl_hierarchical_all_tasks, evaluate_flat_custom, compute_fine_vs_coarse_metrics, get_coarse_label_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8wlcPozt2_L"
      },
      "source": [
        "# Ablation - mixed dataset function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXp1Sexzt0dL"
      },
      "outputs": [],
      "source": [
        "def prepare_data_MTL_mixed(\n",
        "    task,\n",
        "    train_domains,\n",
        "    test_domains,\n",
        "    train_languages,\n",
        "    model_name,\n",
        "    max_len,\n",
        "    batch_size,\n",
        "    granularity_s1=\"coarse\",\n",
        "    granularity_s2=\"fine\"\n",
        "):\n",
        "    # --- Task 1 ---\n",
        "    if granularity_s1 == \"fine\":\n",
        "        (\n",
        "            df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n",
        "            _, _, _, _, _, _, _,\n",
        "            train_loader_s1, val_loader_s1, test_loader_s1,\n",
        "            _, _, _,\n",
        "            _\n",
        "        ) = prepare_data_MTL_fine_flat(\n",
        "            task,\n",
        "            train_domains=train_domains,\n",
        "            test_domains=test_domains,\n",
        "            train_languages=train_languages,\n",
        "            model_name=model_name,\n",
        "            max_len=max_len,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "    elif granularity_s1 == \"coarse\":\n",
        "        (\n",
        "            df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n",
        "            _, _, _, _, _, _, _,\n",
        "            train_loader_s1, val_loader_s1, test_loader_s1,\n",
        "            _, _, _,\n",
        "            _\n",
        "        ) = prepare_data_MTL_coarse(\n",
        "            task,\n",
        "            train_domains=train_domains,\n",
        "            test_domains=test_domains,\n",
        "            train_languages=train_languages,\n",
        "            model_name=model_name,\n",
        "            max_len=max_len,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "    # --- Task 2 ---\n",
        "    if granularity_s2 == \"fine\":\n",
        "        (\n",
        "            _, _, _, _, _, _, _,\n",
        "            df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n",
        "            _, _, _,\n",
        "            train_loader_s2, val_loader_s2, test_loader_s2,\n",
        "            _\n",
        "        ) = prepare_data_MTL_fine_flat(\n",
        "            task,\n",
        "            train_domains=train_domains,\n",
        "            test_domains=test_domains,\n",
        "            train_languages=train_languages,\n",
        "            model_name=model_name,\n",
        "            max_len=max_len,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "    elif granularity_s2 == \"coarse\":\n",
        "        (\n",
        "            _, _, _, _, _, _, _,\n",
        "            df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n",
        "            _, _, _,\n",
        "            train_loader_s2, val_loader_s2, test_loader_s2,\n",
        "            _\n",
        "        ) = prepare_data_MTL_coarse(\n",
        "            task,\n",
        "            train_domains=train_domains,\n",
        "            test_domains=test_domains,\n",
        "            train_languages=train_languages,\n",
        "            model_name=model_name,\n",
        "            max_len=max_len,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "    num_classes_dict = {\n",
        "        \"task1\": len(mlb_s1.classes_),\n",
        "        \"task2\": len(mlb_s2.classes_)\n",
        "    }\n",
        "\n",
        "    return (\n",
        "        df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n",
        "        df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n",
        "        train_loader_s1, val_loader_s1, test_loader_s1,\n",
        "        train_loader_s2, val_loader_s2, test_loader_s2,\n",
        "        num_classes_dict\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJIHWWzpSRqz"
      },
      "source": [
        "# Ablation - Granularity - MTL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "up45fgTAsbJj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "8e246843-06bc-4f5b-bd22-dd2bbe95707a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-771ccb925bf3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ablation_results_final_seeds\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1966\u001b[0m                                 )\n\u001b[1;32m   1967\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1968\u001b[0;31m                         for template in list_repo_templates(\n\u001b[0m\u001b[1;32m   1969\u001b[0m                             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m                             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mlist_repo_templates\u001b[0;34m(repo_id, local_files_only, revision, cache_dir)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremoveprefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{CHAT_TEMPLATE_DIR}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 for entry in list_repo_tree(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremoveprefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{CHAT_TEMPLATE_DIR}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 for entry in list_repo_tree(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mlist_repo_tree\u001b[0;34m(self, repo_id, path_in_repo, recursive, expand, revision, repo_type, token)\u001b[0m\n\u001b[1;32m   3138\u001b[0m         \u001b[0mencoded_path_in_repo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_in_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath_in_repo\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3139\u001b[0m         \u001b[0mtree_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{self.endpoint}/api/{repo_type}s/{repo_id}/tree/{revision}{encoded_path_in_repo}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpath_info\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaginate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"recursive\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"expand\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3141\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mRepoFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpath_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mRepoFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpath_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_pagination.py\u001b[0m in \u001b[0;36mpaginate\u001b[0;34m(path, params, headers)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \"\"\"\n\u001b[1;32m     35\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allow_redirects\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0;31m# Redirect resolving generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_redirects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0;31m# Redirect resolving generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_redirects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                 resp = self.send(\n\u001b[0m\u001b[1;32m    266\u001b[0m                     \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Send: {_curlify(request)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_AMZN_TRACE_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import itertools\n",
        "import torch\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# === Configuration ===\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "MAX_LEN = 512\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 3e-5\n",
        "TRAIN_LANGUAGES = [\"ALL\"]\n",
        "\n",
        "granularity_options = ['fine','coarse'] #  38, 39, 40, 54, 55, 71, 72, 73, 74, 75\n",
        "seeds = [71, 72, 73, 74, 75] # 42, 43, 44\n",
        "granularity_configs = [('coarse', 'coarse')]\n",
        "\n",
        "\n",
        "train_domains_all = [[\"UA\"], [\"CC\"], [\"UA\", \"CC\"]]\n",
        "test_domains = [\"UA\", \"CC\"]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "os.makedirs(\"ablation_results_final_seeds\", exist_ok=True)\n",
        "\n",
        "# training loop\n",
        "for gran_s1, gran_s2 in granularity_configs:\n",
        "    config_id = f\"ef-{gran_s1}_nc-{gran_s2}\"\n",
        "    results = []\n",
        "\n",
        "    for train_domain in train_domains_all:\n",
        "        for task_type in [\"multi_task\", \"multi_task_adapter\"]:\n",
        "            for seed in seeds:\n",
        "                torch.manual_seed(seed)\n",
        "\n",
        "                # set model path\n",
        "                model_path = f\"{task_type}_{'-'.join(train_domain)}_to_{'-'.join(test_domains)}_ef-{gran_s1}_nc-{gran_s2}_seed{seed}.pt\"\n",
        "\n",
        "                # --- Data Prep ---\n",
        "                (\n",
        "                    df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n",
        "                    df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n",
        "                    train_loader_s1, val_loader_s1, test_loader_s1,\n",
        "                    train_loader_s2, val_loader_s2, test_loader_s2,\n",
        "                    num_classes_dict\n",
        "                ) = prepare_data_MTL_mixed(\n",
        "                    task=task_type,\n",
        "                    train_domains=train_domain,\n",
        "                    test_domains=test_domains,\n",
        "                    train_languages=TRAIN_LANGUAGES,\n",
        "                    model_name=MODEL_NAME,\n",
        "                    max_len=MAX_LEN,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    granularity_s1=gran_s1,\n",
        "                    granularity_s2=gran_s2\n",
        "                )\n",
        "\n",
        "                task_classes = {\n",
        "                    \"narrative_classification\": y_train_s2.shape[1],\n",
        "                    \"entity_framing\": y_train_s1.shape[1]\n",
        "                }\n",
        "\n",
        "                # --- Model ---\n",
        "                if task_type == \"multi_task\":\n",
        "                    model = MultiTaskTransformer(MODEL_NAME, task_classes).to(device)\n",
        "                else:\n",
        "                    model = AdapterMultiTaskTransformer(\n",
        "                        model_name=MODEL_NAME,\n",
        "                        num_classes_dict=task_classes,\n",
        "                        adapter_dim=128\n",
        "                    ).to(device)\n",
        "\n",
        "                optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "                criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "                # --- Train ---\n",
        "                train_mtl_flat(\n",
        "                    model=model,\n",
        "                    loaders={\n",
        "                        \"narrative_classification\": train_loader_s2,\n",
        "                        \"entity_framing\": train_loader_s1\n",
        "                    },\n",
        "                    val_data={\n",
        "                        \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2),\n",
        "                        \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1)\n",
        "                    },\n",
        "                    mlbs={\n",
        "                        \"narrative_classification\": mlb_s2,\n",
        "                        \"entity_framing\": mlb_s1\n",
        "                    },\n",
        "                    optimizer=optimizer,\n",
        "                    criterion=criterion,\n",
        "                    device=device,\n",
        "                    epochs=EPOCHS,\n",
        "                    train_domain=train_domain,\n",
        "                    test_domain=test_domains\n",
        "                )\n",
        "\n",
        "                # torch.save(model.state_dict(), model_path)\n",
        "\n",
        "                #  Evaluate\n",
        "                eval_results = evaluate_mtl_all_tasks(\n",
        "                    model=model,\n",
        "                    task_loaders={\n",
        "                        \"narrative_classification\": test_loader_s2,\n",
        "                        \"entity_framing\": test_loader_s1\n",
        "                    },\n",
        "                    task_dfs={\n",
        "                        \"narrative_classification\": df_test_s2,\n",
        "                        \"entity_framing\": df_test_s1\n",
        "                    },\n",
        "                    task_targets={\n",
        "                        \"narrative_classification\": y_test_s2,\n",
        "                        \"entity_framing\": y_test_s1\n",
        "                    },\n",
        "                    task_mlbs={\n",
        "                        \"narrative_classification\": mlb_s2,\n",
        "                        \"entity_framing\": mlb_s1\n",
        "                    },\n",
        "                    domain_list=train_domain,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                ef = eval_results[\"entity_framing\"]\n",
        "                nc = eval_results[\"narrative_classification\"]\n",
        "\n",
        "                results.append({\n",
        "                    \"task_type\": task_type,\n",
        "                    \"seed\": seed,\n",
        "                    \"train_domain\": \"-\".join(train_domain),\n",
        "                    \"ef_granularity\": gran_s1,\n",
        "                    \"nc_granularity\": gran_s2,\n",
        "\n",
        "                    # EF metrics\n",
        "                    \"ef_micro_ua\": ef[\"UA\"][\"micro\"],\n",
        "                    \"ef_macro_ua\": ef[\"UA\"][\"macro\"],\n",
        "                    \"ef_exact_ua\": ef[\"UA\"][\"exact\"],\n",
        "                    \"ef_micro_cc\": ef[\"CC\"][\"micro\"],\n",
        "                    \"ef_macro_cc\": ef[\"CC\"][\"macro\"],\n",
        "                    \"ef_exact_cc\": ef[\"CC\"][\"exact\"],\n",
        "\n",
        "                    # NC metrics\n",
        "                    \"nc_micro_ua\": nc[\"UA\"][\"micro\"],\n",
        "                    \"nc_macro_ua\": nc[\"UA\"][\"macro\"],\n",
        "                    \"nc_exact_ua\": nc[\"UA\"][\"exact\"],\n",
        "                    \"nc_micro_cc\": nc[\"CC\"][\"micro\"],\n",
        "                    \"nc_macro_cc\": nc[\"CC\"][\"macro\"],\n",
        "                    \"nc_exact_cc\": nc[\"CC\"][\"exact\"]\n",
        "                })\n",
        "\n",
        "\n",
        "    df_out = pd.DataFrame(results)\n",
        "    out_path = f\"ablation_results/ablation_{config_id}.csv\"\n",
        "    df_out.to_csv(out_path, index=False)\n",
        "    print(f\" Saved: {out_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRZs9vaeSXHO"
      },
      "source": [
        "# Ablation - Domain/Task - MTL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NphV8HReSWxW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import itertools\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# === Configuration ===\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "MAX_LEN = 512\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 3e-5\n",
        "TRAIN_LANGUAGES = [\"ALL\"]\n",
        "TEST_LANGUAGES = [\"ALL\"]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "os.makedirs(\"ablation_results_cross_domain_more_seeds\", exist_ok=True)\n",
        "\n",
        "# === Domain Configurations for Task Splits ===\n",
        "domain_configs = [\n",
        "    (\"UA\", \"CC\"),  # EF on UA, NC on CC\n",
        "    (\"CC\", \"UA\")   # EF on CC, NC on UA\n",
        "]\n",
        "seeds = [71, 72, 73, 74, 75] # 42, 43, 44, 31, 32\n",
        "\n",
        "for ef_domain, nc_domain in domain_configs:\n",
        "    config_id = f\"EF-{ef_domain}_NC-{nc_domain}\"\n",
        "    results = []\n",
        "\n",
        "    for task_type in [\"multi_task\", \"multi_task_adapter\"]:\n",
        "        for seed in seeds:\n",
        "            torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "            model_path = f\"{task_type}_EF-{ef_domain}_NC-{nc_domain}_seed{seed}.pt\"\n",
        "\n",
        "            #Prepare data.\n",
        "            (\n",
        "                df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n",
        "                df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n",
        "                train_loader_s1, val_loader_s1, test_loader_s1,\n",
        "                train_loader_s2, val_loader_s2, test_loader_s2,\n",
        "                num_classes_dict\n",
        "            ) = prepare_data_MTL_mixed(\n",
        "                task=task_type,\n",
        "                train_domains=[ef_domain, nc_domain],\n",
        "                test_domains=[\"UA\", \"CC\"],\n",
        "                train_languages=TRAIN_LANGUAGES,\n",
        "                model_name=MODEL_NAME,\n",
        "                max_len=MAX_LEN,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                granularity_s1=\"fine\",\n",
        "                granularity_s2=\"fine\"\n",
        "            )\n",
        "\n",
        "            task_classes = {\n",
        "                \"entity_framing\": y_train_s1.shape[1],\n",
        "                \"narrative_classification\": y_train_s2.shape[1]\n",
        "            }\n",
        "\n",
        "            # --- Model\n",
        "            if task_type == \"multi_task\":\n",
        "                model = MultiTaskTransformer(MODEL_NAME, task_classes).to(device)\n",
        "            else:\n",
        "                model = AdapterMultiTaskTransformer(\n",
        "                    model_name=MODEL_NAME,\n",
        "                    num_classes_dict=task_classes,\n",
        "                    adapter_dim=128\n",
        "                ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "            criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "            # --- Train\n",
        "            train_mtl_flat(\n",
        "                model=model,\n",
        "                loaders={\n",
        "                    \"narrative_classification\": train_loader_s2,\n",
        "                    \"entity_framing\": train_loader_s1\n",
        "                },\n",
        "                val_data={\n",
        "                    \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2),\n",
        "                    \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1)\n",
        "                },\n",
        "                mlbs={\n",
        "                    \"narrative_classification\": mlb_s2,\n",
        "                    \"entity_framing\": mlb_s1\n",
        "                },\n",
        "                optimizer=optimizer,\n",
        "                criterion=criterion,\n",
        "                device=device,\n",
        "                epochs=EPOCHS,\n",
        "                train_domain=[ef_domain, nc_domain],\n",
        "                test_domain=[\"UA\", \"CC\"]\n",
        "            )\n",
        "\n",
        "            # --- Evaluate\n",
        "            eval_results = evaluate_mtl_all_tasks(\n",
        "                model=model,\n",
        "                task_loaders={\n",
        "                    \"narrative_classification\": test_loader_s2,\n",
        "                    \"entity_framing\": test_loader_s1\n",
        "                },\n",
        "                task_dfs={\n",
        "                    \"narrative_classification\": df_test_s2,\n",
        "                    \"entity_framing\": df_test_s1\n",
        "                },\n",
        "                task_targets={\n",
        "                    \"narrative_classification\": y_test_s2,\n",
        "                    \"entity_framing\": y_test_s1\n",
        "                },\n",
        "                task_mlbs={\n",
        "                    \"narrative_classification\": mlb_s2,\n",
        "                    \"entity_framing\": mlb_s1\n",
        "                },\n",
        "                domain_list=[ef_domain, nc_domain],\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            ef = eval_results[\"entity_framing\"]\n",
        "            nc = eval_results[\"narrative_classification\"]\n",
        "\n",
        "            results.append({\n",
        "                \"task_type\": task_type,\n",
        "                \"seed\": seed,\n",
        "                \"ef_train_domain\": ef_domain,\n",
        "                \"nc_train_domain\": nc_domain,\n",
        "\n",
        "                \"ef_micro_ua\": ef[\"UA\"][\"micro\"],\n",
        "                \"ef_macro_ua\": ef[\"UA\"][\"macro\"],\n",
        "                \"ef_exact_ua\": ef[\"UA\"][\"exact\"],\n",
        "                \"ef_micro_cc\": ef[\"CC\"][\"micro\"],\n",
        "                \"ef_macro_cc\": ef[\"CC\"][\"macro\"],\n",
        "                \"ef_exact_cc\": ef[\"CC\"][\"exact\"],\n",
        "\n",
        "                \"nc_micro_ua\": nc[\"UA\"][\"micro\"],\n",
        "                \"nc_macro_ua\": nc[\"UA\"][\"macro\"],\n",
        "                \"nc_exact_ua\": nc[\"UA\"][\"exact\"],\n",
        "                \"nc_micro_cc\": nc[\"CC\"][\"micro\"],\n",
        "                \"nc_macro_cc\": nc[\"CC\"][\"macro\"],\n",
        "                \"nc_exact_cc\": nc[\"CC\"][\"exact\"]\n",
        "            })\n",
        "\n",
        "\n",
        "    df_out = pd.DataFrame(results)\n",
        "    out_path = f\"ablation_results_cross_domain_more_seeds/ablation_{config_id}.csv\"\n",
        "    df_out.to_csv(out_path, index=False)\n",
        "    print(f\" Saved: {out_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQEOGZaA8HEE"
      },
      "source": [
        "# STL - Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# === Config ===\n",
        "MODELS = [\"distilbert-base-uncased\"]\n",
        "TASKS = [\"entity_framing\"]\n",
        "TAXONOMY_DEPTHS = [\"fine\"]\n",
        "SEEDS = [71]\n",
        "TRAIN_DOMAINS = [[\"UA\"]]\n",
        "TEST_DOMAIN = [\"UA\", \"CC\"]\n",
        "TRAIN_LANGUAGES = [\"ALL\"]\n",
        "MAX_LEN = 512\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 3e-5\n",
        "THRESHOLD = 0.35\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "os.makedirs(\"shap_plots_final\", exist_ok=True)\n",
        "results = []\n",
        "\n",
        "for model_name, task, taxonomy, train_domain, seed in itertools.product(MODELS, TASKS, TAXONOMY_DEPTHS, TRAIN_DOMAINS, SEEDS):\n",
        "    domain_str = \"-\".join(train_domain)\n",
        "    print(f\"\\n--- Running STL: {task} | {taxonomy} | {model_name} | Seed={seed} | Train on {domain_str} ---\")\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    #Data prep\n",
        "    if taxonomy == \"fine\":\n",
        "        df_train, df_val, df_test, y_train, y_val, y_test, mlb, TEXT_COL, LABEL_COL = prepare_data_STL_fine(\n",
        "            TASK=task,\n",
        "            train_domains=train_domain,\n",
        "            test_domains=TEST_DOMAIN,\n",
        "            train_languages=TRAIN_LANGUAGES\n",
        "        )\n",
        "    else:\n",
        "        df_train, df_val, df_test, y_train, y_val, y_test, mlb, TEXT_COL, LABEL_COL = prepare_data_STL_coarse(\n",
        "            TASK=task,\n",
        "            train_domains=train_domain,\n",
        "            test_domains=TEST_DOMAIN,\n",
        "            train_languages=TRAIN_LANGUAGES\n",
        "        )\n",
        "\n",
        "    #  Dataset Setup\n",
        "    train_dataset = MultiLabelDataset(df_train[TEXT_COL].tolist(), y_train, tokenizer, MAX_LEN)\n",
        "    val_dataset = MultiLabelDataset(df_val[TEXT_COL].tolist(), y_val, tokenizer, MAX_LEN)\n",
        "    test_dataset = MultiLabelDataset(df_test[TEXT_COL].tolist(), y_test, tokenizer, MAX_LEN)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Model Init\n",
        "    num_classes = y_train.shape[1]\n",
        "    model = TransformerClassifier(model_name, num_classes)\n",
        "\n",
        "    model_path = (\n",
        "        f\"model_{task}_{taxonomy}_trained_on_{domain_str}\"\n",
        "        f\"_{model_name.replace('/', '-')}_seed{seed}.pt\"\n",
        "    )\n",
        "\n",
        "    # === Train ===\n",
        "    model = train_single_task_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        y_val=y_val,\n",
        "        MODEL_PATH=model_path,\n",
        "        LEARNING_RATE=LEARNING_RATE,\n",
        "        EPOCHS=EPOCHS,\n",
        "        device=device,\n",
        "        predict_proba=predict_proba,\n",
        "        evaluate_threshold_sweep=evaluate_threshold_sweep\n",
        "    )\n",
        "\n",
        "    # evaluate\n",
        "    eval_result = evaluate_flat(\n",
        "        model=model,\n",
        "        loader=test_loader,\n",
        "        df_source=df_test,\n",
        "        mlb=mlb,\n",
        "        device=device,\n",
        "        label=\"TEST\",\n",
        "        threshold=THRESHOLD\n",
        "    )\n",
        "\n",
        "    #Extract Per-Domain Metrics\n",
        "    for domain in [\"UA\", \"CC\"]:\n",
        "        results.append({\n",
        "            \"model\": model_name,\n",
        "            \"task\": task,\n",
        "            \"taxonomy\": taxonomy,\n",
        "            \"train_domain\": domain_str,\n",
        "            \"seed\": seed,\n",
        "            \"eval_domain\": domain,\n",
        "            \"micro\": eval_result[\"per_domain\"][domain][\"micro\"],\n",
        "            \"macro\": eval_result[\"per_domain\"][domain][\"macro\"],\n",
        "            \"exact\": eval_result[\"per_domain\"][domain][\"exact\"]\n",
        "        })\n",
        "\n",
        "    # SHAP\n",
        "    texts_ef = df_test[\"Translated_Text\"].tolist()\n",
        "    shap_values_ef = explain_shap(model, tokenizer, texts_ef, max_explain=5)\n",
        "    save_shap_waterfall_plots(shap_values_ef, \"shap_plots\", task, seed, task, mlb)\n",
        "\n",
        "\n",
        "df_out = pd.DataFrame(results)\n",
        "df_out.to_csv(\"ablation_results_stl_augmented_new/stl_all_results_augmented_more_seeds.csv\", index=False)\n",
        "print(\"ablation_results_stl_augmented_new/stl_all_results_augmented_more_seeds.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "042639888a4a4c68a6852b6e764b7c20",
            "d5f497c1ffe444b48a5d8e1186117533",
            "882344abfe924d4f971c6f7265f5623b",
            "f2243025599b4490b41876f8d376e1d3",
            "d455152b6ae4465b8a68ca4570449da2",
            "7b67916412bc4aef9b4cd1eb58c54495",
            "06bc20c4690b4b098ee527ce85c125d1",
            "5269f2e4c60746a5ac1aad906788eb23",
            "924d3c804d624b6386e495bd6510899e",
            "1bacfd58d7564b539fe24ba2e3f41e80",
            "52359529d90440e48e9950d6b99c6105",
            "e98582a6976540fa92cf59f053c9b598",
            "8a5afe073ba0484e90ce46d74fa17149",
            "f638d78a6c4b419a8d9036d69c7ecc09",
            "e96f86a79c844e4c919d0f93722ce0c1",
            "e23e73b886be470b99105c2868044e85",
            "5792cf4d98974aa18adabcae005f437d",
            "a20aa5491a4e49559e32f02897aed569",
            "b11ae35b962e4b748c2ba18ed83774a7",
            "c5bb5d81aaf4462d9e7b1868ddf168c5",
            "275dbdf0aae545f998184475e3c25d35",
            "6d0768178f934361b34281daf9ec0d9d",
            "292335598b064d189814458c7d4fa299",
            "6e559c98ddd1464b95c72bd3961ac63f",
            "af8f9e67935140f38a3e889cc1555e23",
            "cb75c5a357bc4802ad1b7e32603ee792",
            "8c1bc66c674f436282342f9b3c7ec85b",
            "0bcb29880ac94eecb3bcb6bf0aeb1868",
            "f2da75b3cea04ee39397be0731ada68c",
            "08b53cc2b77844c294e205b99cd6b545",
            "e6cd209aaa9f4c5994a1e3f9af12b445",
            "1688eaf89dba40be8cd48b599a59ab07",
            "1023152b2a524d1abe6b15d037f31dfe"
          ]
        },
        "id": "M_5C2Gi7PWJj",
        "outputId": "213ff940-5caf-4b15-c639-5cd09591d0a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running STL: entity_framing | fine | distilbert-base-uncased | Seed=71 | Train on UA ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "042639888a4a4c68a6852b6e764b7c20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e98582a6976540fa92cf59f053c9b598"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "292335598b064d189814458c7d4fa299"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|| 602/602 [01:01<00:00,  9.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: Loss = 0.2422\n",
            "Threshold sweep results:\n",
            "Thresh 0.10 | Macro F1: 0.157 | Micro F1: 0.399 | Exact Match: 0.000\n",
            "Thresh 0.15 | Macro F1: 0.136 | Micro F1: 0.430 | Exact Match: 0.001\n",
            "Thresh 0.20 | Macro F1: 0.121 | Micro F1: 0.454 | Exact Match: 0.009\n",
            "Thresh 0.25 | Macro F1: 0.101 | Micro F1: 0.446 | Exact Match: 0.015\n",
            "Thresh 0.30 | Macro F1: 0.087 | Micro F1: 0.427 | Exact Match: 0.005\n",
            "Thresh 0.35 | Macro F1: 0.080 | Micro F1: 0.411 | Exact Match: 0.003\n",
            "Thresh 0.40 | Macro F1: 0.075 | Micro F1: 0.392 | Exact Match: 0.024\n",
            "Thresh 0.45 | Macro F1: 0.066 | Micro F1: 0.366 | Exact Match: 0.023\n",
            "Thresh 0.50 | Macro F1: 0.057 | Micro F1: 0.325 | Exact Match: 0.017\n",
            "Thresh 0.55 | Macro F1: 0.041 | Micro F1: 0.265 | Exact Match: 0.000\n",
            "Thresh 0.60 | Macro F1: 0.032 | Micro F1: 0.198 | Exact Match: 0.000\n",
            "Thresh 0.65 | Macro F1: 0.019 | Micro F1: 0.121 | Exact Match: 0.000\n",
            "Thresh 0.70 | Macro F1: 0.008 | Micro F1: 0.052 | Exact Match: 0.000\n",
            "Thresh 0.75 | Macro F1: 0.000 | Micro F1: 0.003 | Exact Match: 0.000\n",
            "Thresh 0.80 | Macro F1: 0.000 | Micro F1: 0.000 | Exact Match: 0.000\n",
            "Thresh 0.85 | Macro F1: 0.000 | Micro F1: 0.000 | Exact Match: 0.000\n",
            "\n",
            " Best threshold = 0.10 with Macro F1 = 0.157\n",
            "Validation Macro F1 (Epoch 1): 0.1573\n",
            "Saved best model (Epoch 1) to model_entity_framing_fine_trained_on_UA_distilbert-base-uncased_seed71.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|| 602/602 [01:01<00:00,  9.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: Loss = 0.2092\n",
            "Threshold sweep results:\n",
            "Thresh 0.10 | Macro F1: 0.240 | Micro F1: 0.439 | Exact Match: 0.002\n",
            "Thresh 0.15 | Macro F1: 0.234 | Micro F1: 0.503 | Exact Match: 0.027\n",
            "Thresh 0.20 | Macro F1: 0.207 | Micro F1: 0.535 | Exact Match: 0.065\n",
            "Thresh 0.25 | Macro F1: 0.190 | Micro F1: 0.555 | Exact Match: 0.136\n",
            "Thresh 0.30 | Macro F1: 0.183 | Micro F1: 0.559 | Exact Match: 0.187\n",
            "Thresh 0.35 | Macro F1: 0.176 | Micro F1: 0.548 | Exact Match: 0.199\n",
            "Thresh 0.40 | Macro F1: 0.162 | Micro F1: 0.526 | Exact Match: 0.172\n",
            "Thresh 0.45 | Macro F1: 0.144 | Micro F1: 0.494 | Exact Match: 0.135\n",
            "Thresh 0.50 | Macro F1: 0.126 | Micro F1: 0.466 | Exact Match: 0.104\n",
            "Thresh 0.55 | Macro F1: 0.106 | Micro F1: 0.420 | Exact Match: 0.074\n",
            "Thresh 0.60 | Macro F1: 0.086 | Micro F1: 0.373 | Exact Match: 0.046\n",
            "Thresh 0.65 | Macro F1: 0.063 | Micro F1: 0.310 | Exact Match: 0.021\n",
            "Thresh 0.70 | Macro F1: 0.051 | Micro F1: 0.263 | Exact Match: 0.010\n",
            "Thresh 0.75 | Macro F1: 0.034 | Micro F1: 0.189 | Exact Match: 0.000\n",
            "Thresh 0.80 | Macro F1: 0.022 | Micro F1: 0.117 | Exact Match: 0.000\n",
            "Thresh 0.85 | Macro F1: 0.005 | Micro F1: 0.024 | Exact Match: 0.000\n",
            "\n",
            " Best threshold = 0.10 with Macro F1 = 0.240\n",
            "Validation Macro F1 (Epoch 2): 0.2402\n",
            "Saved best model (Epoch 2) to model_entity_framing_fine_trained_on_UA_distilbert-base-uncased_seed71.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|| 602/602 [01:01<00:00,  9.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: Loss = 0.1833\n",
            "Threshold sweep results:\n",
            "Thresh 0.10 | Macro F1: 0.333 | Micro F1: 0.520 | Exact Match: 0.007\n",
            "Thresh 0.15 | Macro F1: 0.345 | Micro F1: 0.595 | Exact Match: 0.085\n",
            "Thresh 0.20 | Macro F1: 0.332 | Micro F1: 0.638 | Exact Match: 0.203\n",
            "Thresh 0.25 | Macro F1: 0.317 | Micro F1: 0.661 | Exact Match: 0.306\n",
            "Thresh 0.30 | Macro F1: 0.286 | Micro F1: 0.663 | Exact Match: 0.337\n",
            "Thresh 0.35 | Macro F1: 0.265 | Micro F1: 0.663 | Exact Match: 0.350\n",
            "Thresh 0.40 | Macro F1: 0.251 | Micro F1: 0.655 | Exact Match: 0.332\n",
            "Thresh 0.45 | Macro F1: 0.241 | Micro F1: 0.647 | Exact Match: 0.314\n",
            "Thresh 0.50 | Macro F1: 0.229 | Micro F1: 0.634 | Exact Match: 0.283\n",
            "Thresh 0.55 | Macro F1: 0.216 | Micro F1: 0.612 | Exact Match: 0.250\n",
            "Thresh 0.60 | Macro F1: 0.199 | Micro F1: 0.586 | Exact Match: 0.215\n",
            "Thresh 0.65 | Macro F1: 0.177 | Micro F1: 0.549 | Exact Match: 0.174\n",
            "Thresh 0.70 | Macro F1: 0.143 | Micro F1: 0.498 | Exact Match: 0.123\n",
            "Thresh 0.75 | Macro F1: 0.111 | Micro F1: 0.440 | Exact Match: 0.082\n",
            "Thresh 0.80 | Macro F1: 0.087 | Micro F1: 0.375 | Exact Match: 0.052\n",
            "Thresh 0.85 | Macro F1: 0.054 | Micro F1: 0.269 | Exact Match: 0.014\n",
            "\n",
            " Best threshold = 0.15 with Macro F1 = 0.345\n",
            "Validation Macro F1 (Epoch 3): 0.3453\n",
            "Saved best model (Epoch 3) to model_entity_framing_fine_trained_on_UA_distilbert-base-uncased_seed71.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|| 602/602 [01:01<00:00,  9.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4: Loss = 0.1534\n",
            "Threshold sweep results:\n",
            "Thresh 0.10 | Macro F1: 0.431 | Micro F1: 0.588 | Exact Match: 0.046\n",
            "Thresh 0.15 | Macro F1: 0.471 | Micro F1: 0.667 | Exact Match: 0.183\n",
            "Thresh 0.20 | Macro F1: 0.456 | Micro F1: 0.711 | Exact Match: 0.304\n",
            "Thresh 0.25 | Macro F1: 0.430 | Micro F1: 0.732 | Exact Match: 0.383\n",
            "Thresh 0.30 | Macro F1: 0.398 | Micro F1: 0.741 | Exact Match: 0.417\n",
            "Thresh 0.35 | Macro F1: 0.376 | Micro F1: 0.744 | Exact Match: 0.439\n",
            "Thresh 0.40 | Macro F1: 0.358 | Micro F1: 0.741 | Exact Match: 0.432\n",
            "Thresh 0.45 | Macro F1: 0.324 | Micro F1: 0.730 | Exact Match: 0.407\n",
            "Thresh 0.50 | Macro F1: 0.305 | Micro F1: 0.720 | Exact Match: 0.380\n",
            "Thresh 0.55 | Macro F1: 0.283 | Micro F1: 0.708 | Exact Match: 0.351\n",
            "Thresh 0.60 | Macro F1: 0.266 | Micro F1: 0.689 | Exact Match: 0.320\n",
            "Thresh 0.65 | Macro F1: 0.246 | Micro F1: 0.665 | Exact Match: 0.275\n",
            "Thresh 0.70 | Macro F1: 0.217 | Micro F1: 0.632 | Exact Match: 0.227\n",
            "Thresh 0.75 | Macro F1: 0.189 | Micro F1: 0.593 | Exact Match: 0.174\n",
            "Thresh 0.80 | Macro F1: 0.149 | Micro F1: 0.541 | Exact Match: 0.112\n",
            "Thresh 0.85 | Macro F1: 0.097 | Micro F1: 0.458 | Exact Match: 0.037\n",
            "\n",
            " Best threshold = 0.15 with Macro F1 = 0.471\n",
            "Validation Macro F1 (Epoch 4): 0.4709\n",
            "Saved best model (Epoch 4) to model_entity_framing_fine_trained_on_UA_distilbert-base-uncased_seed71.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating TEST: 100%|| 56/56 [00:02<00:00, 21.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " TEST (Fixed Threshold=0.35):\n",
            "Macro F1: 0.207\n",
            "Micro F1: 0.519\n",
            "Exact Match: 0.243\n",
            "\n",
            "----------------------------\n",
            "Per-Domain Breakdown\n",
            "----------------------------\n",
            "\n",
            " Domain: CC\n",
            "Macro F1: 0.135\n",
            "Micro F1: 0.683\n",
            "Exact Match: 0.440\n",
            "\n",
            " Domain: UA\n",
            "Macro F1: 0.189\n",
            "Micro F1: 0.477\n",
            "Exact Match: 0.193\n",
            "ablation_results_stl_augmented_new/stl_all_results_augmented_more_seeds.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLrDn238dXH6"
      },
      "source": [
        "# MTL/MTL-PAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln_E4MHPdW2H",
        "outputId": "0e73fb6f-39d6-4f3f-d831-17d055b62c24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- MULTI_TASK | Train on: UA | Seed=71 ---\n",
            "\n",
            "Starting Epoch 1/4...\n",
            "\n",
            "Epoch 1 - Average Loss: 0.4554\n",
            "\n",
            "Validating task: narrative_classification\n",
            "[narrative_classification] Macro F1: 0.0488\n",
            "Best model for task 'narrative_classification' saved to narrative_classification_MTL_UA_to_UA-CC.pt\n",
            "\n",
            "Validating task: entity_framing\n",
            "[entity_framing] Macro F1: 0.1333\n",
            "Best model for task 'entity_framing' saved to entity_framing_MTL_UA_to_UA-CC.pt\n",
            "\n",
            "Starting Epoch 2/4...\n",
            "\n",
            "Epoch 2 - Average Loss: 0.3294\n",
            "\n",
            "Validating task: narrative_classification\n",
            "[narrative_classification] Macro F1: 0.1113\n",
            "Best model for task 'narrative_classification' saved to narrative_classification_MTL_UA_to_UA-CC.pt\n",
            "\n",
            "Validating task: entity_framing\n",
            "[entity_framing] Macro F1: 0.2487\n",
            "Best model for task 'entity_framing' saved to entity_framing_MTL_UA_to_UA-CC.pt\n",
            "\n",
            "Starting Epoch 3/4...\n",
            "\n",
            "Epoch 3 - Average Loss: 0.2711\n",
            "\n",
            "Validating task: narrative_classification\n",
            "[narrative_classification] Macro F1: 0.1372\n",
            "Best model for task 'narrative_classification' saved to narrative_classification_MTL_UA_to_UA-CC.pt\n",
            "\n",
            "Validating task: entity_framing\n",
            "[entity_framing] Macro F1: 0.3603\n",
            "Best model for task 'entity_framing' saved to entity_framing_MTL_UA_to_UA-CC.pt\n",
            "\n",
            "Starting Epoch 4/4...\n",
            "\n",
            "Epoch 4 - Average Loss: 0.2242\n",
            "\n",
            "Validating task: narrative_classification\n",
            "[narrative_classification] Macro F1: 0.2256\n",
            "Best model for task 'narrative_classification' saved to narrative_classification_MTL_UA_to_UA-CC.pt\n",
            "\n",
            "Validating task: entity_framing\n",
            "[entity_framing] Macro F1: 0.4817\n",
            "Best model for task 'entity_framing' saved to entity_framing_MTL_UA_to_UA-CC.pt\n",
            "\n",
            "--- Task: NARRATIVE_CLASSIFICATION ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating TEST [narrative_classification]: 100%|| 23/23 [00:01<00:00, 21.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Domain: CC\n",
            "Macro F1: 0.014\n",
            "Micro F1: 0.230\n",
            "Exact Match: 0.014\n",
            "\n",
            "Domain: UA\n",
            "Macro F1: 0.125\n",
            "Micro F1: 0.426\n",
            "Exact Match: 0.056\n",
            "\n",
            "--- Task: ENTITY_FRAMING ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating TEST [entity_framing]: 100%|| 56/56 [00:02<00:00, 21.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Domain: CC\n",
            "Macro F1: 0.138\n",
            "Micro F1: 0.763\n",
            "Exact Match: 0.648\n",
            "\n",
            "Domain: UA\n",
            "Macro F1: 0.195\n",
            "Micro F1: 0.455\n",
            "Exact Match: 0.216\n",
            " Saved: new_augmented_seeds/baseline_mtl_more_seeds_augmented_70UA.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import itertools\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# === Config ===\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "MAX_LEN = 512\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 3e-5\n",
        "TRAIN_LANGUAGES = [\"ALL\"]\n",
        "TEST_DOMAINS = [\"UA\", \"CC\"]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "os.makedirs(\"new_augmented_seeds\", exist_ok=True)\n",
        "\n",
        "# Domain configs\n",
        "train_domain_configs = [[\"UA\"]]\n",
        "seeds = [71] #71, 72, 73, 74, 75, 31, 32, 42, 43, 44\n",
        "\n",
        "for train_domains in train_domain_configs:\n",
        "    domain_str = \"-\".join(train_domains)\n",
        "    results = []\n",
        "\n",
        "    for task_type in [\"multi_task\"]:\n",
        "        for seed in seeds:\n",
        "            torch.manual_seed(seed)\n",
        "\n",
        "            print(f\"\\n--- {task_type.upper()} | Train on: {domain_str} | Seed={seed} ---\")\n",
        "\n",
        "            model_path = f\"{task_type}_distilbert_trained_on_{domain_str}_seed{seed}.pt\"\n",
        "\n",
        "            # === Prepare Data ===\n",
        "            (\n",
        "                df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n",
        "                df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n",
        "                train_loader_s1, val_loader_s1, test_loader_s1,\n",
        "                train_loader_s2, val_loader_s2, test_loader_s2,\n",
        "                num_classes_dict\n",
        "            ) = prepare_data_MTL_mixed(\n",
        "                task=task_type,\n",
        "                train_domains=train_domains,\n",
        "                test_domains=TEST_DOMAINS,\n",
        "                train_languages=TRAIN_LANGUAGES,\n",
        "                model_name=MODEL_NAME,\n",
        "                max_len=MAX_LEN,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                granularity_s1=\"fine\",\n",
        "                granularity_s2=\"fine\"\n",
        "            )\n",
        "\n",
        "            task_classes = {\n",
        "                \"entity_framing\": y_train_s1.shape[1],\n",
        "                \"narrative_classification\": y_train_s2.shape[1]\n",
        "            }\n",
        "\n",
        "            # Initialise Model\n",
        "            if task_type == \"multi_task\":\n",
        "                model = MultiTaskTransformer(MODEL_NAME, task_classes).to(device)\n",
        "            else:\n",
        "                model = AdapterMultiTaskTransformer(\n",
        "                    model_name=MODEL_NAME,\n",
        "                    num_classes_dict=task_classes,\n",
        "                    adapter_dim=128\n",
        "                ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "            criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "            # Train\n",
        "            train_mtl_flat(\n",
        "                model=model,\n",
        "                loaders={\n",
        "                    \"narrative_classification\": train_loader_s2,\n",
        "                    \"entity_framing\": train_loader_s1\n",
        "                },\n",
        "                val_data={\n",
        "                    \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2),\n",
        "                    \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1)\n",
        "                },\n",
        "                mlbs={\n",
        "                    \"narrative_classification\": mlb_s2,\n",
        "                    \"entity_framing\": mlb_s1\n",
        "                },\n",
        "                optimizer=optimizer,\n",
        "                criterion=criterion,\n",
        "                device=device,\n",
        "                epochs=EPOCHS,\n",
        "                train_domain=train_domains,\n",
        "                test_domain=TEST_DOMAINS\n",
        "            )\n",
        "\n",
        "            # === Evaluate ===\n",
        "            eval_results = evaluate_mtl_all_tasks(\n",
        "                model=model,\n",
        "                task_loaders={\n",
        "                    \"narrative_classification\": test_loader_s2,\n",
        "                    \"entity_framing\": test_loader_s1\n",
        "                },\n",
        "                task_dfs={\n",
        "                    \"narrative_classification\": df_test_s2,\n",
        "                    \"entity_framing\": df_test_s1\n",
        "                },\n",
        "                task_targets={\n",
        "                    \"narrative_classification\": y_test_s2,\n",
        "                    \"entity_framing\": y_test_s1\n",
        "                },\n",
        "                task_mlbs={\n",
        "                    \"narrative_classification\": mlb_s2,\n",
        "                    \"entity_framing\": mlb_s1\n",
        "                },\n",
        "                domain_list=train_domains,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            ef = eval_results[\"entity_framing\"]\n",
        "            nc = eval_results[\"narrative_classification\"]\n",
        "\n",
        "            results.append({\n",
        "                \"task_type\": task_type,\n",
        "                \"model\": \"distilbert-base-uncased\",\n",
        "                \"seed\": seed,\n",
        "                \"train_domain\": domain_str,\n",
        "\n",
        "                \"ef_micro_ua\": ef[\"UA\"][\"micro\"],\n",
        "                \"ef_macro_ua\": ef[\"UA\"][\"macro\"],\n",
        "                \"ef_exact_ua\": ef[\"UA\"][\"exact\"],\n",
        "                \"ef_micro_cc\": ef[\"CC\"][\"micro\"],\n",
        "                \"ef_macro_cc\": ef[\"CC\"][\"macro\"],\n",
        "                \"ef_exact_cc\": ef[\"CC\"][\"exact\"],\n",
        "\n",
        "                \"nc_micro_ua\": nc[\"UA\"][\"micro\"],\n",
        "                \"nc_macro_ua\": nc[\"UA\"][\"macro\"],\n",
        "                \"nc_exact_ua\": nc[\"UA\"][\"exact\"],\n",
        "                \"nc_micro_cc\": nc[\"CC\"][\"micro\"],\n",
        "                \"nc_macro_cc\": nc[\"CC\"][\"macro\"],\n",
        "                \"nc_exact_cc\": nc[\"CC\"][\"exact\"]\n",
        "            })\n",
        "            texts_ef = df_test_s1[\"Translated_Text\"].tolist()\n",
        "            texts_nc = df_test_s2[\"Translated_Text\"].tolist()\n",
        "\n",
        "            shap_values_ef = explain_shap(model, tokenizer, texts_ef, \"entity_framing\", max_explain=5)\n",
        "            shap_values_nc = explain_shap(model, tokenizer, texts_nc, \"narrative_classification\", max_explain=5)\n",
        "\n",
        "            save_shap_waterfall_plots(shap_values_nc, \"shap_plots\", \"narrative_classification\", seed, task_type, mlb_s2)\n",
        "            save_shap_waterfall_plots(shap_values_ef, \"shap_plots\", \"entity_framing\", seed, task_type, mlb_s1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # === Save CSV for each domain setting ===\n",
        "    df_out = pd.DataFrame(results)\n",
        "    out_path = f\"new_augmented_seeds/baseline_mtl_more_seeds_augmented_70{domain_str}.csv\"\n",
        "    df_out.to_csv(out_path, index=False)\n",
        "    print(f\" Saved: {out_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHAP functions"
      ],
      "metadata": {
        "id": "YTH4WsLMdgIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MTL SHAP functions"
      ],
      "metadata": {
        "id": "VxPhu81ZO0Ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def truncate_texts(texts, tokenizer, max_len):\n",
        "    truncated = []\n",
        "    for text in texts:\n",
        "        tokens = tokenizer.encode(text, truncation=True, max_length=max_len)\n",
        "        truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "        truncated.append(truncated_text)\n",
        "    return truncated\n",
        "\n",
        "# Build SHAP Explainer for MTL\n",
        "def get_shap_explainer(model, tokenizer, task_name, model_type=\"entity_framing\"):\n",
        "    def forward_func(inputs):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, task=task_name)\n",
        "            if isinstance(outputs, (tuple, list)):\n",
        "                logits = outputs[0]\n",
        "            else:\n",
        "                logits = outputs\n",
        "            return torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "    class Wrapper:\n",
        "        def __call__(self, text):\n",
        "            if isinstance(text, str):\n",
        "                text = [text]\n",
        "            elif isinstance(text, np.ndarray):\n",
        "                text = text.tolist()\n",
        "            elif isinstance(text, list) and isinstance(text[0], np.ndarray):\n",
        "                text = [str(t) for t in text]\n",
        "\n",
        "            encoded = tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=MAX_LEN\n",
        "            ).to(device)\n",
        "            return forward_func(encoded)\n",
        "\n",
        "    return shap.Explainer(Wrapper(), tokenizer, algorithm=\"permutation\")\n",
        "\n",
        "# run analysis\n",
        "def explain_shap(model, tokenizer, texts, task_name, model_type=\"entity_framing\", max_explain=5, visualize=False):\n",
        "    texts = truncate_texts(texts[:max_explain], tokenizer, MAX_LEN)\n",
        "    explainer = get_shap_explainer(model, tokenizer, task_name, model_type)\n",
        "    shap_values = explainer(texts, max_evals=1500, silent=True)\n",
        "    if visualize:\n",
        "        shap.plots.text(shap_values)\n",
        "    return shap_values\n",
        "\n",
        "\n",
        "# save to PNG\n",
        "\n",
        "def save_shap_waterfall_plots(shap_values, output_dir, task_name, seed, model_type, mlb, top_k=3):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import os\n",
        "    from shap import Explanation\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for i, sv in enumerate(shap_values):\n",
        "        try:\n",
        "            mean_abs = np.abs(sv.values).mean(axis=0)\n",
        "            top_outputs = np.argsort(mean_abs)[-top_k:]\n",
        "\n",
        "            for j in top_outputs:\n",
        "                try:\n",
        "                    # Create SHAP Explanation for one label\n",
        "                    single_sv = Explanation(\n",
        "                        values=sv.values[:, j],\n",
        "                        base_values=sv.base_values[j] if hasattr(sv.base_values, '__len__') else sv.base_values,\n",
        "                        data=sv.data,\n",
        "                        feature_names=sv.feature_names\n",
        "                    )\n",
        "\n",
        "                    # Get label name from mlb\n",
        "                    label_name = mlb.classes_[j].replace(\" \", \"_\")  # safer for filenames\n",
        "\n",
        "                    # Generate plot and save\n",
        "                    ax = shap.plots.waterfall(single_sv, show=False)\n",
        "                    fig = ax.figure\n",
        "                    fname = os.path.join(\n",
        "                        output_dir, f\"{task_name}_{model_type}_seed{seed}_sample{i}_label_{label_name}.png\"\n",
        "                    )\n",
        "                    fig.savefig(fname, bbox_inches=\"tight\")\n",
        "                    plt.close(fig)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  [!] Skipped sample {i}, label {j}  {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[!] Error processing sample {i}  {e}\")\n"
      ],
      "metadata": {
        "id": "gnsbhfcvtBQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STL SHAP functions"
      ],
      "metadata": {
        "id": "QmGekXwEO36S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from shap import Explanation\n",
        "\n",
        "\n",
        "def truncate_texts(texts, tokenizer, max_len):\n",
        "    truncated = []\n",
        "    for text in texts:\n",
        "        tokens = tokenizer.encode(text, truncation=True, max_length=max_len)\n",
        "        truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "        truncated.append(truncated_text)\n",
        "    return truncated\n",
        "\n",
        "# Build SHAP Explainer for STL\n",
        "def get_shap_explainer(model, tokenizer):\n",
        "    def forward_func(inputs):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            if hasattr(outputs, 'logits'):\n",
        "                logits = outputs.logits\n",
        "            elif isinstance(outputs, (tuple, list)):\n",
        "                logits = outputs[0]\n",
        "            else:\n",
        "                logits = outputs\n",
        "            return torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "    class Wrapper:\n",
        "        def __call__(self, text):\n",
        "            if isinstance(text, str):\n",
        "                text = [text]\n",
        "            elif isinstance(text, np.ndarray):\n",
        "                text = text.tolist()\n",
        "            elif isinstance(text, list) and isinstance(text[0], np.ndarray):\n",
        "                text = [str(t) for t in text]\n",
        "\n",
        "            encoded = tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=MAX_LEN\n",
        "            ).to(device)\n",
        "            return forward_func(encoded)\n",
        "\n",
        "    return shap.Explainer(Wrapper(), tokenizer, algorithm=\"permutation\")\n",
        "\n",
        "# run analysis\n",
        "def explain_shap(model, tokenizer, texts, max_explain=5, visualize=False):\n",
        "    texts = truncate_texts(texts[:max_explain], tokenizer, MAX_LEN)\n",
        "    explainer = get_shap_explainer(model, tokenizer)\n",
        "    shap_values = explainer(texts, max_evals=1500, silent=True)\n",
        "    if visualize:\n",
        "        shap.plots.text(shap_values)\n",
        "    return shap_values\n",
        "\n",
        "# save to PNG\n",
        "def save_shap_waterfall_plots(shap_values, output_dir, task_name, seed, model_type, mlb, top_k=3):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for i, sv in enumerate(shap_values):\n",
        "        try:\n",
        "            mean_abs = np.abs(sv.values).mean(axis=0)\n",
        "            top_outputs = np.argsort(mean_abs)[-top_k:]\n",
        "\n",
        "            for j in top_outputs:\n",
        "                try:\n",
        "                    single_sv = Explanation(\n",
        "                        values=sv.values[:, j],\n",
        "                        base_values=sv.base_values[j] if hasattr(sv.base_values, '__len__') else sv.base_values,\n",
        "                        data=sv.data,\n",
        "                        feature_names=sv.feature_names\n",
        "                    )\n",
        "\n",
        "                    label_name = mlb.classes_[j].replace(\" \", \"_\")\n",
        "\n",
        "                    ax = shap.plots.waterfall(single_sv, show=False)\n",
        "                    fig = ax.figure\n",
        "                    fname = os.path.join(\n",
        "                        output_dir, f\"{task_name}_{model_type}_seed{seed}_sample{i}_label_{label_name}.png\"\n",
        "                    )\n",
        "                    fig.savefig(fname, bbox_inches=\"tight\")\n",
        "                    plt.close(fig)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  [!] Skipped sample {i}, label {j}  {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[!] Error processing sample {i}  {e}\")\n"
      ],
      "metadata": {
        "id": "cX8LiIz4O2tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8CVgp5Vr3gL"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TWDLafBZr3gL",
        "outputId": "a61597ad-57cb-41c5-d2fc-114d55c6e95d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-10 10:29:04,051] A new study created in memory with name: no-name-efb61a5a-df9a-4bc7-a131-8da529d3017d\n",
            "\n",
            " Starting Optuna Study  Setup: MTL_ADAPTER | Task: MTL | Encoder: roberta-base\n",
            "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fca33dea170>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/toolkit-cache/0.2.16/python3.10/kernel-libs/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
            "    def _clean_thread_parent_frames(\n",
            "KeyboardInterrupt: \n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Starting Epoch 1/2...\n",
            "[W 2025-05-10 10:29:12,061] Trial 0 failed with parameters: {'learning_rate': 2.5620923423875518e-05, 'batch_size': 8, 'epochs': 2, 'threshold': 0.2050096184706236} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"/tmp/ipykernel_280/1264774272.py\", line 36, in <lambda>\n",
            "    study.optimize(lambda trial: objective_mtl_adapter(trial, model_name=encoder), n_trials=3)\n",
            "  File \"/root/work/merged_optuna_script.py\", line 166, in objective_mtl_adapter\n",
            "    train_mtl_flat(\n",
            "  File \"/root/work/multi_task.py\", line 68, in train_mtl_flat\n",
            "    loss.backward()\n",
            "  File \"/root/venv/lib/python3.10/site-packages/torch/_tensor.py\", line 648, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/root/venv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 353, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/root/venv/lib/python3.10/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "[W 2025-05-10 10:29:12,277] Trial 0 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective_mtl(trial, model_name\u001b[38;5;241m=\u001b[39mencoder), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m setup \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmtl_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective_mtl_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown setup: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msetup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
            "Cell \u001b[0;32mIn[2], line 36\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     34\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective_mtl(trial, model_name\u001b[38;5;241m=\u001b[39mencoder), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m setup \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmtl_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 36\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective_mtl_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown setup: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msetup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/work/merged_optuna_script.py:166\u001b[0m, in \u001b[0;36mobjective_mtl_adapter\u001b[0;34m(trial, model_name)\u001b[0m\n\u001b[1;32m    163\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    164\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[0;32m--> 166\u001b[0m \u001b[43mtrain_mtl_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnarrative_classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_s2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity_framing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_s1\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnarrative_classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader_s2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val_s2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_s2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlb_s2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity_framing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader_s1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val_s1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_s1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlb_s1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnarrative_classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlb_s2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity_framing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlb_s1\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_domain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_domain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m results \u001b[38;5;241m=\u001b[39m eval_util\u001b[38;5;241m.\u001b[39mevaluate_mtl_all_tasks(\n\u001b[1;32m    189\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    190\u001b[0m     task_loaders\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     load_from_disk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    209\u001b[0m )\n\u001b[1;32m    211\u001b[0m f1s \u001b[38;5;241m=\u001b[39m [v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mvalues()]\n",
            "File \u001b[0;32m~/work/multi_task.py:68\u001b[0m, in \u001b[0;36mtrain_mtl_flat\u001b[0;34m(model, loaders, val_data, mlbs, optimizer, criterion, device, epochs, train_domain, test_domain)\u001b[0m\n\u001b[1;32m     65\u001b[0m     task_loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     66\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m task_loss\n\u001b[0;32m---> 68\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     70\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\".\")  # Ensure current directory is in path\n",
        "\n",
        "from merged_optuna_script import objective_stl, objective_mtl, objective_mtl_adapter\n",
        "import optuna\n",
        "import pandas as pd\n",
        "\n",
        "# === Fast Experiment Sweep ===\n",
        "EXPERIMENTS = [\n",
        "    {\"setup\": \"stl\", \"task\": \"entity_framing\", \"encoder\": \"roberta-base\"},\n",
        "    {\"setup\": \"stl\", \"task\": \"narrative_classification\", \"encoder\": \"roberta-base\"},\n",
        "    {\"setup\": \"mtl\", \"task\": None, \"encoder\": \"roberta-base\"},\n",
        "    {\"setup\": \"stl\", \"task\": \"entity_framing\", \"encoder\": \"distilbert-base-uncased\"},\n",
        "    {\"setup\": \"stl\", \"task\": \"narrative_classification\", \"encoder\": \"distilbert-base-uncased\"},\n",
        "    {\"setup\": \"mtl\", \"task\": None, \"encoder\": \"distilbert-base-uncased\"},\n",
        "    {\"setup\": \"mtl_adapter\", \"task\": None, \"encoder\": \"roberta-base\"},\n",
        "    {\"setup\": \"mtl_adapter\", \"task\": None, \"encoder\": \"distilbert-base-uncased\"},\n",
        "]\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for config in EXPERIMENTS:\n",
        "    setup = config[\"setup\"]\n",
        "    task = config[\"task\"]\n",
        "    encoder = config[\"encoder\"]\n",
        "\n",
        "    print(f\"\\n Starting Optuna Study  Setup: {setup.upper()} | Task: {task or 'MTL'} | Encoder: {encoder}\")\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "\n",
        "    if setup == \"stl\":\n",
        "        study.optimize(lambda trial: objective_stl(trial, task_type=task, model_name=encoder), n_trials=3)\n",
        "    elif setup == \"mtl\":\n",
        "        study.optimize(lambda trial: objective_mtl(trial, model_name=encoder), n_trials=3)\n",
        "    elif setup == \"mtl_adapter\":\n",
        "        study.optimize(lambda trial: objective_mtl_adapter(trial, model_name=encoder), n_trials=3)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown setup: {setup}\")\n",
        "\n",
        "    best_params = study.best_trial.params\n",
        "    best_score = study.best_trial.value\n",
        "\n",
        "    print(f\"\\n Best hyperparameters for {setup.upper()} | {task or 'MTL'} | {encoder}:\")\n",
        "    for k, v in best_params.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "    print(f\"  score: {best_score:.4f}\")\n",
        "\n",
        "    all_results.append({\n",
        "        \"setup\": setup,\n",
        "        \"task\": task or \"mtl\",\n",
        "        \"encoder\": encoder,\n",
        "        \"score\": best_score,\n",
        "        **best_params\n",
        "    })\n",
        "\n",
        "# === Save results ===\n",
        "df = pd.DataFrame(all_results)\n",
        "df.to_csv(\"optuna_quick_sweep_results_adapter.csv\", index=False)\n",
        "print(\"\\n Saved results to optuna_quick_sweep_results.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old Experiments [obsolete]"
      ],
      "metadata": {
        "id": "YroYuog3dIbo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jCiegG7r3gN"
      },
      "source": [
        "## Control Panel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u30gpB1nr3gO"
      },
      "outputs": [],
      "source": [
        "# Choose a task for the pipeline below: \"narrative_classification\" or \"entity_framing\" or \"multi_task\" or \"multi_task_adapter\"\n",
        "TASK = \"entity_framing\" # or \"entity_framing\" or \"multi_task\" or \"multi_task_adapter\n",
        "\n",
        "# select domains for training and testing: \"UA\"; \"CC\"; \"UA\", \"CC\";\n",
        "TRAIN_DOMAIN = [\"UA\",\"CC\"]\n",
        "TEST_DOMAIN = [\"UA\", \"CC\"] # The test data comes from a separate dataset.\n",
        "# The test data is always the same regardless of the domain we choose to train on. This is for consistency.\n",
        "\n",
        "# select languages for training and testing: \"ALL\";\"EN\";\"HI\";\"BG\";\"RU\";\"PT\"\n",
        "TRAIN_LANGUAGES = [\"ALL\"]\n",
        "TEST_LANGUAGES = [\"ALL\"]\n",
        "\n",
        "# Taxonomy Depth\n",
        "TAXONOMY_DEPTH = \"COARSE\" # \"COARSE\" OR \"FINE\"\n",
        "\n",
        "# Classifier Complexity\n",
        "CLASSIFIER_COMPLEXITY = \"FLAT\" # \"FLAT\" OR \"HIERARCHICAL\"\n",
        "\n",
        "# change the training hyperparameters here\n",
        "MODEL_NAME = \"distilbert-base-uncased\" # OR  \"distilbert-base-uncased\" \"roberta-base\" \"\"FacebookAI/roberta-base\"\"\n",
        "MAX_LEN = 512\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 3e-5\n",
        "MODEL_PATH = f\"{TASK}_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\" # -- to save the model later\n",
        "\n",
        "#tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
        "#tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "\n",
        "# debug mode -- reduced samples\n",
        "DEBUG_MODE = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jT1qbd2r3gO"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ippVT4Air3gO"
      },
      "source": [
        "## UTILS Assemble Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUl9l80Yr3gO"
      },
      "outputs": [],
      "source": [
        "if TASK != \"multi_task\" and TASK != \"multi_task_adapter\":\n",
        "    if TAXONOMY_DEPTH == 'FINE':\n",
        "        if CLASSIFIER_COMPLEXITY == 'FLAT':\n",
        "            df_train, df_val, df_test, y_train, y_val, y_test, mlb, TEXT_COL, LABEL_COL = prepare_data_STL_fine(\n",
        "                TASK,\n",
        "                TRAIN_DOMAIN,\n",
        "                TEST_DOMAIN,\n",
        "            )\n",
        "        elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n",
        "            df_train, df_val, df_test, y_train, y_val, y_test, mlb, TEXT_COL, LABEL_COL, child_to_parent, label_to_index = prepare_data_STL_hierarchical(\n",
        "                TASK,\n",
        "                TRAIN_DOMAIN,\n",
        "                TEST_DOMAIN,\n",
        "            )\n",
        "\n",
        "\n",
        "    elif TAXONOMY_DEPTH == 'COARSE':\n",
        "        df_train, df_val, df_test, y_train, y_val, y_test, mlb, TEXT_COL, LABEL_COL = prepare_data_STL_coarse(\n",
        "                TASK,\n",
        "                TRAIN_DOMAIN,\n",
        "                TEST_DOMAIN,\n",
        "            )\n",
        "\n",
        "    train_dataset = MultiLabelDataset(df_train[TEXT_COL].tolist(), y_train, tokenizer, MAX_LEN)\n",
        "    val_dataset = MultiLabelDataset(df_val[TEXT_COL].tolist(), y_val, tokenizer, MAX_LEN)\n",
        "    test_dataset = MultiLabelDataset(df_test[TEXT_COL].tolist(), y_test, tokenizer, MAX_LEN)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "    num_classes = len(mlb.classes_)\n",
        "\n",
        "\n",
        "elif TASK == \"multi_task\" or TASK == \"multi_task_adapter\":\n",
        "\n",
        "    if TAXONOMY_DEPTH == 'FINE':\n",
        "\n",
        "        if CLASSIFIER_COMPLEXITY == 'FLAT':\n",
        "            (\n",
        "                df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n",
        "                df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n",
        "                train_loader_s1, val_loader_s1, test_loader_s1,\n",
        "                train_loader_s2, val_loader_s2, test_loader_s2,\n",
        "                num_classes_dict\n",
        "            ) = prepare_data_MTL_fine_flat(\n",
        "                TASK,\n",
        "                train_domains=TRAIN_DOMAIN,\n",
        "                test_domains=TEST_DOMAIN,\n",
        "                train_languages=TRAIN_LANGUAGES,\n",
        "                model_name=MODEL_NAME,\n",
        "                max_len=MAX_LEN,\n",
        "                batch_size=BATCH_SIZE\n",
        "            )\n",
        "\n",
        "        elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n",
        "            (\n",
        "                df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n",
        "                df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n",
        "                train_loader_s1, val_loader_s1, test_loader_s1,\n",
        "                train_loader_s2, val_loader_s2, test_loader_s2,\n",
        "                num_classes_dict,\n",
        "                child_to_parent_map,\n",
        "                label_to_index_map\n",
        "            ) = prepare_data_MTL_hierarchical(\n",
        "                TASK,\n",
        "                train_domains=TRAIN_DOMAIN,\n",
        "                test_domains=TEST_DOMAIN,\n",
        "                train_languages=TRAIN_LANGUAGES,\n",
        "                model_name=MODEL_NAME,\n",
        "                max_len=MAX_LEN,\n",
        "                batch_size=BATCH_SIZE\n",
        "            )\n",
        "\n",
        "    elif TAXONOMY_DEPTH == 'COARSE':\n",
        "        (\n",
        "            df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n",
        "            df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n",
        "            train_loader_s1, val_loader_s1, test_loader_s1,\n",
        "            train_loader_s2, val_loader_s2, test_loader_s2,\n",
        "            num_classes_dict\n",
        "        ) = prepare_data_MTL_coarse(\n",
        "            TASK,\n",
        "            train_domains=TRAIN_DOMAIN,\n",
        "            test_domains=TEST_DOMAIN,\n",
        "            train_languages=TRAIN_LANGUAGES,\n",
        "            model_name=MODEL_NAME,\n",
        "            max_len=MAX_LEN,\n",
        "            batch_size=BATCH_SIZE\n",
        "        )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ivW39mKr3gP"
      },
      "outputs": [],
      "source": [
        "# Fine-Fine\n",
        "prepare_data_MTL_mixed(..., \"fine\", \"fine\")\n",
        "\n",
        "# Fine-Coarse\n",
        "prepare_data_MTL_mixed(..., \"fine\", \"coarse\")\n",
        "\n",
        "# Coarse-Fine\n",
        "prepare_data_MTL_mixed(..., \"coarse\", \"fine\")\n",
        "\n",
        "# Coarse-Coarse\n",
        "prepare_data_MTL_mixed(..., \"coarse\", \"coarse\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l26Y0sEEtBk6",
        "outputId": "043ff7ab-ec3e-4cb7-b1a3-a84497405706"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['URW: Amplifying war-related fears', 'URW: Blaming the war on others rather than the invader', 'URW: Discrediting Ukraine', 'URW: Distrust towards Media', 'URW: Hidden plots by secret schemes of powerful groups', 'URW: Negative Consequences for the West', 'URW: Other', 'URW: Overpraising the West', 'URW: Russia is the Victim', 'URW: Speculating war outcomes'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['Spy', 'Underdog'] will be ignored\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "(\n",
        "    df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n",
        "    df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n",
        "    train_loader_s1, val_loader_s1, test_loader_s1,\n",
        "    train_loader_s2, val_loader_s2, test_loader_s2,\n",
        "    num_classes_dict\n",
        ") = prepare_data_MTL_mixed(\n",
        "    TASK,\n",
        "    train_domains=TRAIN_DOMAIN,\n",
        "    test_domains=TEST_DOMAIN,\n",
        "    train_languages=TRAIN_LANGUAGES,\n",
        "    model_name=MODEL_NAME,\n",
        "    max_len=MAX_LEN,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    granularity_s1=\"coarse\",\n",
        "    granularity_s2=\"fine\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW4sjS0er3gP"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ce48f9ecbcc04fe5b668db2318658637",
            "3a470f63f73745f695067ec4343da4d9",
            "810fcaf7fb8149f884783245d393000a",
            "13719eb8a4e14a609a6227cfa5bc7494",
            "bb1ec30603664534b3b6ea58a73ee334",
            "2d45de785c314f10b1eb458fcb521e3f",
            "10ab6181deac4a48b729313ab36408f6",
            "62b25d3c344f46e38142f63cbb648db9",
            "b5fc87d570c1434ca7c6c017e9e2821d",
            "83d89ed09c4145ebacc9c74bf59bf4a2",
            "2da1d143ae2e476ca3426588c313d41f",
            "2c9a72ad6ff84d5cbaa04d822af7488c",
            "a4d07ba4fd224163bfb10ab488d886b0",
            "59703786b70e4520a75e7eb397581b60",
            "c309c4d9623d467daf8f1f3a0dc28215",
            "e8afcccc659a4b0cba93f59cf4e5c0ff",
            "16ea308ad2a943f6a39f2d60dce49b09",
            "ea10286b33bd4205881bfac62c6cbf11",
            "729cac118a9a44359d6552f0d4c179b7",
            "246abede770943fd8378bfa9eac27ac4",
            "51a96d630662436c94aa0edc3e264278",
            "96f21f82a80f47849e648b38031bd5ec",
            "4147fd2351f648abb8f38a389c549471",
            "a0d0fbd9f64d46c5b100a6d0f25d470d",
            "af9373733fb44989b58a6a133b312ae6",
            "50e39ad24022419e93a8ac46045f5c27",
            "09b163dcceaa48b3bac4afa392947e1d",
            "328cc31e869f4257a671903882f8d9e6",
            "ba101f4e071c439db3db8b62b68fcc71",
            "149169ae34ea4af8970d7e30ee2d6beb",
            "d47ab654488c4ffbb9a1f163e27f4ec2",
            "7bdd2299145743ffbe96b7289737f554",
            "449d0e5b6d2a4f30a6c1311d4749f973"
          ]
        },
        "id": "bFAfrlWQr3gP",
        "outputId": "ac20bbc6-215d-4075-f823-26d7f3c85e56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> Running Single-Task (no adapter) Model <<<\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce48f9ecbcc04fe5b668db2318658637",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c9a72ad6ff84d5cbaa04d822af7488c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4147fd2351f648abb8f38a389c549471",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|| 671/671 [02:02<00:00,  5.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: Loss = 0.5614\n",
            "Threshold sweep results:\n",
            "Thresh 0.10 | Macro F1: 0.525 | Micro F1: 0.546 | Exact Match: 0.047\n",
            "Thresh 0.15 | Macro F1: 0.583 | Micro F1: 0.614 | Exact Match: 0.243\n",
            "Thresh 0.20 | Macro F1: 0.595 | Micro F1: 0.636 | Exact Match: 0.406\n",
            "Thresh 0.25 | Macro F1: 0.589 | Micro F1: 0.643 | Exact Match: 0.504\n",
            "Thresh 0.30 | Macro F1: 0.561 | Micro F1: 0.635 | Exact Match: 0.556\n",
            "Thresh 0.35 | Macro F1: 0.529 | Micro F1: 0.625 | Exact Match: 0.581\n",
            "Thresh 0.40 | Macro F1: 0.499 | Micro F1: 0.618 | Exact Match: 0.598\n",
            "Thresh 0.45 | Macro F1: 0.482 | Micro F1: 0.613 | Exact Match: 0.596\n",
            "Thresh 0.50 | Macro F1: 0.464 | Micro F1: 0.602 | Exact Match: 0.567\n",
            "Thresh 0.55 | Macro F1: 0.445 | Micro F1: 0.593 | Exact Match: 0.540\n",
            "Thresh 0.60 | Macro F1: 0.417 | Micro F1: 0.571 | Exact Match: 0.494\n",
            "Thresh 0.65 | Macro F1: 0.393 | Micro F1: 0.542 | Exact Match: 0.440\n",
            "Thresh 0.70 | Macro F1: 0.336 | Micro F1: 0.469 | Exact Match: 0.341\n",
            "Thresh 0.75 | Macro F1: 0.268 | Micro F1: 0.349 | Exact Match: 0.221\n",
            "Thresh 0.80 | Macro F1: 0.184 | Micro F1: 0.207 | Exact Match: 0.117\n",
            "Thresh 0.85 | Macro F1: 0.029 | Micro F1: 0.019 | Exact Match: 0.010\n",
            "\n",
            " Best threshold = 0.20 with Macro F1 = 0.595\n",
            "Validation Macro F1 (Epoch 1): 0.5952\n",
            "Saved best model (Epoch 1) to entity_framing_UA-CC_to_UA-CC.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|| 671/671 [02:02<00:00,  5.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2: Loss = 0.4726\n",
            "Threshold sweep results:\n",
            "Thresh 0.10 | Macro F1: 0.671 | Micro F1: 0.671 | Exact Match: 0.316\n",
            "Thresh 0.15 | Macro F1: 0.714 | Micro F1: 0.716 | Exact Match: 0.453\n",
            "Thresh 0.20 | Macro F1: 0.739 | Micro F1: 0.747 | Exact Match: 0.542\n",
            "Thresh 0.25 | Macro F1: 0.752 | Micro F1: 0.766 | Exact Match: 0.612\n",
            "Thresh 0.30 | Macro F1: 0.753 | Micro F1: 0.777 | Exact Match: 0.666\n",
            "Thresh 0.35 | Macro F1: 0.757 | Micro F1: 0.787 | Exact Match: 0.716\n",
            "Thresh 0.40 | Macro F1: 0.756 | Micro F1: 0.794 | Exact Match: 0.748\n",
            "Thresh 0.45 | Macro F1: 0.757 | Micro F1: 0.800 | Exact Match: 0.766\n",
            "Thresh 0.50 | Macro F1: 0.749 | Micro F1: 0.797 | Exact Match: 0.757\n",
            "Thresh 0.55 | Macro F1: 0.739 | Micro F1: 0.789 | Exact Match: 0.726\n",
            "Thresh 0.60 | Macro F1: 0.722 | Micro F1: 0.777 | Exact Match: 0.693\n",
            "Thresh 0.65 | Macro F1: 0.696 | Micro F1: 0.751 | Exact Match: 0.643\n",
            "Thresh 0.70 | Macro F1: 0.673 | Micro F1: 0.727 | Exact Match: 0.602\n",
            "Thresh 0.75 | Macro F1: 0.645 | Micro F1: 0.697 | Exact Match: 0.556\n",
            "Thresh 0.80 | Macro F1: 0.603 | Micro F1: 0.647 | Exact Match: 0.489\n",
            "Thresh 0.85 | Macro F1: 0.541 | Micro F1: 0.572 | Exact Match: 0.406\n",
            "\n",
            " Best threshold = 0.35 with Macro F1 = 0.757\n",
            "Validation Macro F1 (Epoch 2): 0.7575\n",
            "Saved best model (Epoch 2) to entity_framing_UA-CC_to_UA-CC.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|| 671/671 [02:02<00:00,  5.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3: Loss = 0.3479\n",
            "Threshold sweep results:\n",
            "Thresh 0.10 | Macro F1: 0.750 | Micro F1: 0.754 | Exact Match: 0.529\n",
            "Thresh 0.15 | Macro F1: 0.794 | Micro F1: 0.796 | Exact Match: 0.622\n",
            "Thresh 0.20 | Macro F1: 0.818 | Micro F1: 0.823 | Exact Match: 0.686\n",
            "Thresh 0.25 | Macro F1: 0.842 | Micro F1: 0.847 | Exact Match: 0.742\n",
            "Thresh 0.30 | Macro F1: 0.855 | Micro F1: 0.862 | Exact Match: 0.789\n",
            "Thresh 0.35 | Macro F1: 0.867 | Micro F1: 0.875 | Exact Match: 0.824\n",
            "Thresh 0.40 | Macro F1: 0.872 | Micro F1: 0.884 | Exact Match: 0.850\n",
            "Thresh 0.45 | Macro F1: 0.871 | Micro F1: 0.885 | Exact Match: 0.854\n",
            "Thresh 0.50 | Macro F1: 0.870 | Micro F1: 0.884 | Exact Match: 0.848\n",
            "Thresh 0.55 | Macro F1: 0.865 | Micro F1: 0.878 | Exact Match: 0.828\n",
            "Thresh 0.60 | Macro F1: 0.855 | Micro F1: 0.867 | Exact Match: 0.800\n",
            "Thresh 0.65 | Macro F1: 0.837 | Micro F1: 0.851 | Exact Match: 0.766\n",
            "Thresh 0.70 | Macro F1: 0.824 | Micro F1: 0.838 | Exact Match: 0.741\n",
            "Thresh 0.75 | Macro F1: 0.797 | Micro F1: 0.811 | Exact Match: 0.697\n",
            "Thresh 0.80 | Macro F1: 0.768 | Micro F1: 0.780 | Exact Match: 0.650\n",
            "Thresh 0.85 | Macro F1: 0.727 | Micro F1: 0.735 | Exact Match: 0.587\n",
            "\n",
            " Best threshold = 0.40 with Macro F1 = 0.872\n",
            "Validation Macro F1 (Epoch 3): 0.8721\n",
            "Saved best model (Epoch 3) to entity_framing_UA-CC_to_UA-CC.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|| 671/671 [02:02<00:00,  5.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4: Loss = 0.2459\n",
            "Threshold sweep results:\n",
            "Thresh 0.10 | Macro F1: 0.817 | Micro F1: 0.834 | Exact Match: 0.743\n",
            "Thresh 0.15 | Macro F1: 0.861 | Micro F1: 0.873 | Exact Match: 0.803\n",
            "Thresh 0.20 | Macro F1: 0.889 | Micro F1: 0.898 | Exact Match: 0.842\n",
            "Thresh 0.25 | Macro F1: 0.904 | Micro F1: 0.911 | Exact Match: 0.867\n",
            "Thresh 0.30 | Macro F1: 0.917 | Micro F1: 0.923 | Exact Match: 0.890\n",
            "Thresh 0.35 | Macro F1: 0.924 | Micro F1: 0.931 | Exact Match: 0.909\n",
            "Thresh 0.40 | Macro F1: 0.927 | Micro F1: 0.935 | Exact Match: 0.923\n",
            "Thresh 0.45 | Macro F1: 0.926 | Micro F1: 0.934 | Exact Match: 0.919\n",
            "Thresh 0.50 | Macro F1: 0.924 | Micro F1: 0.932 | Exact Match: 0.911\n",
            "Thresh 0.55 | Macro F1: 0.920 | Micro F1: 0.928 | Exact Match: 0.899\n",
            "Thresh 0.60 | Macro F1: 0.912 | Micro F1: 0.923 | Exact Match: 0.881\n",
            "Thresh 0.65 | Macro F1: 0.905 | Micro F1: 0.915 | Exact Match: 0.863\n",
            "Thresh 0.70 | Macro F1: 0.898 | Micro F1: 0.909 | Exact Match: 0.848\n",
            "Thresh 0.75 | Macro F1: 0.890 | Micro F1: 0.900 | Exact Match: 0.829\n",
            "Thresh 0.80 | Macro F1: 0.868 | Micro F1: 0.881 | Exact Match: 0.796\n",
            "Thresh 0.85 | Macro F1: 0.842 | Micro F1: 0.858 | Exact Match: 0.757\n",
            "\n",
            " Best threshold = 0.40 with Macro F1 = 0.927\n",
            "Validation Macro F1 (Epoch 4): 0.9267\n",
            "Saved best model (Epoch 4) to entity_framing_UA-CC_to_UA-CC.pt\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if TASK != \"multi_task\" and TASK != \"multi_task_adapter\":\n",
        "    print(\"\\n>>> Running Single-Task (no adapter) Model <<<\")\n",
        "    model = TransformerClassifier(MODEL_NAME, num_classes).to(device)\n",
        "\n",
        "    if CLASSIFIER_COMPLEXITY == 'FLAT':\n",
        "        trained_model = train_single_task_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            y_val=y_val,\n",
        "            MODEL_PATH=MODEL_PATH,\n",
        "            LEARNING_RATE=LEARNING_RATE,\n",
        "            EPOCHS=EPOCHS,\n",
        "            device=device,\n",
        "            predict_proba=eval_util.predict_proba,\n",
        "            evaluate_threshold_sweep=eval_util.evaluate_threshold_sweep\n",
        "        )\n",
        "        trained_model.load_state_dict(torch.load(MODEL_PATH))\n",
        "        trained_model.to(device)\n",
        "\n",
        "    elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n",
        "        trained_model = train_hierarchical_classifier(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            y_val=y_val,\n",
        "            MODEL_PATH=MODEL_PATH,\n",
        "            child_to_parent=child_to_parent,\n",
        "            label_to_index=label_to_index,\n",
        "            predict_proba=eval_util.predict_proba,\n",
        "            evaluate_threshold_sweep=eval_util.evaluate_threshold_sweep,\n",
        "            LEARNING_RATE=LEARNING_RATE,\n",
        "            EPOCHS=EPOCHS\n",
        "        )\n",
        "        trained_model.load_state_dict(torch.load(MODEL_PATH))\n",
        "        trained_model.to(device)\n",
        "\n",
        "elif TASK == \"multi_task\":\n",
        "    print(\"\\n>>> Running Multi-Task (no adapter) Model <<<\")\n",
        "    task_classes = {\n",
        "        \"narrative_classification\": y_train_s2.shape[1],\n",
        "        \"entity_framing\": y_train_s1.shape[1]\n",
        "    }\n",
        "    model = MultiTaskTransformer(MODEL_NAME, task_classes).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    if CLASSIFIER_COMPLEXITY == 'FLAT':\n",
        "        train_mtl_flat(\n",
        "            model=model,\n",
        "            loaders={\n",
        "                \"narrative_classification\": train_loader_s2,\n",
        "                \"entity_framing\": train_loader_s1\n",
        "            },\n",
        "            val_data={\n",
        "                \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2),\n",
        "                \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1)\n",
        "            },\n",
        "            mlbs={\n",
        "                \"narrative_classification\": mlb_s2,\n",
        "                \"entity_framing\": mlb_s1\n",
        "            },\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            device=device,\n",
        "            epochs=EPOCHS,\n",
        "            train_domain=TRAIN_DOMAIN,\n",
        "            test_domain=TEST_DOMAIN\n",
        "        )\n",
        "\n",
        "    elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n",
        "        train_mtl_hierarchical(\n",
        "            model=model,\n",
        "            loaders={\n",
        "                \"narrative_classification\": train_loader_s2,\n",
        "                \"entity_framing\": train_loader_s1\n",
        "            },\n",
        "            val_data={\n",
        "                \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2),\n",
        "                \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1)\n",
        "            },\n",
        "            child_to_parent_map=child_to_parent_map,\n",
        "            label_to_index_map=label_to_index_map,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            device=device,\n",
        "            epochs=EPOCHS,\n",
        "            train_domain=TRAIN_DOMAIN,\n",
        "            test_domain=TEST_DOMAIN\n",
        "        )\n",
        "\n",
        "\n",
        "    # Re-load best saved model per task\n",
        "    model.load_state_dict(torch.load(f\"entity_framing_MTL_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\"), strict=False)\n",
        "    model.load_state_dict(torch.load(f\"narrative_classification_MTL_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\"), strict=False)\n",
        "    trained_model = model\n",
        "\n",
        "\n",
        "elif TASK == \"multi_task_adapter\":\n",
        "    print(\"\\n>>> Running Multi-Task Adapter Model <<<\")\n",
        "\n",
        "    task_classes = {\n",
        "        \"narrative_classification\": y_train_s2.shape[1],\n",
        "        \"entity_framing\": y_train_s1.shape[1]\n",
        "    }\n",
        "\n",
        "    model = AdapterMultiTaskTransformer(\n",
        "        model_name=MODEL_NAME,\n",
        "        num_classes_dict=task_classes,\n",
        "        adapter_dim=128\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    if CLASSIFIER_COMPLEXITY == 'FLAT':\n",
        "        train_mtl_flat(\n",
        "            model=model,\n",
        "            loaders={\n",
        "                \"narrative_classification\": train_loader_s2,\n",
        "                \"entity_framing\": train_loader_s1\n",
        "            },\n",
        "            val_data={\n",
        "                \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2),\n",
        "                \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1)\n",
        "            },\n",
        "            mlbs={\n",
        "                \"narrative_classification\": mlb_s2,\n",
        "                \"entity_framing\": mlb_s1\n",
        "            },\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            device=device,\n",
        "            epochs=EPOCHS,\n",
        "            train_domain=TRAIN_DOMAIN,\n",
        "            test_domain=TEST_DOMAIN\n",
        "        )\n",
        "\n",
        "    # load best saved models\n",
        "    model.load_state_dict(torch.load(f\"entity_framing_MTL_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\"), strict=False)\n",
        "    model.load_state_dict(torch.load(f\"narrative_classification_MTL_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\"), strict=False)\n",
        "    trained_model = model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62l8IF1Lr3gP"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XgxBR-ar3gP",
        "outputId": "96a59504-fb91-488f-9f5e-3d8b2718a24f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Single-Task Model (entity_framing)\n",
            "\n",
            "=========================\n",
            "Validation (Fixed Threshold)\n",
            "=========================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating VALIDATION: 100%|| 287/287 [00:17<00:00, 15.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " VALIDATION (Fixed Threshold=0.35):\n",
            "Macro F1: 0.886\n",
            "Micro F1: 0.898\n",
            "Exact Match: 0.865\n",
            "\n",
            "----------------------------\n",
            "Per-Domain Breakdown\n",
            "----------------------------\n",
            "\n",
            " Domain: UA\n",
            "Macro F1: 0.886\n",
            "Micro F1: 0.898\n",
            "Exact Match: 0.865\n",
            "\n",
            "=========================\n",
            "Test (Fixed Threshold)\n",
            "=========================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating TEST: 100%|| 56/56 [00:03<00:00, 15.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " TEST (Fixed Threshold=0.35):\n",
            "Macro F1: 0.634\n",
            "Micro F1: 0.634\n",
            "Exact Match: 0.562\n",
            "\n",
            "----------------------------\n",
            "Per-Domain Breakdown\n",
            "----------------------------\n",
            "\n",
            " Domain: CC\n",
            "Macro F1: 0.635\n",
            "Micro F1: 0.787\n",
            "Exact Match: 0.780\n",
            "\n",
            " Domain: UA\n",
            "Macro F1: 0.557\n",
            "Micro F1: 0.598\n",
            "Exact Match: 0.507\n",
            "\n",
            "=========================\n",
            "OOD Generalization (Fixed Threshold)\n",
            "=========================\n",
            " Macro F1 (val - test): 0.253\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating TEST: 100%|| 56/56 [00:03<00:00, 15.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TEST (Fixed Threshold=0.35):\n",
            "Macro F1: 0.634\n",
            "Micro F1: 0.634\n",
            "Exact Match: 0.562\n",
            "\n",
            "----------------------------\n",
            "Classification Report (All Domains)\n",
            "----------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Antagonist       0.61      0.71      0.65       168\n",
            "    Innocent       0.59      0.70      0.64       122\n",
            " Protagonist       0.60      0.62      0.61       158\n",
            "\n",
            "   micro avg       0.60      0.67      0.63       448\n",
            "   macro avg       0.60      0.68      0.63       448\n",
            "weighted avg       0.60      0.67      0.63       448\n",
            " samples avg       0.62      0.67      0.64       448\n",
            "\n",
            "\n",
            "----------------------------\n",
            "Per-Domain Breakdown\n",
            "----------------------------\n",
            "\n",
            "Domain: CC\n",
            "Macro F1: 0.635\n",
            "Micro F1: 0.787\n",
            "Exact Match: 0.780\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Antagonist       0.75      0.23      0.35        13\n",
            "    Innocent       0.77      0.98      0.86        60\n",
            " Protagonist       0.91      0.56      0.69        18\n",
            "\n",
            "   micro avg       0.78      0.79      0.79        91\n",
            "   macro avg       0.81      0.59      0.63        91\n",
            "weighted avg       0.79      0.79      0.75        91\n",
            " samples avg       0.79      0.79      0.79        91\n",
            "\n",
            "\n",
            "Domain: UA\n",
            "Macro F1: 0.557\n",
            "Micro F1: 0.598\n",
            "Exact Match: 0.507\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Antagonist       0.60      0.75      0.67       155\n",
            "    Innocent       0.39      0.42      0.40        62\n",
            " Protagonist       0.58      0.63      0.60       140\n",
            "\n",
            "   micro avg       0.56      0.64      0.60       357\n",
            "   macro avg       0.52      0.60      0.56       357\n",
            "weighted avg       0.56      0.64      0.60       357\n",
            " samples avg       0.58      0.64      0.60       357\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# EVALUATION (Single Task)\n",
        "# ==========================\n",
        "if TASK != \"multi_task\" and TASK != \"multi_task_adapter\":\n",
        "    print(f\"\\nEvaluating Single-Task Model ({TASK})\")\n",
        "\n",
        "    if CLASSIFIER_COMPLEXITY == 'FLAT':\n",
        "        results_domain = eval_util.evaluate_per_domain_flat(\n",
        "            trained_model,\n",
        "            val_loader, df_val.reset_index(drop=True),\n",
        "            test_loader, df_test.reset_index(drop=True),\n",
        "            mlb,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        results_class = eval_util.evaluate_per_class_flat(\n",
        "            trained_model,\n",
        "            test_loader,\n",
        "            df_test.reset_index(drop=True),\n",
        "            mlb,\n",
        "            device=device,\n",
        "            label=\"TEST\"\n",
        "        )\n",
        "\n",
        "\n",
        "    elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n",
        "        results_hierarchical = eval_util.evaluate_and_compare_hierarchical(\n",
        "            model=trained_model,\n",
        "            val_loader=val_loader,\n",
        "            val_df=df_val.reset_index(drop=True),\n",
        "            val_targets=y_val,\n",
        "            test_loader=test_loader,\n",
        "            test_df=df_test.reset_index(drop=True),\n",
        "            test_targets=y_test,\n",
        "            mlb=mlb,\n",
        "            device=device,\n",
        "            child_to_parent=child_to_parent,\n",
        "            label_to_index=label_to_index\n",
        "        )\n",
        "\n",
        "# ==========================\n",
        "# EVALUATION (Multi-Task)\n",
        "# ==========================\n",
        "elif TASK == \"multi_task\" or TASK == \"multi_task_adapter\":\n",
        "    print(f\"\\nEvaluating Multi-Task Model ({TASK})\")\n",
        "    if CLASSIFIER_COMPLEXITY == 'FLAT':\n",
        "        task_loaders = {\n",
        "            \"narrative_classification\": test_loader_s2,\n",
        "            \"entity_framing\": test_loader_s1,\n",
        "        }\n",
        "\n",
        "        task_dfs = {\n",
        "            \"narrative_classification\": df_test_s2,\n",
        "            \"entity_framing\": df_test_s1,\n",
        "        }\n",
        "\n",
        "        task_targets = {\n",
        "            \"narrative_classification\": y_test_s2,\n",
        "            \"entity_framing\": y_test_s1,\n",
        "        }\n",
        "\n",
        "        task_mlbs = {\n",
        "            \"narrative_classification\": mlb_s2,\n",
        "            \"entity_framing\": mlb_s1,\n",
        "        }\n",
        "\n",
        "        results_mtl = eval_util.evaluate_mtl_all_tasks(\n",
        "            model=trained_model,\n",
        "            task_loaders=task_loaders,\n",
        "            task_dfs=task_dfs,\n",
        "            task_targets=task_targets,\n",
        "            task_mlbs=task_mlbs,\n",
        "            domain_list=TRAIN_DOMAIN,\n",
        "            device=device,\n",
        "            load_from_disk=False\n",
        "        )\n",
        "\n",
        "\n",
        "    elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n",
        "        eval_util.evaluate_mtl_hierarchical_all_tasks(\n",
        "            model=trained_model,\n",
        "            test_loaders={\n",
        "                \"narrative_classification\": test_loader_s2,\n",
        "                \"entity_framing\": test_loader_s1\n",
        "            },\n",
        "            df_tests={\n",
        "                \"narrative_classification\": df_test_s2,\n",
        "                \"entity_framing\": df_test_s1\n",
        "            },\n",
        "            y_tests={\n",
        "                \"narrative_classification\": y_test_s2,\n",
        "                \"entity_framing\": y_test_s1\n",
        "            },\n",
        "            mlbs={\n",
        "                \"narrative_classification\": mlb_s2,\n",
        "                \"entity_framing\": mlb_s1\n",
        "            },\n",
        "            child_to_parent_map=child_to_parent_map,\n",
        "            label_to_index_map=label_to_index_map,\n",
        "            device=device\n",
        "        )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "collapsed_sections": [
        "YTH4WsLMdgIg"
      ]
    },
    "deepnote_notebook_id": "28c06ed7bf87409192461e9c1e8d8ba3",
    "deepnote_persisted_session": {
      "createdAt": "2025-05-13T16:22:54.164Z"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09b163dcceaa48b3bac4afa392947e1d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10ab6181deac4a48b729313ab36408f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13719eb8a4e14a609a6227cfa5bc7494": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83d89ed09c4145ebacc9c74bf59bf4a2",
            "placeholder": "",
            "style": "IPY_MODEL_2da1d143ae2e476ca3426588c313d41f",
            "value": "483/483[00:00&lt;00:00,60.9kB/s]"
          }
        },
        "149169ae34ea4af8970d7e30ee2d6beb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16ea308ad2a943f6a39f2d60dce49b09": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "246abede770943fd8378bfa9eac27ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c9a72ad6ff84d5cbaa04d822af7488c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4d07ba4fd224163bfb10ab488d886b0",
              "IPY_MODEL_59703786b70e4520a75e7eb397581b60",
              "IPY_MODEL_c309c4d9623d467daf8f1f3a0dc28215"
            ],
            "layout": "IPY_MODEL_e8afcccc659a4b0cba93f59cf4e5c0ff"
          }
        },
        "2d45de785c314f10b1eb458fcb521e3f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2da1d143ae2e476ca3426588c313d41f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "328cc31e869f4257a671903882f8d9e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a470f63f73745f695067ec4343da4d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d45de785c314f10b1eb458fcb521e3f",
            "placeholder": "",
            "style": "IPY_MODEL_10ab6181deac4a48b729313ab36408f6",
            "value": "config.json:100%"
          }
        },
        "4147fd2351f648abb8f38a389c549471": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0d0fbd9f64d46c5b100a6d0f25d470d",
              "IPY_MODEL_af9373733fb44989b58a6a133b312ae6",
              "IPY_MODEL_50e39ad24022419e93a8ac46045f5c27"
            ],
            "layout": "IPY_MODEL_09b163dcceaa48b3bac4afa392947e1d"
          }
        },
        "449d0e5b6d2a4f30a6c1311d4749f973": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50e39ad24022419e93a8ac46045f5c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bdd2299145743ffbe96b7289737f554",
            "placeholder": "",
            "style": "IPY_MODEL_449d0e5b6d2a4f30a6c1311d4749f973",
            "value": "268M/268M[00:01&lt;00:00,51.4MB/s]"
          }
        },
        "51a96d630662436c94aa0edc3e264278": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59703786b70e4520a75e7eb397581b60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_729cac118a9a44359d6552f0d4c179b7",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_246abede770943fd8378bfa9eac27ac4",
            "value": 483
          }
        },
        "62b25d3c344f46e38142f63cbb648db9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "729cac118a9a44359d6552f0d4c179b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bdd2299145743ffbe96b7289737f554": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "810fcaf7fb8149f884783245d393000a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62b25d3c344f46e38142f63cbb648db9",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5fc87d570c1434ca7c6c017e9e2821d",
            "value": 483
          }
        },
        "83d89ed09c4145ebacc9c74bf59bf4a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96f21f82a80f47849e648b38031bd5ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0d0fbd9f64d46c5b100a6d0f25d470d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_328cc31e869f4257a671903882f8d9e6",
            "placeholder": "",
            "style": "IPY_MODEL_ba101f4e071c439db3db8b62b68fcc71",
            "value": "model.safetensors:100%"
          }
        },
        "a4d07ba4fd224163bfb10ab488d886b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16ea308ad2a943f6a39f2d60dce49b09",
            "placeholder": "",
            "style": "IPY_MODEL_ea10286b33bd4205881bfac62c6cbf11",
            "value": "config.json:100%"
          }
        },
        "af9373733fb44989b58a6a133b312ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_149169ae34ea4af8970d7e30ee2d6beb",
            "max": 267954768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d47ab654488c4ffbb9a1f163e27f4ec2",
            "value": 267954768
          }
        },
        "b5fc87d570c1434ca7c6c017e9e2821d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba101f4e071c439db3db8b62b68fcc71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb1ec30603664534b3b6ea58a73ee334": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c309c4d9623d467daf8f1f3a0dc28215": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51a96d630662436c94aa0edc3e264278",
            "placeholder": "",
            "style": "IPY_MODEL_96f21f82a80f47849e648b38031bd5ec",
            "value": "483/483[00:00&lt;00:00,63.9kB/s]"
          }
        },
        "ce48f9ecbcc04fe5b668db2318658637": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a470f63f73745f695067ec4343da4d9",
              "IPY_MODEL_810fcaf7fb8149f884783245d393000a",
              "IPY_MODEL_13719eb8a4e14a609a6227cfa5bc7494"
            ],
            "layout": "IPY_MODEL_bb1ec30603664534b3b6ea58a73ee334"
          }
        },
        "d47ab654488c4ffbb9a1f163e27f4ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8afcccc659a4b0cba93f59cf4e5c0ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea10286b33bd4205881bfac62c6cbf11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "042639888a4a4c68a6852b6e764b7c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d5f497c1ffe444b48a5d8e1186117533",
              "IPY_MODEL_882344abfe924d4f971c6f7265f5623b",
              "IPY_MODEL_f2243025599b4490b41876f8d376e1d3"
            ],
            "layout": "IPY_MODEL_d455152b6ae4465b8a68ca4570449da2"
          }
        },
        "d5f497c1ffe444b48a5d8e1186117533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b67916412bc4aef9b4cd1eb58c54495",
            "placeholder": "",
            "style": "IPY_MODEL_06bc20c4690b4b098ee527ce85c125d1",
            "value": "config.json:100%"
          }
        },
        "882344abfe924d4f971c6f7265f5623b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5269f2e4c60746a5ac1aad906788eb23",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_924d3c804d624b6386e495bd6510899e",
            "value": 483
          }
        },
        "f2243025599b4490b41876f8d376e1d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bacfd58d7564b539fe24ba2e3f41e80",
            "placeholder": "",
            "style": "IPY_MODEL_52359529d90440e48e9950d6b99c6105",
            "value": "483/483[00:00&lt;00:00,55.2kB/s]"
          }
        },
        "d455152b6ae4465b8a68ca4570449da2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b67916412bc4aef9b4cd1eb58c54495": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06bc20c4690b4b098ee527ce85c125d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5269f2e4c60746a5ac1aad906788eb23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "924d3c804d624b6386e495bd6510899e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1bacfd58d7564b539fe24ba2e3f41e80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52359529d90440e48e9950d6b99c6105": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e98582a6976540fa92cf59f053c9b598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a5afe073ba0484e90ce46d74fa17149",
              "IPY_MODEL_f638d78a6c4b419a8d9036d69c7ecc09",
              "IPY_MODEL_e96f86a79c844e4c919d0f93722ce0c1"
            ],
            "layout": "IPY_MODEL_e23e73b886be470b99105c2868044e85"
          }
        },
        "8a5afe073ba0484e90ce46d74fa17149": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5792cf4d98974aa18adabcae005f437d",
            "placeholder": "",
            "style": "IPY_MODEL_a20aa5491a4e49559e32f02897aed569",
            "value": "config.json:100%"
          }
        },
        "f638d78a6c4b419a8d9036d69c7ecc09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b11ae35b962e4b748c2ba18ed83774a7",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5bb5d81aaf4462d9e7b1868ddf168c5",
            "value": 483
          }
        },
        "e96f86a79c844e4c919d0f93722ce0c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_275dbdf0aae545f998184475e3c25d35",
            "placeholder": "",
            "style": "IPY_MODEL_6d0768178f934361b34281daf9ec0d9d",
            "value": "483/483[00:00&lt;00:00,61.9kB/s]"
          }
        },
        "e23e73b886be470b99105c2868044e85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5792cf4d98974aa18adabcae005f437d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a20aa5491a4e49559e32f02897aed569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b11ae35b962e4b748c2ba18ed83774a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5bb5d81aaf4462d9e7b1868ddf168c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "275dbdf0aae545f998184475e3c25d35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d0768178f934361b34281daf9ec0d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "292335598b064d189814458c7d4fa299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e559c98ddd1464b95c72bd3961ac63f",
              "IPY_MODEL_af8f9e67935140f38a3e889cc1555e23",
              "IPY_MODEL_cb75c5a357bc4802ad1b7e32603ee792"
            ],
            "layout": "IPY_MODEL_8c1bc66c674f436282342f9b3c7ec85b"
          }
        },
        "6e559c98ddd1464b95c72bd3961ac63f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bcb29880ac94eecb3bcb6bf0aeb1868",
            "placeholder": "",
            "style": "IPY_MODEL_f2da75b3cea04ee39397be0731ada68c",
            "value": "model.safetensors:100%"
          }
        },
        "af8f9e67935140f38a3e889cc1555e23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08b53cc2b77844c294e205b99cd6b545",
            "max": 267954768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6cd209aaa9f4c5994a1e3f9af12b445",
            "value": 267954768
          }
        },
        "cb75c5a357bc4802ad1b7e32603ee792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1688eaf89dba40be8cd48b599a59ab07",
            "placeholder": "",
            "style": "IPY_MODEL_1023152b2a524d1abe6b15d037f31dfe",
            "value": "268M/268M[00:00&lt;00:00,501MB/s]"
          }
        },
        "8c1bc66c674f436282342f9b3c7ec85b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bcb29880ac94eecb3bcb6bf0aeb1868": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2da75b3cea04ee39397be0731ada68c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08b53cc2b77844c294e205b99cd6b545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6cd209aaa9f4c5994a1e3f9af12b445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1688eaf89dba40be8cd48b599a59ab07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1023152b2a524d1abe6b15d037f31dfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}