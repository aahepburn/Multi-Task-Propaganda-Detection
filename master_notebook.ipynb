{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-multilearn\n"
      ],
      "metadata": {
        "id": "I0BWAmkkhll8",
        "outputId": "7228c531-a363-4739-dd05-1c0c533ce452",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/89.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bvdI4KuvicB9",
        "outputId": "927aca43-a72b-40d8-aa02-6406df9b2f69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "24b055ea",
        "execution_start": 1747152495425,
        "execution_millis": 1560,
        "execution_context_id": "a3bbeff2-281a-4251-a916-b92ab0ccf07e",
        "cell_id": "ab1260066dce4271b3037f622527d193",
        "deepnote_cell_type": "code",
        "id": "xwSD-NHJf4dh"
      },
      "source": [
        "# RUN THESE IMPORTS FIRST\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DebertaV2Tokenizer, AutoModel, RobertaTokenizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "import numpy as np\n",
        "import shap\n",
        "#from captum.attr import IntegratedGradients\n",
        "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
        "import torch.nn.functional as F\n",
        "import hf_xet\n",
        "#import optuna\n",
        "import sys\n",
        "import importlib # !pip install importlib\n",
        "\n",
        "# Append the directory to your python path using sys\n",
        "sys.path.append('/content/drive/MyDrive/Thesis Repository')\n",
        "# Import the module\n",
        "\n",
        "\n",
        "### Custom built modules ###\n",
        "import importlib\n",
        "\n",
        "import data_loader_STL\n",
        "importlib.reload(data_loader_STL)\n",
        "from data_loader_STL import prepare_data_STL_fine, prepare_data_STL_hierarchical, prepare_data_STL_coarse\n",
        "\n",
        "import single_task\n",
        "importlib.reload(single_task)\n",
        "from single_task import TransformerClassifier, MultiLabelDataset, train_single_task_model, train_hierarchical_classifier\n",
        "\n",
        "import multi_task\n",
        "importlib.reload(multi_task)\n",
        "from multi_task import MultiTaskTransformer, train_mtl_flat, train_mtl_hierarchical, apply_hierarchical_constraints_mtl, hierarchical_loss_mtl, AdapterMultiTaskTransformer\n",
        "\n",
        "import data_loader_MTL\n",
        "importlib.reload(data_loader_MTL)\n",
        "from data_loader_MTL import prepare_data_MTL_fine_flat, prepare_data_MTL_hierarchical, prepare_data_MTL_coarse, MultiTaskDataset\n",
        "\n",
        "import evaluation_utils as eval_util\n",
        "importlib.reload(eval_util)\n",
        "from evaluation_utils import evaluate_flat, evaluate_hierarchy, evaluate_mtl_all_tasks, evaluate_per_class_flat, evaluate_per_domain_flat, predict_proba, evaluate_threshold_sweep, evaluate_mtl_hierarchical_task, evaluate_mtl_hierarchical_all_tasks, evaluate_flat_custom, compute_fine_vs_coarse_metrics, get_coarse_label_list,\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Thesis Repository\")"
      ],
      "metadata": {
        "id": "BIeX20Wfh0Wd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ed80e4343e90452d9d4c49673adfe981",
        "deepnote_cell_type": "markdown",
        "id": "7ZV0YWUgf4dj"
      },
      "source": [
        "# Master Notebook\n",
        "\n",
        "Through this interface the user can experiment with all the models and experimental conditions used in the thesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d1b1137e1bff477a9d7318171c644458",
        "deepnote_cell_type": "markdown",
        "id": "5uuODno1f4dj"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "be2ffd0c",
        "is_code_hidden": true,
        "execution_start": 1746887343724,
        "execution_millis": 9678,
        "is_output_hidden": true,
        "execution_context_id": "deb92800-6cc8-4a1a-b0a6-2b27f25ec5f4",
        "deepnote_app_is_code_hidden": true,
        "deepnote_app_is_output_hidden": true,
        "cell_id": "c50b5dce8d574e94b6d9af88126700c0",
        "deepnote_cell_type": "code",
        "id": "wsJPlawNf4dj",
        "outputId": "6343fcf9-1315-417b-de38-29751839b36b"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\".\")  # Ensure current directory is in path\n",
        "\n",
        "from merged_optuna_script import objective_stl, objective_mtl, objective_mtl_adapter\n",
        "import optuna\n",
        "import pandas as pd\n",
        "\n",
        "# === Fast Experiment Sweep ===\n",
        "EXPERIMENTS = [\n",
        "    {\"setup\": \"stl\", \"task\": \"entity_framing\", \"encoder\": \"roberta-base\"},\n",
        "    {\"setup\": \"stl\", \"task\": \"narrative_classification\", \"encoder\": \"roberta-base\"},\n",
        "    {\"setup\": \"mtl\", \"task\": None, \"encoder\": \"roberta-base\"},\n",
        "    {\"setup\": \"stl\", \"task\": \"entity_framing\", \"encoder\": \"distilbert-base-uncased\"},\n",
        "    {\"setup\": \"stl\", \"task\": \"narrative_classification\", \"encoder\": \"distilbert-base-uncased\"},\n",
        "    {\"setup\": \"mtl\", \"task\": None, \"encoder\": \"distilbert-base-uncased\"},\n",
        "    {\"setup\": \"mtl_adapter\", \"task\": None, \"encoder\": \"roberta-base\"},\n",
        "    {\"setup\": \"mtl_adapter\", \"task\": None, \"encoder\": \"distilbert-base-uncased\"},\n",
        "]\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for config in EXPERIMENTS:\n",
        "    setup = config[\"setup\"]\n",
        "    task = config[\"task\"]\n",
        "    encoder = config[\"encoder\"]\n",
        "\n",
        "    print(f\"\\n Starting Optuna Study → Setup: {setup.upper()} | Task: {task or 'MTL'} | Encoder: {encoder}\")\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "\n",
        "    if setup == \"stl\":\n",
        "        study.optimize(lambda trial: objective_stl(trial, task_type=task, model_name=encoder), n_trials=3)\n",
        "    elif setup == \"mtl\":\n",
        "        study.optimize(lambda trial: objective_mtl(trial, model_name=encoder), n_trials=3)\n",
        "    elif setup == \"mtl_adapter\":\n",
        "        study.optimize(lambda trial: objective_mtl_adapter(trial, model_name=encoder), n_trials=3)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown setup: {setup}\")\n",
        "\n",
        "    best_params = study.best_trial.params\n",
        "    best_score = study.best_trial.value\n",
        "\n",
        "    print(f\"\\n Best hyperparameters for {setup.upper()} | {task or 'MTL'} | {encoder}:\")\n",
        "    for k, v in best_params.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "    print(f\"  score: {best_score:.4f}\")\n",
        "\n",
        "    all_results.append({\n",
        "        \"setup\": setup,\n",
        "        \"task\": task or \"mtl\",\n",
        "        \"encoder\": encoder,\n",
        "        \"score\": best_score,\n",
        "        **best_params\n",
        "    })\n",
        "\n",
        "# === Save results ===\n",
        "df = pd.DataFrame(all_results)\n",
        "df.to_csv(\"optuna_quick_sweep_results_adapter.csv\", index=False)\n",
        "print(\"\\n Saved results to optuna_quick_sweep_results.csv\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "[I 2025-05-10 10:29:04,051] A new study created in memory with name: no-name-efb61a5a-df9a-4bc7-a131-8da529d3017d\n\n Starting Optuna Study → Setup: MTL_ADAPTER | Task: MTL | Encoder: roberta-base\nException ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fca33dea170>>\nTraceback (most recent call last):\n  File \"/toolkit-cache/0.2.16/python3.10/kernel-libs/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\nKeyboardInterrupt: \nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nStarting Epoch 1/2...\n[W 2025-05-10 10:29:12,061] Trial 0 failed with parameters: {'learning_rate': 2.5620923423875518e-05, 'batch_size': 8, 'epochs': 2, 'threshold': 0.2050096184706236} because of the following error: KeyboardInterrupt().\nTraceback (most recent call last):\n  File \"/root/venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n  File \"/tmp/ipykernel_280/1264774272.py\", line 36, in <lambda>\n    study.optimize(lambda trial: objective_mtl_adapter(trial, model_name=encoder), n_trials=3)\n  File \"/root/work/merged_optuna_script.py\", line 166, in objective_mtl_adapter\n    train_mtl_flat(\n  File \"/root/work/multi_task.py\", line 68, in train_mtl_flat\n    loss.backward()\n  File \"/root/venv/lib/python3.10/site-packages/torch/_tensor.py\", line 648, in backward\n    torch.autograd.backward(\n  File \"/root/venv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 353, in backward\n    _engine_run_backward(\n  File \"/root/venv/lib/python3.10/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nKeyboardInterrupt\n[W 2025-05-10 10:29:12,277] Trial 0 failed with value None.\n",
          "output_type": "stream"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective_mtl(trial, model_name\u001b[38;5;241m=\u001b[39mencoder), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m setup \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmtl_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective_mtl_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown setup: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msetup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
            "Cell \u001b[0;32mIn[2], line 36\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     34\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective_mtl(trial, model_name\u001b[38;5;241m=\u001b[39mencoder), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m setup \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmtl_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 36\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective_mtl_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown setup: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msetup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/work/merged_optuna_script.py:166\u001b[0m, in \u001b[0;36mobjective_mtl_adapter\u001b[0;34m(trial, model_name)\u001b[0m\n\u001b[1;32m    163\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    164\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[0;32m--> 166\u001b[0m \u001b[43mtrain_mtl_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnarrative_classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_s2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity_framing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_s1\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnarrative_classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader_s2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val_s2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_s2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlb_s2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity_framing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader_s1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val_s1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_s1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlb_s1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnarrative_classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlb_s2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity_framing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlb_s1\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_domain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_domain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m results \u001b[38;5;241m=\u001b[39m eval_util\u001b[38;5;241m.\u001b[39mevaluate_mtl_all_tasks(\n\u001b[1;32m    189\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    190\u001b[0m     task_loaders\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     load_from_disk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    209\u001b[0m )\n\u001b[1;32m    211\u001b[0m f1s \u001b[38;5;241m=\u001b[39m [v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mvalues()]\n",
            "File \u001b[0;32m~/work/multi_task.py:68\u001b[0m, in \u001b[0;36mtrain_mtl_flat\u001b[0;34m(model, loaders, val_data, mlbs, optimizer, criterion, device, epochs, train_domain, test_domain)\u001b[0m\n\u001b[1;32m     65\u001b[0m     task_loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     66\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m task_loss\n\u001b[0;32m---> 68\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     70\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "31c2319a",
        "is_code_hidden": true,
        "execution_start": 1746918824209,
        "execution_millis": 173639,
        "execution_context_id": "c1121123-96fd-4595-bbe1-655047eb1bd9",
        "deepnote_app_is_code_hidden": true,
        "cell_id": "1112607fb25545b9b9d19854e4729c8f",
        "deepnote_cell_type": "code",
        "id": "3hfNczYof4dk"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from single_task import TransformerClassifier, train_single_task_model, MultiLabelDataset\n",
        "from multi_task import MultiTaskTransformer, AdapterMultiTaskTransformer, train_mtl_flat\n",
        "from data_loader_STL import prepare_data_STL_fine\n",
        "from data_loader_MTL import prepare_data_MTL_fine_flat\n",
        "from evaluation_utils import evaluate_flat_custom, compute_fine_vs_coarse_metrics, get_coarse_label_list\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "PARAMS = {\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"batch_size\": 8,\n",
        "    \"epochs\": 3,\n",
        "    \"threshold\": 0.35,\n",
        "    \"max_len\": 512\n",
        "}\n",
        "\n",
        "SETUPS = [\n",
        "    #{\"setup\": \"stl\", \"task\": \"entity_framing\", \"encoder\": \"roberta-base\"},\n",
        "    #{\"setup\": \"stl\", \"task\": \"narrative_classification\", \"encoder\": \"roberta-base\"},\n",
        "    #{\"setup\": \"mtl\", \"task\": None, \"encoder\": \"roberta-base\"},\n",
        "    #{\"setup\": \"mtl_adapter\", \"task\": None, \"encoder\": \"roberta-base\"},\n",
        "    #{\"setup\": \"stl\", \"task\": \"entity_framing\", \"encoder\": \"distilbert-base-uncased\"},\n",
        "    #{\"setup\": \"stl\", \"task\": \"narrative_classification\", \"encoder\": \"distilbert-base-uncased\"},\n",
        "   {\"setup\": \"mtl\", \"task\": None, \"encoder\": \"distilbert-base-uncased\"},\n",
        "    #{\"setup\": \"mtl_adapter\", \"task\": None, \"encoder\": \"distilbert-base-uncased\"},\n",
        "]\n",
        "\n",
        "TRAIN_SPLITS = [[\"CC\"]]\n",
        "EVAL_SPLITS = [\"UA\", \"CC\"]\n",
        "\n",
        "SUMMARY_COLUMNS = [\n",
        "    \"setup\", \"encoder\", \"task\", \"train_domain\", \"eval_domain\",\n",
        "    \"overall_macro\", \"overall_micro\", \"overall_exact\",\n",
        "    \"macro_fine\", \"micro_fine\", \"macro_coarse\", \"micro_coarse\"\n",
        "]\n",
        "\n",
        "for config in SETUPS:\n",
        "    setup = config[\"setup\"]\n",
        "    task = config[\"task\"]\n",
        "    encoder = config[\"encoder\"]\n",
        "    tokenizer = AutoTokenizer.from_pretrained(encoder)\n",
        "\n",
        "    setup_name = f\"{setup}_{task or 'mtl'}_{encoder.replace('/', '-')}\"\n",
        "    csv_path = f\"results_summary__{setup_name}.csv\"\n",
        "    all_rows = []\n",
        "\n",
        "    for train_domains in TRAIN_SPLITS:\n",
        "        train_str = \"+\".join(train_domains)\n",
        "\n",
        "        if setup == \"stl\":\n",
        "            df_train, df_val, df_test, y_train, y_val, y_test, mlb, text_col, label_col = prepare_data_STL_fine(\n",
        "                task, train_domains, [\"UA\", \"CC\"]\n",
        "            )\n",
        "            train_dataset = MultiLabelDataset(df_train[text_col].tolist(), y_train, tokenizer, PARAMS[\"max_len\"])\n",
        "            val_dataset = MultiLabelDataset(df_val[text_col].tolist(), y_val, tokenizer, PARAMS[\"max_len\"])\n",
        "            train_loader = DataLoader(train_dataset, batch_size=PARAMS[\"batch_size\"], shuffle=True)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=PARAMS[\"batch_size\"])\n",
        "\n",
        "            model = TransformerClassifier(encoder, len(mlb.classes_)).to(device)\n",
        "            model = train_single_task_model(\n",
        "                model=model,\n",
        "                train_loader=train_loader,\n",
        "                val_loader=val_loader,\n",
        "                y_val=y_val,\n",
        "                MODEL_PATH=\"tmp.pt\",\n",
        "                LEARNING_RATE=PARAMS[\"learning_rate\"],\n",
        "                EPOCHS=PARAMS[\"epochs\"],\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            results_fine, results_coarse = {}, {}\n",
        "            coarse_list = get_coarse_label_list(task)\n",
        "\n",
        "            for domain in EVAL_SPLITS:\n",
        "                df_eval = df_test[df_test[\"Domain\"] == domain].copy()\n",
        "                known_labels = set(mlb.classes_)\n",
        "                df_eval[label_col] = df_eval[label_col].apply(lambda labels: [l for l in labels if l in known_labels])\n",
        "                y_eval = mlb.transform(df_eval[label_col])\n",
        "                test_loader = DataLoader(\n",
        "                    MultiLabelDataset(df_eval[text_col].tolist(), y_eval, tokenizer, PARAMS[\"max_len\"]),\n",
        "                    batch_size=PARAMS[\"batch_size\"]\n",
        "                )\n",
        "                eval_result = evaluate_flat_custom(model, test_loader, df_eval, mlb, device, threshold=PARAMS[\"threshold\"])\n",
        "                score_dict = compute_fine_vs_coarse_metrics(eval_result[\"y_true\"], eval_result[\"y_pred_bin\"], list(mlb.classes_), coarse_list)\n",
        "\n",
        "                all_rows.append({\n",
        "                    \"setup\": setup,\n",
        "                    \"encoder\": encoder,\n",
        "                    \"task\": task,\n",
        "                    \"train_domain\": train_str,\n",
        "                    \"eval_domain\": domain,\n",
        "                    \"overall_macro\": round((score_dict[\"macro_fine\"] + score_dict[\"macro_coarse\"]) / 2, 4),\n",
        "                    \"overall_micro\": round((score_dict[\"micro_fine\"] + score_dict[\"micro_coarse\"]) / 2, 4),\n",
        "                    \"overall_exact\": round(eval_result[\"exact\"], 4),\n",
        "                    \"macro_fine\": round(score_dict[\"macro_fine\"], 4),\n",
        "                    \"micro_fine\": round(score_dict[\"micro_fine\"], 4),\n",
        "                    \"macro_coarse\": round(score_dict[\"macro_coarse\"], 4),\n",
        "                    \"micro_coarse\": round(score_dict[\"micro_coarse\"], 4)\n",
        "                })\n",
        "\n",
        "        elif setup in [\"mtl\", \"mtl_adapter\"]:\n",
        "            (\n",
        "                df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n",
        "                df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n",
        "                train_loader_s1, val_loader_s1, test_loader_s1,\n",
        "                train_loader_s2, val_loader_s2, test_loader_s2,\n",
        "                num_classes_dict\n",
        "            ) = prepare_data_MTL_fine_flat(\n",
        "                TASK=\"multi_task\",\n",
        "                model_name=encoder,\n",
        "                max_len=PARAMS[\"max_len\"],\n",
        "                batch_size=PARAMS[\"batch_size\"],\n",
        "                train_domains=train_domains,\n",
        "                test_domains=[\"UA\", \"CC\"],\n",
        "                train_languages=[\"ALL\"]\n",
        "            )\n",
        "\n",
        "            task_classes = {\n",
        "                \"entity_framing\": y_train_s1.shape[1],\n",
        "                \"narrative_classification\": y_train_s2.shape[1]\n",
        "            }\n",
        "            model = MultiTaskTransformer(encoder, task_classes).to(device) if setup == \"mtl\" else \\\n",
        "                AdapterMultiTaskTransformer(model_name=encoder, num_classes_dict=task_classes).to(device)\n",
        "\n",
        "            optimizer = torch.optim.AdamW(model.parameters(), lr=PARAMS[\"learning_rate\"])\n",
        "            criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "            train_mtl_flat(\n",
        "                model=model,\n",
        "                loaders={\"entity_framing\": train_loader_s1, \"narrative_classification\": train_loader_s2},\n",
        "                val_data={\n",
        "                    \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1),\n",
        "                    \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2)\n",
        "                },\n",
        "                mlbs={\"entity_framing\": mlb_s1, \"narrative_classification\": mlb_s2},\n",
        "                optimizer=optimizer,\n",
        "                criterion=criterion,\n",
        "                device=device,\n",
        "                epochs=PARAMS[\"epochs\"],\n",
        "                train_domain=train_domains,\n",
        "                test_domain=[\"UA\", \"CC\"]\n",
        "            )\n",
        "\n",
        "            for domain in EVAL_SPLITS:\n",
        "                for subtask, df_test, mlb, text_key, label_key in [\n",
        "                    (\"entity_framing\", df_test_s1, mlb_s1, \"Input_Text\", \"Label\"),\n",
        "                    (\"narrative_classification\", df_test_s2, mlb_s2, \"Translated_Text\", \"Label\")\n",
        "                ]:\n",
        "                    df_eval = df_test[df_test[\"Domain\"] == domain].copy()\n",
        "                    known_labels = set(mlb.classes_)\n",
        "                    df_eval[label_key] = df_eval[label_key].apply(\n",
        "                        lambda labels: [l for l in labels if l in known_labels] if isinstance(labels, list) else []\n",
        "                    )\n",
        "                    y_eval = mlb.transform(df_eval[label_key])\n",
        "\n",
        "                    test_loader = DataLoader(\n",
        "                        MultiLabelDataset(df_eval[text_key].tolist(), y_eval, tokenizer, PARAMS[\"max_len\"]),\n",
        "                        batch_size=PARAMS[\"batch_size\"]\n",
        "                    )\n",
        "\n",
        "                    model_path = f\"{subtask}_MTL_{'-'.join(train_domains)}_to_{'-'.join(EVAL_SPLITS)}.pt\"\n",
        "                    if os.path.exists(model_path):\n",
        "                        model.load_state_dict(torch.load(model_path))\n",
        "                        model.to(device)\n",
        "                        print(f\"✅ Loaded model for {subtask}\")\n",
        "                    else:\n",
        "                        print(f\"⚠️ Missing checkpoint: {model_path}\")\n",
        "\n",
        "\n",
        "                    eval_result = evaluate_flat_custom(\n",
        "                        model=model,\n",
        "                        loader=test_loader,\n",
        "                        df_source=df_eval,\n",
        "                        mlb=mlb,\n",
        "                        device=device,\n",
        "                        threshold=PARAMS[\"threshold\"],\n",
        "                        task=subtask\n",
        "                    )\n",
        "                    coarse_list = get_coarse_label_list(subtask)\n",
        "                    score_dict = compute_fine_vs_coarse_metrics(\n",
        "                        eval_result[\"y_true\"], eval_result[\"y_pred_bin\"], list(mlb.classes_), coarse_list\n",
        "                    )\n",
        "\n",
        "                    all_rows.append({\n",
        "                        \"setup\": setup,\n",
        "                        \"encoder\": encoder,\n",
        "                        \"task\": subtask,\n",
        "                        \"train_domain\": train_str,\n",
        "                        \"eval_domain\": domain,\n",
        "                        \"overall_macro\": round((score_dict[\"macro_fine\"] + score_dict[\"macro_coarse\"]) / 2, 4),\n",
        "                        \"overall_micro\": round((score_dict[\"micro_fine\"] + score_dict[\"micro_coarse\"]) / 2, 4),\n",
        "                        \"overall_exact\": round(eval_result[\"exact\"], 4),\n",
        "                        \"macro_fine\": round(score_dict[\"macro_fine\"], 4),\n",
        "                        \"micro_fine\": round(score_dict[\"micro_fine\"], 4),\n",
        "                        \"macro_coarse\": round(score_dict[\"macro_coarse\"], 4),\n",
        "                        \"micro_coarse\": round(score_dict[\"micro_coarse\"], 4),\n",
        "                    })\n",
        "\n",
        "\n",
        "    pd.DataFrame(all_rows, columns=SUMMARY_COLUMNS).to_csv(csv_path, index=False)\n",
        "    print(f\"✅ Saved: {csv_path}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "9f01182e",
        "is_code_hidden": true,
        "execution_start": 1746921270867,
        "execution_millis": 11327,
        "execution_context_id": "c1121123-96fd-4595-bbe1-655047eb1bd9",
        "deepnote_app_is_code_hidden": true,
        "cell_id": "c1aaa69fe08344b2a18db7bcf990ab39",
        "deepnote_cell_type": "code",
        "id": "XjyAbvpdf4dk"
      },
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "PARAMS = {\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"batch_size\": 8,\n",
        "    \"epochs\": 3,\n",
        "    \"threshold\": 0.35,\n",
        "    \"max_len\": 512\n",
        "}\n",
        "\n",
        "SETUPS = [\n",
        "    {\"setup\": \"mtl\", \"task\": None, \"encoder\": \"distilbert-base-uncased\"},\n",
        "]\n",
        "\n",
        "TRAIN_SPLITS = [[\"CC\"]]\n",
        "EVAL_SPLITS = [\"UA\", \"CC\"]\n",
        "\n",
        "SUMMARY_COLUMNS = [\n",
        "    \"setup\", \"encoder\", \"task\", \"train_domain\", \"eval_domain\",\n",
        "    \"overall_macro\", \"overall_micro\", \"overall_exact\",\n",
        "    \"macro_fine\", \"micro_fine\", \"macro_coarse\", \"micro_coarse\"\n",
        "]\n",
        "\n",
        "for config in SETUPS:\n",
        "    setup = config[\"setup\"]\n",
        "    task = config[\"task\"]\n",
        "    encoder = config[\"encoder\"]\n",
        "    tokenizer = AutoTokenizer.from_pretrained(encoder)\n",
        "\n",
        "    setup_name = f\"{setup}_{task or 'mtl'}_{encoder.replace('/', '-')}\"\n",
        "    csv_path = f\"results_summary__{setup_name}.csv\"\n",
        "    all_rows = []\n",
        "\n",
        "    for train_domains in TRAIN_SPLITS:\n",
        "        train_str = \"+\".join(train_domains)\n",
        "\n",
        "        if setup == \"stl\":\n",
        "            df_train, df_val, df_test, y_train, y_val, y_test, mlb, text_col, label_col = prepare_data_STL_fine(\n",
        "                task, train_domains, EVAL_SPLITS\n",
        "            )\n",
        "            train_dataset = MultiLabelDataset(df_train[text_col].tolist(), y_train, tokenizer, PARAMS[\"max_len\"])\n",
        "            val_dataset = MultiLabelDataset(df_val[text_col].tolist(), y_val, tokenizer, PARAMS[\"max_len\"])\n",
        "            train_loader = DataLoader(train_dataset, batch_size=PARAMS[\"batch_size\"], shuffle=True)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=PARAMS[\"batch_size\"])\n",
        "\n",
        "            model = TransformerClassifier(encoder, len(mlb.classes_)).to(device)\n",
        "            model = train_single_task_model(\n",
        "                model=model,\n",
        "                train_loader=train_loader,\n",
        "                val_loader=val_loader,\n",
        "                y_val=y_val,\n",
        "                MODEL_PATH=\"tmp.pt\",\n",
        "                LEARNING_RATE=PARAMS[\"learning_rate\"],\n",
        "                EPOCHS=PARAMS[\"epochs\"],\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            for domain in EVAL_SPLITS:\n",
        "                df_eval = df_test[df_test[\"Domain\"] == domain].copy()\n",
        "                known_labels = set(mlb.classes_)\n",
        "                df_eval[label_col] = df_eval[label_col].apply(lambda labels: [l for l in labels if l in known_labels])\n",
        "                y_eval = mlb.transform(df_eval[label_col])\n",
        "                test_loader = DataLoader(\n",
        "                    MultiLabelDataset(df_eval[text_col].tolist(), y_eval, tokenizer, PARAMS[\"max_len\"]),\n",
        "                    batch_size=PARAMS[\"batch_size\"]\n",
        "                )\n",
        "                eval_result = evaluate_flat_custom(model, test_loader, df_eval, mlb, device, threshold=PARAMS[\"threshold\"])\n",
        "                score_dict = compute_fine_vs_coarse_metrics(eval_result[\"y_true\"], eval_result[\"y_pred_bin\"], list(mlb.classes_), get_coarse_label_list(task))\n",
        "\n",
        "                all_rows.append({\n",
        "                    \"setup\": setup,\n",
        "                    \"encoder\": encoder,\n",
        "                    \"task\": task,\n",
        "                    \"train_domain\": train_str,\n",
        "                    \"eval_domain\": domain,\n",
        "                    \"overall_macro\": round((score_dict[\"macro_fine\"] + score_dict[\"macro_coarse\"]) / 2, 4),\n",
        "                    \"overall_micro\": round((score_dict[\"micro_fine\"] + score_dict[\"micro_coarse\"]) / 2, 4),\n",
        "                    \"overall_exact\": round(eval_result[\"exact\"], 4),\n",
        "                    \"macro_fine\": round(score_dict[\"macro_fine\"], 4),\n",
        "                    \"micro_fine\": round(score_dict[\"micro_fine\"], 4),\n",
        "                    \"macro_coarse\": round(score_dict[\"macro_coarse\"], 4),\n",
        "                    \"micro_coarse\": round(score_dict[\"micro_coarse\"], 4),\n",
        "                })\n",
        "\n",
        "        elif setup in [\"mtl\", \"mtl_adapter\"]:\n",
        "            (\n",
        "                df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n",
        "                df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n",
        "                train_loader_s1, val_loader_s1, test_loader_s1,\n",
        "                train_loader_s2, val_loader_s2, test_loader_s2,\n",
        "                num_classes_dict\n",
        "            ) = prepare_data_MTL_fine_flat(\n",
        "                TASK=\"multi_task\",\n",
        "                model_name=encoder,\n",
        "                max_len=PARAMS[\"max_len\"],\n",
        "                batch_size=PARAMS[\"batch_size\"],\n",
        "                train_domains=train_domains,\n",
        "                test_domains=EVAL_SPLITS,\n",
        "                train_languages=[\"ALL\"]\n",
        "            )\n",
        "\n",
        "            task_classes = {\n",
        "                \"entity_framing\": y_train_s1.shape[1],\n",
        "                \"narrative_classification\": y_train_s2.shape[1]\n",
        "            }\n",
        "            model = MultiTaskTransformer(encoder, task_classes).to(device) if setup == \"mtl\" else \\\n",
        "                AdapterMultiTaskTransformer(model_name=encoder, num_classes_dict=task_classes).to(device)\n",
        "\n",
        "            for subtask in [\"entity_framing\", \"narrative_classification\"]:\n",
        "                model_path = f\"{subtask}_MTL_{'-'.join(train_domains)}_to_{'-'.join(EVAL_SPLITS)}.pt\"\n",
        "                if os.path.exists(model_path):\n",
        "                    model.load_state_dict(torch.load(model_path), strict=False)\n",
        "                    print(f\"✅ Loaded model for {subtask}\")\n",
        "                else:\n",
        "                    print(f\"⚠️ Missing checkpoint: {model_path}\")\n",
        "\n",
        "                df_test = df_test_s1 if subtask == \"entity_framing\" else df_test_s2\n",
        "                mlb = mlb_s1 if subtask == \"entity_framing\" else mlb_s2\n",
        "                text_key = \"Input_Text\" if subtask == \"entity_framing\" else \"Translated_Text\"\n",
        "                label_key = \"Label\"\n",
        "\n",
        "                for domain in EVAL_SPLITS:\n",
        "                    df_eval = df_test[df_test[\"Domain\"] == domain].copy()\n",
        "                    known_labels = set(mlb.classes_)\n",
        "                    df_eval[label_key] = df_eval[label_key].apply(lambda labels: [l for l in labels if l in known_labels] if isinstance(labels, list) else [])\n",
        "                    y_eval = mlb.transform(df_eval[label_key])\n",
        "\n",
        "                    test_loader = DataLoader(\n",
        "                        MultiTaskDataset(df_eval[text_key].tolist(), {subtask: y_eval}, tokenizer, PARAMS[\"max_len\"]),\n",
        "                        batch_size=PARAMS[\"batch_size\"]\n",
        "                    )\n",
        "\n",
        "                    eval_result = evaluate_flat_custom(\n",
        "                        model=model,\n",
        "                        loader=test_loader,\n",
        "                        df_source=df_eval,\n",
        "                        mlb=mlb,\n",
        "                        device=device,\n",
        "                        threshold=PARAMS[\"threshold\"],\n",
        "                        task=subtask\n",
        "                    )\n",
        "                    coarse_list = get_coarse_label_list(subtask)\n",
        "                    score_dict = compute_fine_vs_coarse_metrics(eval_result[\"y_true\"], eval_result[\"y_pred_bin\"], list(mlb.classes_), coarse_list)\n",
        "\n",
        "                    all_rows.append({\n",
        "                        \"setup\": setup,\n",
        "                        \"encoder\": encoder,\n",
        "                        \"task\": subtask,\n",
        "                        \"train_domain\": train_str,\n",
        "                        \"eval_domain\": domain,\n",
        "                        \"overall_macro\": round((score_dict[\"macro_fine\"] + score_dict[\"macro_coarse\"]) / 2, 4),\n",
        "                        \"overall_micro\": round((score_dict[\"micro_fine\"] + score_dict[\"micro_coarse\"]) / 2, 4),\n",
        "                        \"overall_exact\": round(eval_result[\"exact\"], 4),\n",
        "                        \"macro_fine\": round(score_dict[\"macro_fine\"], 4),\n",
        "                        \"micro_fine\": round(score_dict[\"micro_fine\"], 4),\n",
        "                        \"macro_coarse\": round(score_dict[\"macro_coarse\"], 4),\n",
        "                        \"micro_coarse\": round(score_dict[\"micro_coarse\"], 4),\n",
        "                    })\n",
        "\n",
        "    pd.DataFrame(all_rows, columns=SUMMARY_COLUMNS).to_csv(csv_path, index=False)\n",
        "    print(f\"✅ Saved: {csv_path}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8ef97b96fbf142f4a3edbb657eeb9b03",
        "deepnote_cell_type": "markdown",
        "id": "uRV2HtgMf4dl"
      },
      "source": [
        "## Control Panel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "34d8b3e2",
        "execution_start": 1747152501113,
        "execution_millis": 155,
        "execution_context_id": "a3bbeff2-281a-4251-a916-b92ab0ccf07e",
        "cell_id": "c6c33bfb7b6440a98762b17b647c356f",
        "deepnote_cell_type": "code",
        "id": "RlZHrrRyf4dl"
      },
      "source": [
        "# Choose a task for the pipeline below: \"narrative_classification\" or \"entity_framing\" or \"multi_task\" or \"multi_task_adapter\"\n",
        "TASK = \"entity_framing\"\n",
        "\n",
        "# select domains for training and testing: \"UA\"; \"CC\"; \"UA\", \"CC\";\n",
        "TRAIN_DOMAIN = [\"UA\"]\n",
        "TEST_DOMAIN = [\"UA\", \"CC\"] # The test data comes from a separate dataset.\n",
        "# The test data is always the same regardless of the domain we choose to train on. This is for consistency.\n",
        "\n",
        "# select languages for training and testing: \"ALL\";\"EN\";\"HI\";\"BG\";\"RU\";\"PT\"\n",
        "TRAIN_LANGUAGES = [\"ALL\"]\n",
        "TEST_LANGUAGES = [\"ALL\"]\n",
        "\n",
        "# Taxonomy Depth\n",
        "TAXONOMY_DEPTH = \"FINE\" # \"COARSE\" OR \"FINE\"\n",
        "\n",
        "# Classifier Complexity\n",
        "CLASSIFIER_COMPLEXITY = \"HIERARCHICAL\" # \"FLAT\" OR \"HIERARCHICAL\"\n",
        "\n",
        "# change the training hyperparameters here\n",
        "MODEL_NAME = \"distilbert-base-uncased\" # OR  \"distilbert-base-uncased\" \"roberta-base\" \"\"FacebookAI/roberta-base\"\"\n",
        "MAX_LEN = 512\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 3e-5\n",
        "MODEL_PATH = f\"{TASK}_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\" # -- to save the model later\n",
        "\n",
        "#tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
        "#tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "\n",
        "# debug mode -- reduced samples\n",
        "DEBUG_MODE = False"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "47fe07ac",
        "execution_start": 1747152504387,
        "execution_millis": 2,
        "execution_context_id": "a3bbeff2-281a-4251-a916-b92ab0ccf07e",
        "cell_id": "62726c5321f94778994bc298793e96f9",
        "deepnote_cell_type": "code",
        "id": "DF6-z_g3f4dl"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a5495661d9f442768d73571169ad1f5c",
        "deepnote_cell_type": "markdown",
        "id": "N_HRzpsNf4dl"
      },
      "source": [
        "## UTILS Assemble Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "a9cc233",
        "execution_start": 1747152505490,
        "execution_millis": 1958,
        "execution_context_id": "a3bbeff2-281a-4251-a916-b92ab0ccf07e",
        "cell_id": "e77f7dd60a1a430ebbd37089281db4ea",
        "deepnote_cell_type": "code",
        "id": "j1NSuGhBf4dl"
      },
      "source": [
        "if TASK != \"multi_task\" and TASK != \"multi_task_adapter\":\n",
        "    if TAXONOMY_DEPTH == 'FINE':\n",
        "        if CLASSIFIER_COMPLEXITY == 'FLAT':\n",
        "            df_train, df_val, df_test, y_train, y_val, y_test, mlb, TEXT_COL, LABEL_COL = prepare_data_STL_fine(\n",
        "                TASK,\n",
        "                TRAIN_DOMAIN,\n",
        "                TEST_DOMAIN,\n",
        "            )\n",
        "        elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n",
        "            df_train, df_val, df_test, y_train, y_val, y_test, mlb, TEXT_COL, LABEL_COL, child_to_parent, label_to_index = prepare_data_STL_hierarchical(\n",
        "                TASK,\n",
        "                TRAIN_DOMAIN,\n",
        "                TEST_DOMAIN,\n",
        "            )\n",
        "\n",
        "\n",
        "    elif TAXONOMY_DEPTH == 'COARSE':\n",
        "        df_train, df_val, df_test, y_train, y_val, y_test, mlb, TEXT_COL, LABEL_COL = prepare_data_STL_coarse(\n",
        "                TASK,\n",
        "                TRAIN_DOMAIN,\n",
        "                TEST_DOMAIN,\n",
        "            )\n",
        "\n",
        "    train_dataset = MultiLabelDataset(df_train[TEXT_COL].tolist(), y_train, tokenizer, MAX_LEN)\n",
        "    val_dataset = MultiLabelDataset(df_val[TEXT_COL].tolist(), y_val, tokenizer, MAX_LEN)\n",
        "    test_dataset = MultiLabelDataset(df_test[TEXT_COL].tolist(), y_test, tokenizer, MAX_LEN)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "    num_classes = len(mlb.classes_)\n",
        "\n",
        "\n",
        "elif TASK == \"multi_task\" or TASK == \"multi_task_adapter\":\n",
        "\n",
        "    if TAXONOMY_DEPTH == 'FINE':\n",
        "\n",
        "        if CLASSIFIER_COMPLEXITY == 'FLAT':\n",
        "            (\n",
        "                df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n",
        "                df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n",
        "                train_loader_s1, val_loader_s1, test_loader_s1,\n",
        "                train_loader_s2, val_loader_s2, test_loader_s2,\n",
        "                num_classes_dict\n",
        "            ) = prepare_data_MTL_fine_flat(\n",
        "                TASK,\n",
        "                train_domains=TRAIN_DOMAIN,\n",
        "                test_domains=TEST_DOMAIN,\n",
        "                train_languages=TRAIN_LANGUAGES,\n",
        "                model_name=MODEL_NAME,\n",
        "                max_len=MAX_LEN,\n",
        "                batch_size=BATCH_SIZE\n",
        "            )\n",
        "\n",
        "        elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n",
        "            (\n",
        "                df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n",
        "                df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n",
        "                train_loader_s1, val_loader_s1, test_loader_s1,\n",
        "                train_loader_s2, val_loader_s2, test_loader_s2,\n",
        "                num_classes_dict,\n",
        "                child_to_parent_map,\n",
        "                label_to_index_map\n",
        "            ) = prepare_data_MTL_hierarchical(\n",
        "                TASK,\n",
        "                train_domains=TRAIN_DOMAIN,\n",
        "                test_domains=TEST_DOMAIN,\n",
        "                train_languages=TRAIN_LANGUAGES,\n",
        "                model_name=MODEL_NAME,\n",
        "                max_len=MAX_LEN,\n",
        "                batch_size=BATCH_SIZE\n",
        "            )\n",
        "\n",
        "    elif TAXONOMY_DEPTH == 'COARSE':\n",
        "        (\n",
        "            df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n",
        "            df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n",
        "            train_loader_s1, val_loader_s1, test_loader_s1,\n",
        "            train_loader_s2, val_loader_s2, test_loader_s2,\n",
        "            num_classes_dict\n",
        "        ) = prepare_data_MTL_coarse(\n",
        "            TASK,\n",
        "            train_domains=TRAIN_DOMAIN,\n",
        "            test_domains=TEST_DOMAIN,\n",
        "            train_languages=TRAIN_LANGUAGES,\n",
        "            model_name=MODEL_NAME,\n",
        "            max_len=MAX_LEN,\n",
        "            batch_size=BATCH_SIZE\n",
        "        )\n",
        "\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b4c55c78c616487cb9c73338c9886ab6",
        "deepnote_cell_type": "markdown",
        "id": "CpAM9RN4f4dl"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "c99a6286",
        "execution_start": 1747152509360,
        "execution_millis": 585,
        "execution_context_id": "a3bbeff2-281a-4251-a916-b92ab0ccf07e",
        "cell_id": "3c084ae072e14c75a89644ba77a26804",
        "deepnote_cell_type": "code",
        "id": "oo3ZBty0f4dl",
        "outputId": "b5386825-2680-49c1-bc5f-c4e6b5e91131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "15296fc107174aa0b9439076f942f0b6",
            "140f4658b78944498b589392d1d36233",
            "930b5f1ba05f4195b87a67cb2fa0a63f",
            "c2b92c2d6c364644b9661629d9e63fd4",
            "d3734fe20b4a4fd995ae6d0ebb0ffd13",
            "f6b0e820a2ac4554a02ef262404af56e",
            "3e859ad462e0449fac4fadc4221d80d2",
            "f492fa59aa9e4d94b32f781a8e7ed0fc",
            "dfe13871a7404c97bbb58add41c9b563",
            "f571f60293144697991abc29632479aa",
            "90a73516fea0491e93073f2ba67013db",
            "4f4e2f2fb8494e51aa770893f714121d",
            "5611ef749993464494d197ec34eacc44",
            "b045e35760c94acd818be2cfba0e035b",
            "45cbb84f0f7940838d2d563205ab738b",
            "977173f34b5a4954a94567d6059e6ba4",
            "94dd1db7c4494407be7b5ebd1841187c",
            "4c32bdbf849640d48798ec7bcc197f1f",
            "ad5e7517f28d4e019ec2e7dbb8c4a4a9",
            "563c3a62e9124acfa3677a8539f4b74e",
            "53a82562434e467193ca048c315131a7",
            "91530a7f44c242edb65e46da3d864660",
            "5ffd88c765fc4cd5b6b00cd02f9d8d35",
            "06a53bab0ab6477693ce55ccf7804488",
            "dcdd36d23d6b46fcabe1efa05d0ee7fe",
            "5cfa13adb2e54f0494a7d6f701d9f9a8",
            "f354fb535771424894a59bc1385df74c",
            "c108df9b26e4484b98313147df00d288",
            "3db3f85ad2d446dcb1b308dace00a858",
            "03ba90997f354c67bf52e43078c4bbb6",
            "b5c77c2fed14433c99ebb25a452fa32b",
            "6259133122854a1a848c17c3dcbc0872",
            "766440dc93f94e26aa2ad496b77639de"
          ]
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if TASK != \"multi_task\" and TASK != \"multi_task_adapter\":\n",
        "    print(\"\\n>>> Running Single-Task (no adapter) Model <<<\")\n",
        "    model = TransformerClassifier(MODEL_NAME, num_classes).to(device)\n",
        "\n",
        "    if CLASSIFIER_COMPLEXITY == 'FLAT':\n",
        "        print(\"\\n>>> Running Single-Task (flat Model <<<\")\n",
        "        trained_model = train_single_task_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            y_val=y_val,\n",
        "            MODEL_PATH=MODEL_PATH,\n",
        "            LEARNING_RATE=LEARNING_RATE,\n",
        "            EPOCHS=EPOCHS,\n",
        "            device=device,\n",
        "            predict_proba=eval_util.predict_proba,\n",
        "            evaluate_threshold_sweep=eval_util.evaluate_threshold_sweep\n",
        "        )\n",
        "        trained_model.load_state_dict(torch.load(MODEL_PATH))\n",
        "        trained_model.to(device)\n",
        "\n",
        "    elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n",
        "        print(\"\\n>>> Running Single-Task (hierarchy) Model <<<\")\n",
        "        trained_model = train_hierarchical_classifier(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            y_val=y_val,\n",
        "            MODEL_PATH=MODEL_PATH,\n",
        "            child_to_parent=child_to_parent,\n",
        "            label_to_index=label_to_index,\n",
        "            predict_proba=eval_util.predict_proba,\n",
        "            evaluate_threshold_sweep=eval_util.evaluate_threshold_sweep,\n",
        "            LEARNING_RATE=LEARNING_RATE,\n",
        "            EPOCHS=EPOCHS\n",
        "        )\n",
        "        trained_model.load_state_dict(torch.load(MODEL_PATH))\n",
        "        trained_model.to(device)\n",
        "\n",
        "elif TASK == \"multi_task\":\n",
        "    print(\"\\n>>> Running Multi-Task (no adapter) Model <<<\")\n",
        "    task_classes = {\n",
        "        \"narrative_classification\": y_train_s2.shape[1],\n",
        "        \"entity_framing\": y_train_s1.shape[1]\n",
        "    }\n",
        "    model = MultiTaskTransformer(MODEL_NAME, task_classes).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    if CLASSIFIER_COMPLEXITY == 'FLAT':\n",
        "        train_mtl_flat(\n",
        "            model=model,\n",
        "            loaders={\n",
        "                \"narrative_classification\": train_loader_s2,\n",
        "                \"entity_framing\": train_loader_s1\n",
        "            },\n",
        "            val_data={\n",
        "                \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2),\n",
        "                \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1)\n",
        "            },\n",
        "            mlbs={\n",
        "                \"narrative_classification\": mlb_s2,\n",
        "                \"entity_framing\": mlb_s1\n",
        "            },\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            device=device,\n",
        "            epochs=EPOCHS,\n",
        "            train_domain=TRAIN_DOMAIN,\n",
        "            test_domain=TEST_DOMAIN\n",
        "        )\n",
        "\n",
        "    elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n",
        "        print(\"\\n>>> Running Multi-Task model (hierarchy) <<<\")\n",
        "        train_mtl_hierarchical(\n",
        "            model=model,\n",
        "            loaders={\n",
        "                \"narrative_classification\": train_loader_s2,\n",
        "                \"entity_framing\": train_loader_s1\n",
        "            },\n",
        "            val_data={\n",
        "                \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2),\n",
        "                \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1)\n",
        "            },\n",
        "            child_to_parent_map=child_to_parent_map,\n",
        "            label_to_index_map=label_to_index_map,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            device=device,\n",
        "            epochs=EPOCHS,\n",
        "            train_domain=TRAIN_DOMAIN,\n",
        "            test_domain=TEST_DOMAIN\n",
        "        )\n",
        "\n",
        "\n",
        "    # Re-load best saved model per task\n",
        "    model.load_state_dict(torch.load(f\"entity_framing_MTL_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\"), strict=False)\n",
        "    model.load_state_dict(torch.load(f\"narrative_classification_MTL_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\"), strict=False)\n",
        "    trained_model = model\n",
        "\n",
        "\n",
        "elif TASK == \"multi_task_adapter\":\n",
        "    print(\"\\n>>> Running Multi-Task Adapter Model <<<\")\n",
        "\n",
        "    task_classes = {\n",
        "        \"narrative_classification\": y_train_s2.shape[1],\n",
        "        \"entity_framing\": y_train_s1.shape[1]\n",
        "    }\n",
        "\n",
        "    model = AdapterMultiTaskTransformer(\n",
        "        model_name=MODEL_NAME,\n",
        "        num_classes_dict=task_classes,\n",
        "        adapter_dim=128\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    if CLASSIFIER_COMPLEXITY == 'FLAT':\n",
        "        train_mtl_flat(\n",
        "            model=model,\n",
        "            loaders={\n",
        "                \"narrative_classification\": train_loader_s2,\n",
        "                \"entity_framing\": train_loader_s1\n",
        "            },\n",
        "            val_data={\n",
        "                \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2),\n",
        "                \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1)\n",
        "            },\n",
        "            mlbs={\n",
        "                \"narrative_classification\": mlb_s2,\n",
        "                \"entity_framing\": mlb_s1\n",
        "            },\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            device=device,\n",
        "            epochs=EPOCHS,\n",
        "            train_domain=TRAIN_DOMAIN,\n",
        "            test_domain=TEST_DOMAIN\n",
        "        )\n",
        "    elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n",
        "        train_mtl_hierarchical(\n",
        "            model=model,\n",
        "            loaders={\n",
        "                \"narrative_classification\": train_loader_s2,\n",
        "                \"entity_framing\": train_loader_s1\n",
        "            },\n",
        "            val_data={\n",
        "                \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2),\n",
        "                \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1)\n",
        "            },\n",
        "            child_to_parent_map=child_to_parent_map,\n",
        "            label_to_index_map=label_to_index_map,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            device=device,\n",
        "            epochs=EPOCHS,\n",
        "            train_domain=TRAIN_DOMAIN,\n",
        "            test_domain=TEST_DOMAIN\n",
        "        )\n",
        "\n",
        "    # load best saved models\n",
        "    model.load_state_dict(torch.load(f\"entity_framing_MTL_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\"), strict=False)\n",
        "    model.load_state_dict(torch.load(f\"narrative_classification_MTL_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\"), strict=False)\n",
        "    trained_model = model\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Running Single-Task (no adapter) Model <<<\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15296fc107174aa0b9439076f942f0b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f4e2f2fb8494e51aa770893f714121d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ffd88c765fc4cd5b6b00cd02f9d8d35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 603/603 [04:03<00:00,  2.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: Loss = 0.5722\n",
            "Threshold sweep results:\n",
            "Thresh 0.10 | Macro F1: 0.484 | Micro F1: 0.501 | Exact Match: 0.000\n",
            "Thresh 0.15 | Macro F1: 0.516 | Micro F1: 0.549 | Exact Match: 0.020\n",
            "Thresh 0.20 | Macro F1: 0.548 | Micro F1: 0.607 | Exact Match: 0.168\n",
            "Thresh 0.25 | Macro F1: 0.546 | Micro F1: 0.634 | Exact Match: 0.357\n",
            "Thresh 0.30 | Macro F1: 0.536 | Micro F1: 0.635 | Exact Match: 0.471\n",
            "Thresh 0.35 | Macro F1: 0.499 | Micro F1: 0.624 | Exact Match: 0.537\n",
            "Thresh 0.40 | Macro F1: 0.454 | Micro F1: 0.608 | Exact Match: 0.573\n",
            "Thresh 0.45 | Macro F1: 0.420 | Micro F1: 0.594 | Exact Match: 0.578\n",
            "Thresh 0.50 | Macro F1: 0.377 | Micro F1: 0.579 | Exact Match: 0.535\n",
            "Thresh 0.55 | Macro F1: 0.338 | Micro F1: 0.556 | Exact Match: 0.477\n",
            "Thresh 0.60 | Macro F1: 0.281 | Micro F1: 0.482 | Exact Match: 0.362\n",
            "Thresh 0.65 | Macro F1: 0.211 | Micro F1: 0.348 | Exact Match: 0.224\n",
            "Thresh 0.70 | Macro F1: 0.095 | Micro F1: 0.132 | Exact Match: 0.072\n",
            "Thresh 0.75 | Macro F1: 0.010 | Micro F1: 0.009 | Exact Match: 0.005\n",
            "Thresh 0.80 | Macro F1: 0.000 | Micro F1: 0.000 | Exact Match: 0.000\n",
            "Thresh 0.85 | Macro F1: 0.000 | Micro F1: 0.000 | Exact Match: 0.000\n",
            "\n",
            " Best threshold = 0.20 with Macro F1 = 0.548\n",
            "Validation Macro F1 (Epoch 1): 0.5482\n",
            "Saved best model (Epoch 1) to entity_framing_UA_to_UA-CC.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 603/603 [04:09<00:00,  2.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: Loss = 0.5177\n",
            "Threshold sweep results:\n",
            "Thresh 0.10 | Macro F1: 0.525 | Micro F1: 0.550 | Exact Match: 0.062\n",
            "Thresh 0.15 | Macro F1: 0.575 | Micro F1: 0.606 | Exact Match: 0.170\n",
            "Thresh 0.20 | Macro F1: 0.597 | Micro F1: 0.642 | Exact Match: 0.297\n",
            "Thresh 0.25 | Macro F1: 0.585 | Micro F1: 0.661 | Exact Match: 0.395\n",
            "Thresh 0.30 | Macro F1: 0.581 | Micro F1: 0.678 | Exact Match: 0.491\n",
            "Thresh 0.35 | Macro F1: 0.568 | Micro F1: 0.688 | Exact Match: 0.587\n",
            "Thresh 0.40 | Macro F1: 0.560 | Micro F1: 0.693 | Exact Match: 0.636\n",
            "Thresh 0.45 | Macro F1: 0.543 | Micro F1: 0.682 | Exact Match: 0.644\n",
            "Thresh 0.50 | Macro F1: 0.532 | Micro F1: 0.668 | Exact Match: 0.599\n",
            "Thresh 0.55 | Macro F1: 0.513 | Micro F1: 0.636 | Exact Match: 0.534\n",
            "Thresh 0.60 | Macro F1: 0.474 | Micro F1: 0.581 | Exact Match: 0.449\n",
            "Thresh 0.65 | Macro F1: 0.418 | Micro F1: 0.500 | Exact Match: 0.353\n",
            "Thresh 0.70 | Macro F1: 0.364 | Micro F1: 0.426 | Exact Match: 0.281\n",
            "Thresh 0.75 | Macro F1: 0.273 | Micro F1: 0.311 | Exact Match: 0.187\n",
            "Thresh 0.80 | Macro F1: 0.190 | Micro F1: 0.201 | Exact Match: 0.112\n",
            "Thresh 0.85 | Macro F1: 0.107 | Micro F1: 0.102 | Exact Match: 0.054\n",
            "\n",
            " Best threshold = 0.20 with Macro F1 = 0.597\n",
            "Validation Macro F1 (Epoch 2): 0.5971\n",
            "Saved best model (Epoch 2) to entity_framing_UA_to_UA-CC.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 603/603 [04:01<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: Loss = 0.4441\n",
            "Threshold sweep results:\n",
            "Thresh 0.10 | Macro F1: 0.687 | Micro F1: 0.703 | Exact Match: 0.408\n",
            "Thresh 0.15 | Macro F1: 0.719 | Micro F1: 0.739 | Exact Match: 0.519\n",
            "Thresh 0.20 | Macro F1: 0.733 | Micro F1: 0.760 | Exact Match: 0.595\n",
            "Thresh 0.25 | Macro F1: 0.741 | Micro F1: 0.773 | Exact Match: 0.645\n",
            "Thresh 0.30 | Macro F1: 0.738 | Micro F1: 0.784 | Exact Match: 0.690\n",
            "Thresh 0.35 | Macro F1: 0.741 | Micro F1: 0.794 | Exact Match: 0.729\n",
            "Thresh 0.40 | Macro F1: 0.732 | Micro F1: 0.796 | Exact Match: 0.756\n",
            "Thresh 0.45 | Macro F1: 0.720 | Micro F1: 0.798 | Exact Match: 0.769\n",
            "Thresh 0.50 | Macro F1: 0.705 | Micro F1: 0.795 | Exact Match: 0.764\n",
            "Thresh 0.55 | Macro F1: 0.691 | Micro F1: 0.789 | Exact Match: 0.744\n",
            "Thresh 0.60 | Macro F1: 0.671 | Micro F1: 0.782 | Exact Match: 0.717\n",
            "Thresh 0.65 | Macro F1: 0.654 | Micro F1: 0.769 | Exact Match: 0.685\n",
            "Thresh 0.70 | Macro F1: 0.624 | Micro F1: 0.750 | Exact Match: 0.644\n",
            "Thresh 0.75 | Macro F1: 0.587 | Micro F1: 0.722 | Exact Match: 0.597\n",
            "Thresh 0.80 | Macro F1: 0.559 | Micro F1: 0.695 | Exact Match: 0.552\n",
            "Thresh 0.85 | Macro F1: 0.513 | Micro F1: 0.636 | Exact Match: 0.477\n",
            "\n",
            " Best threshold = 0.35 with Macro F1 = 0.741\n",
            "Validation Macro F1 (Epoch 3): 0.7414\n",
            "Saved best model (Epoch 3) to entity_framing_UA_to_UA-CC.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 603/603 [03:59<00:00,  2.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4: Loss = 0.3495\n",
            "Threshold sweep results:\n",
            "Thresh 0.10 | Macro F1: 0.728 | Micro F1: 0.755 | Exact Match: 0.522\n",
            "Thresh 0.15 | Macro F1: 0.776 | Micro F1: 0.799 | Exact Match: 0.623\n",
            "Thresh 0.20 | Macro F1: 0.803 | Micro F1: 0.824 | Exact Match: 0.690\n",
            "Thresh 0.25 | Macro F1: 0.823 | Micro F1: 0.841 | Exact Match: 0.738\n",
            "Thresh 0.30 | Macro F1: 0.837 | Micro F1: 0.855 | Exact Match: 0.773\n",
            "Thresh 0.35 | Macro F1: 0.848 | Micro F1: 0.867 | Exact Match: 0.815\n",
            "Thresh 0.40 | Macro F1: 0.858 | Micro F1: 0.878 | Exact Match: 0.850\n",
            "Thresh 0.45 | Macro F1: 0.858 | Micro F1: 0.879 | Exact Match: 0.857\n",
            "Thresh 0.50 | Macro F1: 0.852 | Micro F1: 0.874 | Exact Match: 0.842\n",
            "Thresh 0.55 | Macro F1: 0.846 | Micro F1: 0.869 | Exact Match: 0.821\n",
            "Thresh 0.60 | Macro F1: 0.837 | Micro F1: 0.860 | Exact Match: 0.793\n",
            "Thresh 0.65 | Macro F1: 0.819 | Micro F1: 0.842 | Exact Match: 0.756\n",
            "Thresh 0.70 | Macro F1: 0.804 | Micro F1: 0.826 | Exact Match: 0.722\n",
            "Thresh 0.75 | Macro F1: 0.778 | Micro F1: 0.797 | Exact Match: 0.675\n",
            "Thresh 0.80 | Macro F1: 0.735 | Micro F1: 0.759 | Exact Match: 0.620\n",
            "Thresh 0.85 | Macro F1: 0.670 | Micro F1: 0.697 | Exact Match: 0.539\n",
            "\n",
            " Best threshold = 0.40 with Macro F1 = 0.858\n",
            "Validation Macro F1 (Epoch 4): 0.8584\n",
            "Saved best model (Epoch 4) to entity_framing_UA_to_UA-CC.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ef2657310edf4047b2b0d26f3e295f68",
        "deepnote_cell_type": "markdown",
        "id": "hDyBFHv9f4dl"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "4acc9a72",
        "execution_start": 1747007459670,
        "execution_millis": 4641,
        "execution_context_id": "c4447519-994a-403e-aed3-87536834bebd",
        "cell_id": "e41fec67c22844bfaf5807a5c09279c4",
        "deepnote_cell_type": "code",
        "id": "dRHvqFZhf4dm",
        "outputId": "9c475ff9-1737-469f-e283-6213586d03a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ==========================\n",
        "# EVALUATION (Single Task)\n",
        "# ==========================\n",
        "if TASK != \"multi_task\" and TASK != \"multi_task_adapter\":\n",
        "    print(f\"\\nEvaluating Single-Task Model ({TASK})\")\n",
        "\n",
        "    results_domain = evaluate_per_domain_flat(\n",
        "        trained_model,\n",
        "        val_loader, df_val.reset_index(drop=True),\n",
        "        test_loader, df_test.reset_index(drop=True),\n",
        "        mlb,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    results_class = evaluate_per_class_flat(\n",
        "        trained_model,\n",
        "        test_loader,\n",
        "        df_test.reset_index(drop=True),\n",
        "        mlb,\n",
        "        device=device,\n",
        "        label=\"TEST\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# EVALUATION (Multi-Task)\n",
        "# ==========================\n",
        "elif TASK == \"multi_task\" or TASK == \"multi_task_adapter\":\n",
        "      print(f\"\\nEvaluating Multi-Task Model ({TASK})\")\n",
        "      task_loaders = {\n",
        "          \"narrative_classification\": test_loader_s2,\n",
        "          \"entity_framing\": test_loader_s1,\n",
        "      }\n",
        "\n",
        "      task_dfs = {\n",
        "          \"narrative_classification\": df_test_s2,\n",
        "          \"entity_framing\": df_test_s1,\n",
        "      }\n",
        "\n",
        "      task_targets = {\n",
        "          \"narrative_classification\": y_test_s2,\n",
        "          \"entity_framing\": y_test_s1,\n",
        "      }\n",
        "\n",
        "      task_mlbs = {\n",
        "          \"narrative_classification\": mlb_s2,\n",
        "          \"entity_framing\": mlb_s1,\n",
        "      }\n",
        "\n",
        "      results_mtl = evaluate_mtl_all_tasks(\n",
        "          model=trained_model,\n",
        "          task_loaders=task_loaders,\n",
        "          task_dfs=task_dfs,\n",
        "          task_targets=task_targets,\n",
        "          task_mlbs=task_mlbs,\n",
        "          domain_list=TRAIN_DOMAIN,\n",
        "          device=device,\n",
        "          load_from_disk=False\n",
        "      )\n",
        "\n",
        "\n",
        ""
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Single-Task Model (entity_framing)\n",
            "\n",
            "=========================\n",
            "Validation (Fixed Threshold)\n",
            "=========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating VALIDATION: 100%|██████████| 294/294 [00:34<00:00,  8.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " VALIDATION (Fixed Threshold=0.35):\n",
            "Macro F1: 0.848\n",
            "Micro F1: 0.867\n",
            "Exact Match: 0.815\n",
            "\n",
            "----------------------------\n",
            "Per-Domain Breakdown\n",
            "----------------------------\n",
            "\n",
            " Domain: UA\n",
            "Macro F1: 0.848\n",
            "Micro F1: 0.867\n",
            "Exact Match: 0.815\n",
            "\n",
            "=========================\n",
            "Test (Fixed Threshold)\n",
            "=========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating TEST: 100%|██████████| 56/56 [00:06<00:00,  8.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " TEST (Fixed Threshold=0.35):\n",
            "Macro F1: 0.634\n",
            "Micro F1: 0.637\n",
            "Exact Match: 0.527\n",
            "\n",
            "----------------------------\n",
            "Per-Domain Breakdown\n",
            "----------------------------\n",
            "\n",
            " Domain: CC\n",
            "Macro F1: 0.669\n",
            "Micro F1: 0.770\n",
            "Exact Match: 0.725\n",
            "\n",
            " Domain: UA\n",
            "Macro F1: 0.548\n",
            "Micro F1: 0.605\n",
            "Exact Match: 0.476\n",
            "\n",
            "=========================\n",
            "OOD Generalization (Fixed Threshold)\n",
            "=========================\n",
            "Δ Macro F1 (val - test): 0.214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating TEST: 100%|██████████| 56/56 [00:06<00:00,  8.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TEST (Fixed Threshold=0.35):\n",
            "Macro F1: 0.634\n",
            "Micro F1: 0.637\n",
            "Exact Match: 0.527\n",
            "\n",
            "----------------------------\n",
            "Classification Report (All Domains)\n",
            "----------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Antagonist       0.61      0.73      0.67       168\n",
            "    Innocent       0.61      0.61      0.61       122\n",
            " Protagonist       0.55      0.72      0.62       158\n",
            "\n",
            "   micro avg       0.59      0.69      0.64       448\n",
            "   macro avg       0.59      0.69      0.63       448\n",
            "weighted avg       0.59      0.69      0.64       448\n",
            " samples avg       0.61      0.69      0.64       448\n",
            "\n",
            "\n",
            "----------------------------\n",
            "Per-Domain Breakdown\n",
            "----------------------------\n",
            "\n",
            "Domain: CC\n",
            "Macro F1: 0.669\n",
            "Micro F1: 0.770\n",
            "Exact Match: 0.725\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Antagonist       0.71      0.38      0.50        13\n",
            "    Innocent       0.77      0.92      0.84        60\n",
            " Protagonist       0.67      0.67      0.67        18\n",
            "\n",
            "   micro avg       0.75      0.79      0.77        91\n",
            "   macro avg       0.72      0.66      0.67        91\n",
            "weighted avg       0.74      0.79      0.76        91\n",
            " samples avg       0.76      0.79      0.77        91\n",
            "\n",
            "\n",
            "Domain: UA\n",
            "Macro F1: 0.548\n",
            "Micro F1: 0.605\n",
            "Exact Match: 0.476\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Antagonist       0.61      0.76      0.68       155\n",
            "    Innocent       0.38      0.32      0.35        62\n",
            " Protagonist       0.54      0.72      0.62       140\n",
            "\n",
            "   micro avg       0.55      0.67      0.61       357\n",
            "   macro avg       0.51      0.60      0.55       357\n",
            "weighted avg       0.54      0.67      0.60       357\n",
            " samples avg       0.57      0.67      0.61       357\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "952cc04c4d264dbb997032bc5158a630",
        "deepnote_cell_type": "code",
        "id": "B15cxCN-f4dm"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "28c06ed7bf87409192461e9c1e8d8ba3",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15296fc107174aa0b9439076f942f0b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_140f4658b78944498b589392d1d36233",
              "IPY_MODEL_930b5f1ba05f4195b87a67cb2fa0a63f",
              "IPY_MODEL_c2b92c2d6c364644b9661629d9e63fd4"
            ],
            "layout": "IPY_MODEL_d3734fe20b4a4fd995ae6d0ebb0ffd13"
          }
        },
        "140f4658b78944498b589392d1d36233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6b0e820a2ac4554a02ef262404af56e",
            "placeholder": "​",
            "style": "IPY_MODEL_3e859ad462e0449fac4fadc4221d80d2",
            "value": "config.json: 100%"
          }
        },
        "930b5f1ba05f4195b87a67cb2fa0a63f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f492fa59aa9e4d94b32f781a8e7ed0fc",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfe13871a7404c97bbb58add41c9b563",
            "value": 483
          }
        },
        "c2b92c2d6c364644b9661629d9e63fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f571f60293144697991abc29632479aa",
            "placeholder": "​",
            "style": "IPY_MODEL_90a73516fea0491e93073f2ba67013db",
            "value": " 483/483 [00:00&lt;00:00, 50.5kB/s]"
          }
        },
        "d3734fe20b4a4fd995ae6d0ebb0ffd13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6b0e820a2ac4554a02ef262404af56e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e859ad462e0449fac4fadc4221d80d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f492fa59aa9e4d94b32f781a8e7ed0fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfe13871a7404c97bbb58add41c9b563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f571f60293144697991abc29632479aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90a73516fea0491e93073f2ba67013db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f4e2f2fb8494e51aa770893f714121d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5611ef749993464494d197ec34eacc44",
              "IPY_MODEL_b045e35760c94acd818be2cfba0e035b",
              "IPY_MODEL_45cbb84f0f7940838d2d563205ab738b"
            ],
            "layout": "IPY_MODEL_977173f34b5a4954a94567d6059e6ba4"
          }
        },
        "5611ef749993464494d197ec34eacc44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94dd1db7c4494407be7b5ebd1841187c",
            "placeholder": "​",
            "style": "IPY_MODEL_4c32bdbf849640d48798ec7bcc197f1f",
            "value": "config.json: 100%"
          }
        },
        "b045e35760c94acd818be2cfba0e035b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad5e7517f28d4e019ec2e7dbb8c4a4a9",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_563c3a62e9124acfa3677a8539f4b74e",
            "value": 483
          }
        },
        "45cbb84f0f7940838d2d563205ab738b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53a82562434e467193ca048c315131a7",
            "placeholder": "​",
            "style": "IPY_MODEL_91530a7f44c242edb65e46da3d864660",
            "value": " 483/483 [00:00&lt;00:00, 57.8kB/s]"
          }
        },
        "977173f34b5a4954a94567d6059e6ba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94dd1db7c4494407be7b5ebd1841187c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c32bdbf849640d48798ec7bcc197f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad5e7517f28d4e019ec2e7dbb8c4a4a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "563c3a62e9124acfa3677a8539f4b74e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53a82562434e467193ca048c315131a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91530a7f44c242edb65e46da3d864660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ffd88c765fc4cd5b6b00cd02f9d8d35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06a53bab0ab6477693ce55ccf7804488",
              "IPY_MODEL_dcdd36d23d6b46fcabe1efa05d0ee7fe",
              "IPY_MODEL_5cfa13adb2e54f0494a7d6f701d9f9a8"
            ],
            "layout": "IPY_MODEL_f354fb535771424894a59bc1385df74c"
          }
        },
        "06a53bab0ab6477693ce55ccf7804488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c108df9b26e4484b98313147df00d288",
            "placeholder": "​",
            "style": "IPY_MODEL_3db3f85ad2d446dcb1b308dace00a858",
            "value": "model.safetensors: 100%"
          }
        },
        "dcdd36d23d6b46fcabe1efa05d0ee7fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03ba90997f354c67bf52e43078c4bbb6",
            "max": 267954768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5c77c2fed14433c99ebb25a452fa32b",
            "value": 267954768
          }
        },
        "5cfa13adb2e54f0494a7d6f701d9f9a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6259133122854a1a848c17c3dcbc0872",
            "placeholder": "​",
            "style": "IPY_MODEL_766440dc93f94e26aa2ad496b77639de",
            "value": " 268M/268M [00:01&lt;00:00, 198MB/s]"
          }
        },
        "f354fb535771424894a59bc1385df74c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c108df9b26e4484b98313147df00d288": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3db3f85ad2d446dcb1b308dace00a858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03ba90997f354c67bf52e43078c4bbb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5c77c2fed14433c99ebb25a452fa32b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6259133122854a1a848c17c3dcbc0872": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "766440dc93f94e26aa2ad496b77639de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  }
}