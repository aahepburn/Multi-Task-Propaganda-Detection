{
    "cells": [
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "24b055ea",
                "execution_start": 1747172962328,
                "execution_millis": 10321,
                "execution_context_id": "71277792-1256-48f8-957b-3fd929836935",
                "cell_id": "ab1260066dce4271b3037f622527d193",
                "deepnote_cell_type": "code"
            },
            "source": "# RUN THESE IMPORTS FIRST\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DebertaV2Tokenizer, AutoModel, RobertaTokenizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score, classification_report\nimport numpy as np\nimport shap\nfrom captum.attr import IntegratedGradients\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nimport torch.nn.functional as F\nimport hf_xet\nimport optuna\nimport sys\nimport importlib # !pip install importlib\nsys.path.append('.')\n\n\n### Custom built modules ###\nimport importlib\n\nimport data_loader_STL\nimportlib.reload(data_loader_STL)\nfrom data_loader_STL import prepare_data_STL_fine, prepare_data_STL_hierarchical, prepare_data_STL_coarse\n\nimport single_task\nimportlib.reload(single_task)\nfrom single_task import TransformerClassifier, MultiLabelDataset, train_single_task_model, train_hierarchical_classifier\n\nimport multi_task\nimportlib.reload(multi_task)\nfrom multi_task import MultiTaskTransformer, train_mtl_flat, train_mtl_hierarchical, apply_hierarchical_constraints_mtl, hierarchical_loss_mtl, AdapterMultiTaskTransformer\n\nimport data_loader_MTL\nimportlib.reload(data_loader_MTL)\nfrom data_loader_MTL import prepare_data_MTL_fine_flat, prepare_data_MTL_hierarchical, prepare_data_MTL_coarse, MultiTaskDataset\n\nimport evaluation_utils as eval_util\nimportlib.reload(eval_util)\nfrom evaluation_utils import evaluate_flat, evaluate_hierarchy, evaluate_mtl_all_tasks, evaluate_per_class_flat, evaluate_per_domain_flat, predict_proba, evaluate_threshold_sweep, evaluate_mtl_hierarchical_task, evaluate_mtl_hierarchical_all_tasks, evaluate_flat_custom, compute_fine_vs_coarse_metrics, get_coarse_label_list\n",
            "block_group": "e0cb8b09157f411ba4e31f52c44d9a0d",
            "execution_count": 1,
            "outputs": [
                {
                    "name": "stderr",
                    "text": "/root/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "dbtable:cell_outputs/7466a3a3-c35a-4bab-9d4a-db47a8637877",
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "f20302f5",
                "execution_start": 1747151869155,
                "execution_millis": 279,
                "execution_context_id": "ea29cd6f-d986-4027-85ae-49ed29545c24",
                "cell_id": "010a7d027aec4c6f93e7bd7969f14f53",
                "deepnote_cell_type": "code"
            },
            "source": "rm -rf ~/.cache/huggingface\n",
            "block_group": "a0e5719979734a7a95d03b02b557be89",
            "execution_count": 55,
            "outputs": [
                {
                    "name": "stderr",
                    "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "dbtable:cell_outputs/e43add1d-cba2-441a-828d-e07715310b98",
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "ed80e4343e90452d9d4c49673adfe981",
                "deepnote_cell_type": "markdown"
            },
            "source": "# Master Notebook\n\nThrough this interface the user can experiment with all the models and experimental conditions used in the thesis.",
            "block_group": "933c99f2adcd4ebfbc0f16d5feca7b98"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "d1b1137e1bff477a9d7318171c644458",
                "deepnote_cell_type": "markdown"
            },
            "source": "## Hyperparameters",
            "block_group": "318a9802e7f541b58ca7e7a9a578fc10"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "be2ffd0c",
                "is_code_hidden": true,
                "execution_start": 1746887343724,
                "execution_millis": 9678,
                "is_output_hidden": true,
                "execution_context_id": "deb92800-6cc8-4a1a-b0a6-2b27f25ec5f4",
                "deepnote_app_is_code_hidden": true,
                "deepnote_app_is_output_hidden": true,
                "cell_id": "c50b5dce8d574e94b6d9af88126700c0",
                "deepnote_cell_type": "code"
            },
            "source": "import sys\nsys.path.append(\".\")  # Ensure current directory is in path\n\nfrom merged_optuna_script import objective_stl, objective_mtl, objective_mtl_adapter\nimport optuna\nimport pandas as pd\n\n# === Fast Experiment Sweep ===\nEXPERIMENTS = [\n    {\"setup\": \"stl\", \"task\": \"entity_framing\", \"encoder\": \"roberta-base\"},\n    {\"setup\": \"stl\", \"task\": \"narrative_classification\", \"encoder\": \"roberta-base\"},\n    {\"setup\": \"mtl\", \"task\": None, \"encoder\": \"roberta-base\"},\n    {\"setup\": \"stl\", \"task\": \"entity_framing\", \"encoder\": \"distilbert-base-uncased\"},\n    {\"setup\": \"stl\", \"task\": \"narrative_classification\", \"encoder\": \"distilbert-base-uncased\"},\n    {\"setup\": \"mtl\", \"task\": None, \"encoder\": \"distilbert-base-uncased\"},\n    {\"setup\": \"mtl_adapter\", \"task\": None, \"encoder\": \"roberta-base\"},\n    {\"setup\": \"mtl_adapter\", \"task\": None, \"encoder\": \"distilbert-base-uncased\"},\n]\n\nall_results = []\n\nfor config in EXPERIMENTS:\n    setup = config[\"setup\"]\n    task = config[\"task\"]\n    encoder = config[\"encoder\"]\n\n    print(f\"\\n Starting Optuna Study → Setup: {setup.upper()} | Task: {task or 'MTL'} | Encoder: {encoder}\")\n    \n    study = optuna.create_study(direction=\"maximize\")\n\n    if setup == \"stl\":\n        study.optimize(lambda trial: objective_stl(trial, task_type=task, model_name=encoder), n_trials=3)\n    elif setup == \"mtl\":\n        study.optimize(lambda trial: objective_mtl(trial, model_name=encoder), n_trials=3)\n    elif setup == \"mtl_adapter\":\n        study.optimize(lambda trial: objective_mtl_adapter(trial, model_name=encoder), n_trials=3)\n    else:\n        raise ValueError(f\"Unknown setup: {setup}\")\n\n    best_params = study.best_trial.params\n    best_score = study.best_trial.value\n\n    print(f\"\\n Best hyperparameters for {setup.upper()} | {task or 'MTL'} | {encoder}:\")\n    for k, v in best_params.items():\n        print(f\"  {k}: {v}\")\n    print(f\"  score: {best_score:.4f}\")\n\n    all_results.append({\n        \"setup\": setup,\n        \"task\": task or \"mtl\",\n        \"encoder\": encoder,\n        \"score\": best_score,\n        **best_params\n    })\n\n# === Save results ===\ndf = pd.DataFrame(all_results)\ndf.to_csv(\"optuna_quick_sweep_results_adapter.csv\", index=False)\nprint(\"\\n Saved results to optuna_quick_sweep_results.csv\")\n",
            "block_group": "263b1ebde4fc4c5e95d1d713f784bfe2",
            "execution_count": 2,
            "outputs": [
                {
                    "name": "stderr",
                    "text": "[I 2025-05-10 10:29:04,051] A new study created in memory with name: no-name-efb61a5a-df9a-4bc7-a131-8da529d3017d\n\n Starting Optuna Study → Setup: MTL_ADAPTER | Task: MTL | Encoder: roberta-base\nException ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fca33dea170>>\nTraceback (most recent call last):\n  File \"/toolkit-cache/0.2.16/python3.10/kernel-libs/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\nKeyboardInterrupt: \nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nStarting Epoch 1/2...\n[W 2025-05-10 10:29:12,061] Trial 0 failed with parameters: {'learning_rate': 2.5620923423875518e-05, 'batch_size': 8, 'epochs': 2, 'threshold': 0.2050096184706236} because of the following error: KeyboardInterrupt().\nTraceback (most recent call last):\n  File \"/root/venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n  File \"/tmp/ipykernel_280/1264774272.py\", line 36, in <lambda>\n    study.optimize(lambda trial: objective_mtl_adapter(trial, model_name=encoder), n_trials=3)\n  File \"/root/work/merged_optuna_script.py\", line 166, in objective_mtl_adapter\n    train_mtl_flat(\n  File \"/root/work/multi_task.py\", line 68, in train_mtl_flat\n    loss.backward()\n  File \"/root/venv/lib/python3.10/site-packages/torch/_tensor.py\", line 648, in backward\n    torch.autograd.backward(\n  File \"/root/venv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 353, in backward\n    _engine_run_backward(\n  File \"/root/venv/lib/python3.10/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nKeyboardInterrupt\n[W 2025-05-10 10:29:12,277] Trial 0 failed with value None.\n",
                    "output_type": "stream"
                },
                {
                    "output_type": "error",
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[2], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective_mtl(trial, model_name\u001b[38;5;241m=\u001b[39mencoder), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m setup \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmtl_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective_mtl_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown setup: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msetup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
                        "Cell \u001b[0;32mIn[2], line 36\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     34\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective_mtl(trial, model_name\u001b[38;5;241m=\u001b[39mencoder), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m setup \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmtl_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 36\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective_mtl_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown setup: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msetup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[0;32m~/work/merged_optuna_script.py:166\u001b[0m, in \u001b[0;36mobjective_mtl_adapter\u001b[0;34m(trial, model_name)\u001b[0m\n\u001b[1;32m    163\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    164\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[0;32m--> 166\u001b[0m \u001b[43mtrain_mtl_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnarrative_classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_s2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity_framing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_s1\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnarrative_classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader_s2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val_s2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_s2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlb_s2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity_framing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader_s1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val_s1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_s1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlb_s1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnarrative_classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlb_s2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity_framing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlb_s1\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_domain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_domain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m results \u001b[38;5;241m=\u001b[39m eval_util\u001b[38;5;241m.\u001b[39mevaluate_mtl_all_tasks(\n\u001b[1;32m    189\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    190\u001b[0m     task_loaders\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     load_from_disk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    209\u001b[0m )\n\u001b[1;32m    211\u001b[0m f1s \u001b[38;5;241m=\u001b[39m [v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mvalues()]\n",
                        "File \u001b[0;32m~/work/multi_task.py:68\u001b[0m, in \u001b[0;36mtrain_mtl_flat\u001b[0;34m(model, loaders, val_data, mlbs, optimizer, criterion, device, epochs, train_domain, test_domain)\u001b[0m\n\u001b[1;32m     65\u001b[0m     task_loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     66\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m task_loss\n\u001b[0;32m---> 68\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     70\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "outputs_reference": "s3:deepnote-cell-outputs-production/17093a1d-646c-462e-b5f1-40ab2e2a05c2",
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "31c2319a",
                "is_code_hidden": true,
                "execution_start": 1746918824209,
                "execution_millis": 173639,
                "execution_context_id": "c1121123-96fd-4595-bbe1-655047eb1bd9",
                "deepnote_app_is_code_hidden": true,
                "cell_id": "1112607fb25545b9b9d19854e4729c8f",
                "deepnote_cell_type": "code"
            },
            "source": "import os\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer\nfrom torch.utils.data import DataLoader\n\nfrom single_task import TransformerClassifier, train_single_task_model, MultiLabelDataset\nfrom multi_task import MultiTaskTransformer, AdapterMultiTaskTransformer, train_mtl_flat\nfrom data_loader_STL import prepare_data_STL_fine\nfrom data_loader_MTL import prepare_data_MTL_fine_flat\nfrom evaluation_utils import evaluate_flat_custom, compute_fine_vs_coarse_metrics, get_coarse_label_list\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nPARAMS = {\n    \"learning_rate\": 3e-5,\n    \"batch_size\": 8,\n    \"epochs\": 3,\n    \"threshold\": 0.35,\n    \"max_len\": 512\n}\n\nSETUPS = [\n    #{\"setup\": \"stl\", \"task\": \"entity_framing\", \"encoder\": \"roberta-base\"},\n    #{\"setup\": \"stl\", \"task\": \"narrative_classification\", \"encoder\": \"roberta-base\"},\n    #{\"setup\": \"mtl\", \"task\": None, \"encoder\": \"roberta-base\"},\n    #{\"setup\": \"mtl_adapter\", \"task\": None, \"encoder\": \"roberta-base\"},\n    #{\"setup\": \"stl\", \"task\": \"entity_framing\", \"encoder\": \"distilbert-base-uncased\"},\n    #{\"setup\": \"stl\", \"task\": \"narrative_classification\", \"encoder\": \"distilbert-base-uncased\"},\n   {\"setup\": \"mtl\", \"task\": None, \"encoder\": \"distilbert-base-uncased\"},\n    #{\"setup\": \"mtl_adapter\", \"task\": None, \"encoder\": \"distilbert-base-uncased\"},\n]\n\nTRAIN_SPLITS = [[\"CC\"]]\nEVAL_SPLITS = [\"UA\", \"CC\"]\n\nSUMMARY_COLUMNS = [\n    \"setup\", \"encoder\", \"task\", \"train_domain\", \"eval_domain\",\n    \"overall_macro\", \"overall_micro\", \"overall_exact\",\n    \"macro_fine\", \"micro_fine\", \"macro_coarse\", \"micro_coarse\"\n]\n\nfor config in SETUPS:\n    setup = config[\"setup\"]\n    task = config[\"task\"]\n    encoder = config[\"encoder\"]\n    tokenizer = AutoTokenizer.from_pretrained(encoder)\n\n    setup_name = f\"{setup}_{task or 'mtl'}_{encoder.replace('/', '-')}\"\n    csv_path = f\"results_summary__{setup_name}.csv\"\n    all_rows = []\n\n    for train_domains in TRAIN_SPLITS:\n        train_str = \"+\".join(train_domains)\n\n        if setup == \"stl\":\n            df_train, df_val, df_test, y_train, y_val, y_test, mlb, text_col, label_col = prepare_data_STL_fine(\n                task, train_domains, [\"UA\", \"CC\"]\n            )\n            train_dataset = MultiLabelDataset(df_train[text_col].tolist(), y_train, tokenizer, PARAMS[\"max_len\"])\n            val_dataset = MultiLabelDataset(df_val[text_col].tolist(), y_val, tokenizer, PARAMS[\"max_len\"])\n            train_loader = DataLoader(train_dataset, batch_size=PARAMS[\"batch_size\"], shuffle=True)\n            val_loader = DataLoader(val_dataset, batch_size=PARAMS[\"batch_size\"])\n\n            model = TransformerClassifier(encoder, len(mlb.classes_)).to(device)\n            model = train_single_task_model(\n                model=model,\n                train_loader=train_loader,\n                val_loader=val_loader,\n                y_val=y_val,\n                MODEL_PATH=\"tmp.pt\",\n                LEARNING_RATE=PARAMS[\"learning_rate\"],\n                EPOCHS=PARAMS[\"epochs\"],\n                device=device\n            )\n\n            results_fine, results_coarse = {}, {}\n            coarse_list = get_coarse_label_list(task)\n\n            for domain in EVAL_SPLITS:\n                df_eval = df_test[df_test[\"Domain\"] == domain].copy()\n                known_labels = set(mlb.classes_)\n                df_eval[label_col] = df_eval[label_col].apply(lambda labels: [l for l in labels if l in known_labels])\n                y_eval = mlb.transform(df_eval[label_col])\n                test_loader = DataLoader(\n                    MultiLabelDataset(df_eval[text_col].tolist(), y_eval, tokenizer, PARAMS[\"max_len\"]),\n                    batch_size=PARAMS[\"batch_size\"]\n                )\n                eval_result = evaluate_flat_custom(model, test_loader, df_eval, mlb, device, threshold=PARAMS[\"threshold\"])\n                score_dict = compute_fine_vs_coarse_metrics(eval_result[\"y_true\"], eval_result[\"y_pred_bin\"], list(mlb.classes_), coarse_list)\n\n                all_rows.append({\n                    \"setup\": setup,\n                    \"encoder\": encoder,\n                    \"task\": task,\n                    \"train_domain\": train_str,\n                    \"eval_domain\": domain,\n                    \"overall_macro\": round((score_dict[\"macro_fine\"] + score_dict[\"macro_coarse\"]) / 2, 4),\n                    \"overall_micro\": round((score_dict[\"micro_fine\"] + score_dict[\"micro_coarse\"]) / 2, 4),\n                    \"overall_exact\": round(eval_result[\"exact\"], 4),\n                    \"macro_fine\": round(score_dict[\"macro_fine\"], 4),\n                    \"micro_fine\": round(score_dict[\"micro_fine\"], 4),\n                    \"macro_coarse\": round(score_dict[\"macro_coarse\"], 4),\n                    \"micro_coarse\": round(score_dict[\"micro_coarse\"], 4)\n                })\n\n        elif setup in [\"mtl\", \"mtl_adapter\"]:\n            (\n                df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n                df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n                train_loader_s1, val_loader_s1, test_loader_s1,\n                train_loader_s2, val_loader_s2, test_loader_s2,\n                num_classes_dict\n            ) = prepare_data_MTL_fine_flat(\n                TASK=\"multi_task\",\n                model_name=encoder,\n                max_len=PARAMS[\"max_len\"],\n                batch_size=PARAMS[\"batch_size\"],\n                train_domains=train_domains,\n                test_domains=[\"UA\", \"CC\"],\n                train_languages=[\"ALL\"]\n            )\n\n            task_classes = {\n                \"entity_framing\": y_train_s1.shape[1],\n                \"narrative_classification\": y_train_s2.shape[1]\n            }\n            model = MultiTaskTransformer(encoder, task_classes).to(device) if setup == \"mtl\" else \\\n                AdapterMultiTaskTransformer(model_name=encoder, num_classes_dict=task_classes).to(device)\n\n            optimizer = torch.optim.AdamW(model.parameters(), lr=PARAMS[\"learning_rate\"])\n            criterion = torch.nn.BCEWithLogitsLoss()\n\n            train_mtl_flat(\n                model=model,\n                loaders={\"entity_framing\": train_loader_s1, \"narrative_classification\": train_loader_s2},\n                val_data={\n                    \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1),\n                    \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2)\n                },\n                mlbs={\"entity_framing\": mlb_s1, \"narrative_classification\": mlb_s2},\n                optimizer=optimizer,\n                criterion=criterion,\n                device=device,\n                epochs=PARAMS[\"epochs\"],\n                train_domain=train_domains,\n                test_domain=[\"UA\", \"CC\"]\n            )\n\n            for domain in EVAL_SPLITS:\n                for subtask, df_test, mlb, text_key, label_key in [\n                    (\"entity_framing\", df_test_s1, mlb_s1, \"Input_Text\", \"Label\"),\n                    (\"narrative_classification\", df_test_s2, mlb_s2, \"Translated_Text\", \"Label\")\n                ]:\n                    df_eval = df_test[df_test[\"Domain\"] == domain].copy()\n                    known_labels = set(mlb.classes_)\n                    df_eval[label_key] = df_eval[label_key].apply(\n                        lambda labels: [l for l in labels if l in known_labels] if isinstance(labels, list) else []\n                    )\n                    y_eval = mlb.transform(df_eval[label_key])\n\n                    test_loader = DataLoader(\n                        MultiLabelDataset(df_eval[text_key].tolist(), y_eval, tokenizer, PARAMS[\"max_len\"]),\n                        batch_size=PARAMS[\"batch_size\"]\n                    )\n\n                    model_path = f\"{subtask}_MTL_{'-'.join(train_domains)}_to_{'-'.join(EVAL_SPLITS)}.pt\"\n                    if os.path.exists(model_path):\n                        model.load_state_dict(torch.load(model_path))\n                        model.to(device)\n                        print(f\"✅ Loaded model for {subtask}\")\n                    else:\n                        print(f\"⚠️ Missing checkpoint: {model_path}\")\n\n\n                    eval_result = evaluate_flat_custom(\n                        model=model,\n                        loader=test_loader,\n                        df_source=df_eval,\n                        mlb=mlb,\n                        device=device,\n                        threshold=PARAMS[\"threshold\"],\n                        task=subtask\n                    )\n                    coarse_list = get_coarse_label_list(subtask)\n                    score_dict = compute_fine_vs_coarse_metrics(\n                        eval_result[\"y_true\"], eval_result[\"y_pred_bin\"], list(mlb.classes_), coarse_list\n                    )\n\n                    all_rows.append({\n                        \"setup\": setup,\n                        \"encoder\": encoder,\n                        \"task\": subtask,\n                        \"train_domain\": train_str,\n                        \"eval_domain\": domain,\n                        \"overall_macro\": round((score_dict[\"macro_fine\"] + score_dict[\"macro_coarse\"]) / 2, 4),\n                        \"overall_micro\": round((score_dict[\"micro_fine\"] + score_dict[\"micro_coarse\"]) / 2, 4),\n                        \"overall_exact\": round(eval_result[\"exact\"], 4),\n                        \"macro_fine\": round(score_dict[\"macro_fine\"], 4),\n                        \"micro_fine\": round(score_dict[\"micro_fine\"], 4),\n                        \"macro_coarse\": round(score_dict[\"macro_coarse\"], 4),\n                        \"micro_coarse\": round(score_dict[\"micro_coarse\"], 4),\n                    })\n\n\n    pd.DataFrame(all_rows, columns=SUMMARY_COLUMNS).to_csv(csv_path, index=False)\n    print(f\"✅ Saved: {csv_path}\")\n",
            "block_group": "7611105af9d74224b396c7489f31fa50",
            "execution_count": 40,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "9f01182e",
                "is_code_hidden": true,
                "execution_start": 1746921270867,
                "execution_millis": 11327,
                "execution_context_id": "c1121123-96fd-4595-bbe1-655047eb1bd9",
                "deepnote_app_is_code_hidden": true,
                "cell_id": "c1aaa69fe08344b2a18db7bcf990ab39",
                "deepnote_cell_type": "code"
            },
            "source": "\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nPARAMS = {\n    \"learning_rate\": 3e-5,\n    \"batch_size\": 8,\n    \"epochs\": 3,\n    \"threshold\": 0.35,\n    \"max_len\": 512\n}\n\nSETUPS = [\n    {\"setup\": \"mtl\", \"task\": None, \"encoder\": \"distilbert-base-uncased\"},\n]\n\nTRAIN_SPLITS = [[\"CC\"]]\nEVAL_SPLITS = [\"UA\", \"CC\"]\n\nSUMMARY_COLUMNS = [\n    \"setup\", \"encoder\", \"task\", \"train_domain\", \"eval_domain\",\n    \"overall_macro\", \"overall_micro\", \"overall_exact\",\n    \"macro_fine\", \"micro_fine\", \"macro_coarse\", \"micro_coarse\"\n]\n\nfor config in SETUPS:\n    setup = config[\"setup\"]\n    task = config[\"task\"]\n    encoder = config[\"encoder\"]\n    tokenizer = AutoTokenizer.from_pretrained(encoder)\n\n    setup_name = f\"{setup}_{task or 'mtl'}_{encoder.replace('/', '-')}\"\n    csv_path = f\"results_summary__{setup_name}.csv\"\n    all_rows = []\n\n    for train_domains in TRAIN_SPLITS:\n        train_str = \"+\".join(train_domains)\n\n        if setup == \"stl\":\n            df_train, df_val, df_test, y_train, y_val, y_test, mlb, text_col, label_col = prepare_data_STL_fine(\n                task, train_domains, EVAL_SPLITS\n            )\n            train_dataset = MultiLabelDataset(df_train[text_col].tolist(), y_train, tokenizer, PARAMS[\"max_len\"])\n            val_dataset = MultiLabelDataset(df_val[text_col].tolist(), y_val, tokenizer, PARAMS[\"max_len\"])\n            train_loader = DataLoader(train_dataset, batch_size=PARAMS[\"batch_size\"], shuffle=True)\n            val_loader = DataLoader(val_dataset, batch_size=PARAMS[\"batch_size\"])\n\n            model = TransformerClassifier(encoder, len(mlb.classes_)).to(device)\n            model = train_single_task_model(\n                model=model,\n                train_loader=train_loader,\n                val_loader=val_loader,\n                y_val=y_val,\n                MODEL_PATH=\"tmp.pt\",\n                LEARNING_RATE=PARAMS[\"learning_rate\"],\n                EPOCHS=PARAMS[\"epochs\"],\n                device=device\n            )\n\n            for domain in EVAL_SPLITS:\n                df_eval = df_test[df_test[\"Domain\"] == domain].copy()\n                known_labels = set(mlb.classes_)\n                df_eval[label_col] = df_eval[label_col].apply(lambda labels: [l for l in labels if l in known_labels])\n                y_eval = mlb.transform(df_eval[label_col])\n                test_loader = DataLoader(\n                    MultiLabelDataset(df_eval[text_col].tolist(), y_eval, tokenizer, PARAMS[\"max_len\"]),\n                    batch_size=PARAMS[\"batch_size\"]\n                )\n                eval_result = evaluate_flat_custom(model, test_loader, df_eval, mlb, device, threshold=PARAMS[\"threshold\"])\n                score_dict = compute_fine_vs_coarse_metrics(eval_result[\"y_true\"], eval_result[\"y_pred_bin\"], list(mlb.classes_), get_coarse_label_list(task))\n\n                all_rows.append({\n                    \"setup\": setup,\n                    \"encoder\": encoder,\n                    \"task\": task,\n                    \"train_domain\": train_str,\n                    \"eval_domain\": domain,\n                    \"overall_macro\": round((score_dict[\"macro_fine\"] + score_dict[\"macro_coarse\"]) / 2, 4),\n                    \"overall_micro\": round((score_dict[\"micro_fine\"] + score_dict[\"micro_coarse\"]) / 2, 4),\n                    \"overall_exact\": round(eval_result[\"exact\"], 4),\n                    \"macro_fine\": round(score_dict[\"macro_fine\"], 4),\n                    \"micro_fine\": round(score_dict[\"micro_fine\"], 4),\n                    \"macro_coarse\": round(score_dict[\"macro_coarse\"], 4),\n                    \"micro_coarse\": round(score_dict[\"micro_coarse\"], 4),\n                })\n\n        elif setup in [\"mtl\", \"mtl_adapter\"]:\n            (\n                df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n                df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n                train_loader_s1, val_loader_s1, test_loader_s1,\n                train_loader_s2, val_loader_s2, test_loader_s2,\n                num_classes_dict\n            ) = prepare_data_MTL_fine_flat(\n                TASK=\"multi_task\",\n                model_name=encoder,\n                max_len=PARAMS[\"max_len\"],\n                batch_size=PARAMS[\"batch_size\"],\n                train_domains=train_domains,\n                test_domains=EVAL_SPLITS,\n                train_languages=[\"ALL\"]\n            )\n\n            task_classes = {\n                \"entity_framing\": y_train_s1.shape[1],\n                \"narrative_classification\": y_train_s2.shape[1]\n            }\n            model = MultiTaskTransformer(encoder, task_classes).to(device) if setup == \"mtl\" else \\\n                AdapterMultiTaskTransformer(model_name=encoder, num_classes_dict=task_classes).to(device)\n\n            for subtask in [\"entity_framing\", \"narrative_classification\"]:\n                model_path = f\"{subtask}_MTL_{'-'.join(train_domains)}_to_{'-'.join(EVAL_SPLITS)}.pt\"\n                if os.path.exists(model_path):\n                    model.load_state_dict(torch.load(model_path), strict=False)\n                    print(f\"✅ Loaded model for {subtask}\")\n                else:\n                    print(f\"⚠️ Missing checkpoint: {model_path}\")\n\n                df_test = df_test_s1 if subtask == \"entity_framing\" else df_test_s2\n                mlb = mlb_s1 if subtask == \"entity_framing\" else mlb_s2\n                text_key = \"Input_Text\" if subtask == \"entity_framing\" else \"Translated_Text\"\n                label_key = \"Label\"\n\n                for domain in EVAL_SPLITS:\n                    df_eval = df_test[df_test[\"Domain\"] == domain].copy()\n                    known_labels = set(mlb.classes_)\n                    df_eval[label_key] = df_eval[label_key].apply(lambda labels: [l for l in labels if l in known_labels] if isinstance(labels, list) else [])\n                    y_eval = mlb.transform(df_eval[label_key])\n\n                    test_loader = DataLoader(\n                        MultiTaskDataset(df_eval[text_key].tolist(), {subtask: y_eval}, tokenizer, PARAMS[\"max_len\"]),\n                        batch_size=PARAMS[\"batch_size\"]\n                    )\n\n                    eval_result = evaluate_flat_custom(\n                        model=model,\n                        loader=test_loader,\n                        df_source=df_eval,\n                        mlb=mlb,\n                        device=device,\n                        threshold=PARAMS[\"threshold\"],\n                        task=subtask\n                    )\n                    coarse_list = get_coarse_label_list(subtask)\n                    score_dict = compute_fine_vs_coarse_metrics(eval_result[\"y_true\"], eval_result[\"y_pred_bin\"], list(mlb.classes_), coarse_list)\n\n                    all_rows.append({\n                        \"setup\": setup,\n                        \"encoder\": encoder,\n                        \"task\": subtask,\n                        \"train_domain\": train_str,\n                        \"eval_domain\": domain,\n                        \"overall_macro\": round((score_dict[\"macro_fine\"] + score_dict[\"macro_coarse\"]) / 2, 4),\n                        \"overall_micro\": round((score_dict[\"micro_fine\"] + score_dict[\"micro_coarse\"]) / 2, 4),\n                        \"overall_exact\": round(eval_result[\"exact\"], 4),\n                        \"macro_fine\": round(score_dict[\"macro_fine\"], 4),\n                        \"micro_fine\": round(score_dict[\"micro_fine\"], 4),\n                        \"macro_coarse\": round(score_dict[\"macro_coarse\"], 4),\n                        \"micro_coarse\": round(score_dict[\"micro_coarse\"], 4),\n                    })\n\n    pd.DataFrame(all_rows, columns=SUMMARY_COLUMNS).to_csv(csv_path, index=False)\n    print(f\"✅ Saved: {csv_path}\")\n",
            "block_group": "4fc3d86176ed4b7aaa5164ee21ab2d86",
            "execution_count": 13,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "8ef97b96fbf142f4a3edbb657eeb9b03",
                "deepnote_cell_type": "markdown"
            },
            "source": "## Control Panel",
            "block_group": "4eebb8bf059549c2814bb1b392d9fdcc"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "d9733edf",
                "execution_start": 1747172972697,
                "execution_millis": 548,
                "execution_context_id": "71277792-1256-48f8-957b-3fd929836935",
                "cell_id": "c6c33bfb7b6440a98762b17b647c356f",
                "deepnote_cell_type": "code"
            },
            "source": "# Choose a task for the pipeline below: \"narrative_classification\" or \"entity_framing\" or \"multi_task\" or \"multi_task_adapter\"\nTASK = \"entity_framing\"\n\n# select domains for training and testing: \"UA\"; \"CC\"; \"UA\", \"CC\";\nTRAIN_DOMAIN = [\"UA\"]\nTEST_DOMAIN = [\"UA\", \"CC\"] # The test data comes from a separate dataset.\n# The test data is always the same regardless of the domain we choose to train on. This is for consistency.\n\n# select languages for training and testing: \"ALL\";\"EN\";\"HI\";\"BG\";\"RU\";\"PT\"\nTRAIN_LANGUAGES = [\"ALL\"]\nTEST_LANGUAGES = [\"ALL\"]\n\n# Taxonomy Depth\nTAXONOMY_DEPTH = \"FINE\" # \"COARSE\" OR \"FINE\"\n\n# Classifier Complexity\nCLASSIFIER_COMPLEXITY = \"FLAT\" # \"FLAT\" OR \"HIERARCHICAL\"\n\n# change the training hyperparameters here\nMODEL_NAME = \"roberta-base\" # OR  \"distilbert-base-uncased\" \"roberta-base\" \"\"FacebookAI/roberta-base\"\"\nMAX_LEN = 512\nBATCH_SIZE = 8\nEPOCHS = 4\nLEARNING_RATE = 3e-5\nMODEL_PATH = f\"{TASK}_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\" # -- to save the model later\n\n#tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n#tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n\n# debug mode -- reduced samples\nDEBUG_MODE = False",
            "block_group": "0b74c5d9b37b4460b5a91f83b34c80e1",
            "execution_count": 2,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "47fe07ac",
                "execution_start": 1747172975516,
                "execution_millis": 2,
                "execution_context_id": "71277792-1256-48f8-957b-3fd929836935",
                "cell_id": "62726c5321f94778994bc298793e96f9",
                "deepnote_cell_type": "code"
            },
            "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
            "block_group": "0484d130f5604e759dca217a1dc26887",
            "execution_count": 5,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "a5495661d9f442768d73571169ad1f5c",
                "deepnote_cell_type": "markdown"
            },
            "source": "## UTILS Assemble Dataset",
            "block_group": "b50fd602af6c47548e80ec00857d49b9"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "a9cc233",
                "execution_start": 1747172981011,
                "execution_millis": 2273,
                "execution_context_id": "71277792-1256-48f8-957b-3fd929836935",
                "cell_id": "e77f7dd60a1a430ebbd37089281db4ea",
                "deepnote_cell_type": "code"
            },
            "source": "if TASK != \"multi_task\" and TASK != \"multi_task_adapter\":\n    if TAXONOMY_DEPTH == 'FINE':\n        if CLASSIFIER_COMPLEXITY == 'FLAT':\n            df_train, df_val, df_test, y_train, y_val, y_test, mlb, TEXT_COL, LABEL_COL = prepare_data_STL_fine(\n                TASK,\n                TRAIN_DOMAIN,\n                TEST_DOMAIN,\n            )\n        elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n            df_train, df_val, df_test, y_train, y_val, y_test, mlb, TEXT_COL, LABEL_COL, child_to_parent, label_to_index = prepare_data_STL_hierarchical(\n                TASK,\n                TRAIN_DOMAIN,\n                TEST_DOMAIN,\n            )\n\n\n    elif TAXONOMY_DEPTH == 'COARSE':\n        df_train, df_val, df_test, y_train, y_val, y_test, mlb, TEXT_COL, LABEL_COL = prepare_data_STL_coarse(\n                TASK,\n                TRAIN_DOMAIN,\n                TEST_DOMAIN,\n            )\n\n    train_dataset = MultiLabelDataset(df_train[TEXT_COL].tolist(), y_train, tokenizer, MAX_LEN)\n    val_dataset = MultiLabelDataset(df_val[TEXT_COL].tolist(), y_val, tokenizer, MAX_LEN)\n    test_dataset = MultiLabelDataset(df_test[TEXT_COL].tolist(), y_test, tokenizer, MAX_LEN)\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n    num_classes = len(mlb.classes_)\n\n\nelif TASK == \"multi_task\" or TASK == \"multi_task_adapter\":\n\n    if TAXONOMY_DEPTH == 'FINE':\n        \n        if CLASSIFIER_COMPLEXITY == 'FLAT':\n            (\n                df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n                df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n                train_loader_s1, val_loader_s1, test_loader_s1,\n                train_loader_s2, val_loader_s2, test_loader_s2,\n                num_classes_dict\n            ) = prepare_data_MTL_fine_flat(\n                TASK,\n                train_domains=TRAIN_DOMAIN,\n                test_domains=TEST_DOMAIN,\n                train_languages=TRAIN_LANGUAGES,\n                model_name=MODEL_NAME,\n                max_len=MAX_LEN,\n                batch_size=BATCH_SIZE\n            )\n\n        elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n            (\n                df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n                df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n                train_loader_s1, val_loader_s1, test_loader_s1,\n                train_loader_s2, val_loader_s2, test_loader_s2,\n                num_classes_dict,\n                child_to_parent_map,\n                label_to_index_map\n            ) = prepare_data_MTL_hierarchical(\n                TASK,\n                train_domains=TRAIN_DOMAIN,\n                test_domains=TEST_DOMAIN,\n                train_languages=TRAIN_LANGUAGES,\n                model_name=MODEL_NAME,\n                max_len=MAX_LEN,\n                batch_size=BATCH_SIZE\n            )\n\n    elif TAXONOMY_DEPTH == 'COARSE':\n        (\n            df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n            df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n            train_loader_s1, val_loader_s1, test_loader_s1,\n            train_loader_s2, val_loader_s2, test_loader_s2,\n            num_classes_dict\n        ) = prepare_data_MTL_coarse(\n            TASK,\n            train_domains=TRAIN_DOMAIN,\n            test_domains=TEST_DOMAIN,\n            train_languages=TRAIN_LANGUAGES,\n            model_name=MODEL_NAME,\n            max_len=MAX_LEN,\n            batch_size=BATCH_SIZE\n        )\n\n\n",
            "block_group": "a8230a1f49a14ddc99725da2a7b03ade",
            "execution_count": 8,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "ccbf9456",
                "execution_start": 1747151070131,
                "execution_millis": 132,
                "execution_context_id": "edbab4b7-ade7-480e-afae-17c8794d4116",
                "cell_id": "182fb995cd794effae4016ec82303be3",
                "deepnote_cell_type": "code"
            },
            "source": "def prepare_data_MTL_mixed(\n    task,\n    train_domains,\n    test_domains,\n    train_languages,\n    model_name,\n    max_len,\n    batch_size,\n    granularity_s1=\"fine\",\n    granularity_s2=\"fine\"\n):\n    # Load task 1\n    if granularity_s1 == \"fine\":\n        (\n            df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n            _, _, _, _, _, _, _,  # ignore task2 outputs\n            train_loader_s1, val_loader_s1, test_loader_s1,\n            _, _, _\n        ) = prepare_data_MTL_fine_flat(\n            task,\n            train_domains=train_domains,\n            test_domains=test_domains,\n            train_languages=train_languages,\n            model_name=model_name,\n            max_len=max_len,\n            batch_size=batch_size\n        )\n    elif granularity_s1 == \"coarse\":\n        (\n            df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n            _, _, _, _, _, _, _,  # ignore task2 outputs\n            train_loader_s1, val_loader_s1, test_loader_s1,\n            _, _, _\n        ) = prepare_data_MTL_coarse(\n            task,\n            train_domains=train_domains,\n            test_domains=test_domains,\n            train_languages=train_languages,\n            model_name=model_name,\n            max_len=max_len,\n            batch_size=batch_size\n        )\n\n    # Load task 2\n    if granularity_s2 == \"fine\":\n        (\n            _, _, _, _, _, _, _,\n            df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n            _, _, _,\n            train_loader_s2, val_loader_s2, test_loader_s2,\n            _\n        ) = prepare_data_MTL_fine_flat(\n            task,\n            train_domains=train_domains,\n            test_domains=test_domains,\n            train_languages=train_languages,\n            model_name=model_name,\n            max_len=max_len,\n            batch_size=batch_size\n        )\n    elif granularity_s2 == \"coarse\":\n        (\n            _, _, _, _, _, _, _,\n            df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n            _, _, _,\n            train_loader_s2, val_loader_s2, test_loader_s2,\n            _\n        ) = prepare_data_MTL_coarse(\n            task,\n            train_domains=train_domains,\n            test_domains=test_domains,\n            train_languages=train_languages,\n            model_name=model_name,\n            max_len=max_len,\n            batch_size=batch_size\n        )\n\n    # Class count dictionary (can be reassembled from mlb)\n    num_classes_dict = {\n        \"task1\": len(mlb_s1.classes_),\n        \"task2\": len(mlb_s2.classes_)\n    }\n\n    return (\n        df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n        df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n        train_loader_s1, val_loader_s1, test_loader_s1,\n        train_loader_s2, val_loader_s2, test_loader_s2,\n        num_classes_dict\n    )\n",
            "block_group": "616bb8c931be480c8f25e99114a6f13b",
            "execution_count": 16,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "b4c55c78c616487cb9c73338c9886ab6",
                "deepnote_cell_type": "markdown"
            },
            "source": "## Training Loop",
            "block_group": "0eff6925517b43cf9ccf58f9edc98880"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "c99a6286",
                "execution_start": 1747172985407,
                "execution_millis": 1896,
                "execution_context_id": "71277792-1256-48f8-957b-3fd929836935",
                "cell_id": "3c084ae072e14c75a89644ba77a26804",
                "deepnote_cell_type": "code"
            },
            "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nif TASK != \"multi_task\" and TASK != \"multi_task_adapter\":\n    print(\"\\n>>> Running Single-Task (no adapter) Model <<<\")\n    model = TransformerClassifier(MODEL_NAME, num_classes).to(device)\n\n    if CLASSIFIER_COMPLEXITY == 'FLAT':\n        trained_model = train_single_task_model(\n            model=model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            y_val=y_val,\n            MODEL_PATH=MODEL_PATH,\n            LEARNING_RATE=LEARNING_RATE,\n            EPOCHS=EPOCHS,\n            device=device,\n            predict_proba=eval_util.predict_proba,\n            evaluate_threshold_sweep=eval_util.evaluate_threshold_sweep\n        )\n        trained_model.load_state_dict(torch.load(MODEL_PATH))\n        trained_model.to(device)\n\n    elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n        trained_model = train_hierarchical_classifier(\n            model=model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            y_val=y_val,\n            MODEL_PATH=MODEL_PATH,\n            child_to_parent=child_to_parent,\n            label_to_index=label_to_index,\n            predict_proba=eval_util.predict_proba,\n            evaluate_threshold_sweep=eval_util.evaluate_threshold_sweep,\n            LEARNING_RATE=LEARNING_RATE,\n            EPOCHS=EPOCHS\n        )\n        trained_model.load_state_dict(torch.load(MODEL_PATH))\n        trained_model.to(device)\n\nelif TASK == \"multi_task\":\n    print(\"\\n>>> Running Multi-Task (no adapter) Model <<<\")\n    task_classes = {\n        \"narrative_classification\": y_train_s2.shape[1],\n        \"entity_framing\": y_train_s1.shape[1]\n    }\n    model = MultiTaskTransformer(MODEL_NAME, task_classes).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n    criterion = nn.BCEWithLogitsLoss()\n\n    if CLASSIFIER_COMPLEXITY == 'FLAT':\n        train_mtl_flat(\n            model=model,\n            loaders={\n                \"narrative_classification\": train_loader_s2,\n                \"entity_framing\": train_loader_s1\n            },\n            val_data={\n                \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2),\n                \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1)\n            },\n            mlbs={\n                \"narrative_classification\": mlb_s2,\n                \"entity_framing\": mlb_s1\n            },\n            optimizer=optimizer,\n            criterion=criterion,\n            device=device,\n            epochs=EPOCHS,\n            train_domain=TRAIN_DOMAIN,\n            test_domain=TEST_DOMAIN\n        )\n\n    elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n        train_mtl_hierarchical(\n            model=model,\n            loaders={\n                \"narrative_classification\": train_loader_s2,\n                \"entity_framing\": train_loader_s1\n            },\n            val_data={\n                \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2),\n                \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1)\n            },\n            child_to_parent_map=child_to_parent_map,\n            label_to_index_map=label_to_index_map,\n            optimizer=optimizer,\n            criterion=criterion,\n            device=device,\n            epochs=EPOCHS,\n            train_domain=TRAIN_DOMAIN,\n            test_domain=TEST_DOMAIN\n        )\n\n\n    # Re-load best saved model per task\n    model.load_state_dict(torch.load(f\"entity_framing_MTL_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\"), strict=False)\n    model.load_state_dict(torch.load(f\"narrative_classification_MTL_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\"), strict=False)\n    trained_model = model\n\n\nelif TASK == \"multi_task_adapter\":\n    print(\"\\n>>> Running Multi-Task Adapter Model <<<\")\n    \n    task_classes = {\n        \"narrative_classification\": y_train_s2.shape[1],\n        \"entity_framing\": y_train_s1.shape[1]\n    }\n    \n    model = AdapterMultiTaskTransformer(\n        model_name=MODEL_NAME,\n        num_classes_dict=task_classes,\n        adapter_dim=128 \n    ).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n    criterion = nn.BCEWithLogitsLoss()\n\n    if CLASSIFIER_COMPLEXITY == 'FLAT':\n        train_mtl_flat(\n            model=model,\n            loaders={\n                \"narrative_classification\": train_loader_s2,\n                \"entity_framing\": train_loader_s1\n            },\n            val_data={\n                \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2),\n                \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1)\n            },\n            mlbs={\n                \"narrative_classification\": mlb_s2,\n                \"entity_framing\": mlb_s1\n            },\n            optimizer=optimizer,\n            criterion=criterion,\n            device=device,\n            epochs=EPOCHS,\n            train_domain=TRAIN_DOMAIN,\n            test_domain=TEST_DOMAIN\n        )\n\n    # load best saved models\n    model.load_state_dict(torch.load(f\"entity_framing_MTL_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\"), strict=False)\n    model.load_state_dict(torch.load(f\"narrative_classification_MTL_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\"), strict=False)\n    trained_model = model\n",
            "block_group": "3fe48995adf346389329b91b32defed8",
            "execution_count": 11,
            "outputs": [
                {
                    "name": "stdout",
                    "text": "\n>>> Running Single-Task (no adapter) Model <<<\n",
                    "output_type": "stream"
                },
                {
                    "output_type": "error",
                    "ename": "OSError",
                    "evalue": "Can't load the model for 'roberta-base'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'roberta-base' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:585\u001b[0m, in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhf_xet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyXetDownloadInfo, download_files  \u001b[38;5;66;03m# type: ignore[no-redef]\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
                        "\u001b[0;31mImportError\u001b[0m: cannot import name 'PyXetDownloadInfo' from 'hf_xet' (/root/venv/lib/python3.10/site-packages/hf_xet/__init__.py)",
                        "\nDuring handling of the above exception, another exception occurred:\n",
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3776\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3762\u001b[0m cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3763\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   3764\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3774\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   3775\u001b[0m }\n\u001b[0;32m-> 3776\u001b[0m resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3778\u001b[0m \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   3779\u001b[0m \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1008\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1159\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1159\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1708\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1708\u001b[0m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:587\u001b[0m, in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m--> 587\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use optimized download using Xet storage, you need to install the hf_xet package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    589\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTry `pip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface_hub[hf_xet]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m` or `pip install hf_xet`.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    590\u001b[0m     )\n\u001b[1;32m    592\u001b[0m connection_info \u001b[38;5;241m=\u001b[39m refresh_xet_connection_info(file_data\u001b[38;5;241m=\u001b[39mxet_file_data, headers\u001b[38;5;241m=\u001b[39mheaders)\n",
                        "\u001b[0;31mValueError\u001b[0m: To use optimized download using Xet storage, you need to install the hf_xet package. Try `pip install \"huggingface_hub[hf_xet]\"` or `pip install hf_xet`.",
                        "\nThe above exception was the direct cause of the following exception:\n",
                        "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TASK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_task\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m TASK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_task_adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m>>> Running Single-Task (no adapter) Model <<<\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m CLASSIFIER_COMPLEXITY \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFLAT\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      8\u001b[0m         trained_model \u001b[38;5;241m=\u001b[39m train_single_task_model(\n\u001b[1;32m      9\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     10\u001b[0m             train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m             evaluate_threshold_sweep\u001b[38;5;241m=\u001b[39meval_util\u001b[38;5;241m.\u001b[39mevaluate_threshold_sweep\n\u001b[1;32m     19\u001b[0m         )\n",
                        "File \u001b[0;32m~/work/single_task.py:65\u001b[0m, in \u001b[0;36mTransformerClassifier.__init__\u001b[0;34m(self, model_name, num_classes)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name, num_classes):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.3\u001b[39m)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size, num_classes)\n",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
                        "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3892\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3889\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   3890\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3891\u001b[0m         \u001b[38;5;66;03m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[0;32m-> 3892\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   3893\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load the model for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3894\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3895\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m same name. Otherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3896\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m directory containing a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3897\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3898\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   3900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_local:\n\u001b[1;32m   3901\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading weights file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
                        "\u001b[0;31mOSError\u001b[0m: Can't load the model for 'roberta-base'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'roberta-base' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack."
                    ]
                }
            ],
            "outputs_reference": "s3:deepnote-cell-outputs-production/2f914928-cb83-4364-b7c6-a774f61f0994",
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "ef2657310edf4047b2b0d26f3e295f68",
                "deepnote_cell_type": "markdown"
            },
            "source": "## Evaluation",
            "block_group": "905c5b60c2504577b756d16cfa778da5"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "4acc9a72",
                "execution_start": 1747007459670,
                "execution_millis": 4641,
                "execution_context_id": "c4447519-994a-403e-aed3-87536834bebd",
                "cell_id": "e41fec67c22844bfaf5807a5c09279c4",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# EVALUATION (Single Task)\n# ==========================\nif TASK != \"multi_task\" and TASK != \"multi_task_adapter\":\n    print(f\"\\nEvaluating Single-Task Model ({TASK})\")\n\n    if CLASSIFIER_COMPLEXITY == 'FLAT':\n        results_domain = eval_util.evaluate_per_domain_flat(\n            trained_model,\n            val_loader, df_val.reset_index(drop=True),\n            test_loader, df_test.reset_index(drop=True),\n            mlb,\n            device=device\n        )\n\n        results_class = eval_util.evaluate_per_class_flat(\n            trained_model,\n            test_loader,\n            df_test.reset_index(drop=True),\n            mlb,\n            device=device,\n            label=\"TEST\"\n        )\n\n\n    elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n        results_hierarchical = eval_util.evaluate_and_compare_hierarchical(\n            model=trained_model,\n            val_loader=val_loader,\n            val_df=df_val.reset_index(drop=True),\n            val_targets=y_val,\n            test_loader=test_loader,\n            test_df=df_test.reset_index(drop=True),\n            test_targets=y_test,\n            mlb=mlb,\n            device=device,\n            child_to_parent=child_to_parent,\n            label_to_index=label_to_index\n        )\n\n# ==========================\n# EVALUATION (Multi-Task)\n# ==========================\nelif TASK == \"multi_task\" or TASK == \"multi_task_adapter\":\n    print(f\"\\nEvaluating Multi-Task Model ({TASK})\")\n    if CLASSIFIER_COMPLEXITY == 'FLAT':\n        task_loaders = {\n            \"narrative_classification\": test_loader_s2,\n            \"entity_framing\": test_loader_s1,\n        }\n\n        task_dfs = {\n            \"narrative_classification\": df_test_s2,\n            \"entity_framing\": df_test_s1,\n        }\n\n        task_targets = {\n            \"narrative_classification\": y_test_s2,\n            \"entity_framing\": y_test_s1,\n        }\n\n        task_mlbs = {\n            \"narrative_classification\": mlb_s2,\n            \"entity_framing\": mlb_s1,\n        }\n\n        results_mtl = eval_util.evaluate_mtl_all_tasks(\n            model=trained_model,\n            task_loaders=task_loaders,\n            task_dfs=task_dfs,\n            task_targets=task_targets,\n            task_mlbs=task_mlbs,\n            domain_list=TRAIN_DOMAIN,\n            device=device,\n            load_from_disk=False\n        )\n\n\n    elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n        eval_util.evaluate_mtl_hierarchical_all_tasks(\n            model=trained_model,\n            test_loaders={\n                \"narrative_classification\": test_loader_s2,\n                \"entity_framing\": test_loader_s1\n            },\n            df_tests={\n                \"narrative_classification\": df_test_s2,\n                \"entity_framing\": df_test_s1\n            },\n            y_tests={\n                \"narrative_classification\": y_test_s2,\n                \"entity_framing\": y_test_s1\n            },\n            mlbs={\n                \"narrative_classification\": mlb_s2,\n                \"entity_framing\": mlb_s1\n            },\n            child_to_parent_map=child_to_parent_map,\n            label_to_index_map=label_to_index_map,\n            device=device\n        )\n",
            "block_group": "1110bed05b404da6acf9f44db8f0d24b",
            "execution_count": 244,
            "outputs": [
                {
                    "name": "stdout",
                    "text": "\nEvaluating Multi-Task Model (multi_task_adapter)\n\n--- Task: NARRATIVE_CLASSIFICATION ---\nEvaluating TEST [narrative_classification]: 100%|██████████| 23/23 [00:01<00:00, 17.41it/s]\n\nTEST (narrative_classification) [Threshold=0.35]\nMacro F1: 0.054\nMicro F1: 0.276\nExact Match: 0.062\n\n----------------------------\nPer-Domain Breakdown\n----------------------------\n\nDomain: CC\nMacro F1: 0.072\nMicro F1: 0.456\nExact Match: 0.129\n\nDomain: UA\nMacro F1: 0.000\nMicro F1: 0.000\nExact Match: 0.019\n\n--- Task: ENTITY_FRAMING ---\nEvaluating TEST [entity_framing]: 100%|██████████| 56/56 [00:03<00:00, 16.80it/s]\nTEST (entity_framing) [Threshold=0.35]\nMacro F1: 0.105\nMicro F1: 0.431\nExact Match: 0.134\n\n----------------------------\nPer-Domain Breakdown\n----------------------------\n\nDomain: CC\nMacro F1: 0.129\nMicro F1: 0.675\nExact Match: 0.571\n\nDomain: UA\nMacro F1: 0.073\nMicro F1: 0.355\nExact Match: 0.022\n\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "dbtable:cell_outputs/04279297-6091-4ee9-ac62-bf91656fc235",
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "cell_id": "952cc04c4d264dbb997032bc5158a630",
                "deepnote_cell_type": "code"
            },
            "source": "",
            "block_group": "04cbb46acf1546ccb8160672698c8729",
            "execution_count": null,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=49d39932-ba1f-4621-a036-ab99ade88496' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
            "metadata": {
                "created_in_deepnote_cell": true,
                "deepnote_cell_type": "markdown"
            }
        }
    ],
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "deepnote_persisted_session": {
            "createdAt": "2025-05-13T16:22:54.164Z"
        },
        "deepnote_notebook_id": "28c06ed7bf87409192461e9c1e8d8ba3"
    }
}