{
    "cells": [
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "483bc7ba",
                "execution_start": 1745683106106,
                "execution_millis": 1403,
                "execution_context_id": "6c640453-a614-43c5-bfd5-3d171fbd5481",
                "cell_id": "ab1260066dce4271b3037f622527d193",
                "deepnote_cell_type": "code"
            },
            "source": "# RUN THESE IMPORTS FIRST\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DebertaV2Tokenizer, AutoModel, RobertaTokenizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score, classification_report\nimport numpy as np\nimport shap\nfrom captum.attr import IntegratedGradients\nfrom transformers import AutoTokenizer\nimport torch.nn.functional as F\nimport hf_xet\nimport sys\nimport importlib # !pip install importlib\nsys.path.append('.')\n\n\n### Custom built modules ###\nimport importlib\n\nimport data_loader_STL\nimportlib.reload(data_loader_STL)\nfrom data_loader_STL import prepare_data_STL_fine, prepare_data_STL_hierarchical, prepare_data_STL_coarse\n\nimport single_task\nimportlib.reload(single_task)\nfrom single_task import TransformerClassifier, MultiLabelDataset, train_single_task_model, train_hierarchical_classifier\n\nimport multi_task\nimportlib.reload(multi_task)\nfrom multi_task import MultiTaskTransformer, train_mtl_flat, train_mtl_hierarchical, apply_hierarchical_constraints_mtl, hierarchical_loss_mtl\n\nimport data_loader_MTL\nimportlib.reload(data_loader_MTL)\nfrom data_loader_MTL import prepare_data_MTL_fine_flat, prepare_data_MTL_hierarchical, prepare_data_MTL_coarse, MultiTaskDataset\n\nimport evaluation_utils as eval_util\nimportlib.reload(eval_util)\nfrom evaluation_utils import evaluate_flat, evaluate_hierarchy, evaluate_mtl_all_tasks, evaluate_per_class_flat, evaluate_per_domain_flat, predict_proba, evaluate_threshold_sweep, evaluate_mtl_hierarchical_task, evaluate_mtl_hierarchical_all_tasks\n",
            "block_group": "e0cb8b09157f411ba4e31f52c44d9a0d",
            "execution_count": 19,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "ed80e4343e90452d9d4c49673adfe981",
                "deepnote_cell_type": "markdown"
            },
            "source": "# Master Notebook\n\nThrough this interface the user can experiment with all the models and experimental conditions used in the thesis.",
            "block_group": "933c99f2adcd4ebfbc0f16d5feca7b98"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "8ef97b96fbf142f4a3edbb657eeb9b03",
                "deepnote_cell_type": "markdown"
            },
            "source": "## Control Panel",
            "block_group": "4eebb8bf059549c2814bb1b392d9fdcc"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "e2a0c91d",
                "execution_start": 1745680532823,
                "execution_millis": 535,
                "execution_context_id": "6c640453-a614-43c5-bfd5-3d171fbd5481",
                "cell_id": "c6c33bfb7b6440a98762b17b647c356f",
                "deepnote_cell_type": "code"
            },
            "source": "# Choose a task for the pipeline below: \"narrative_classification\" or \"entity_framing\" or \"multi_task\" or \"multi_task_adapter\"\nTASK = \"multi_task\"\n\n# select domains for training and testing: \"UA\"; \"CC\"; \"UA\", \"CC\";\nTRAIN_DOMAIN = [\"UA\",\"CC\"]\nTEST_DOMAIN = [\"UA\", \"CC\"] # The test data comes from a separate dataset.\n# The test data is always the same regardless of the domain we choose to train on. This is for consistency.\n\n# select languages for training and testing: \"ALL\";\"EN\";\"HI\";\"BG\";\"RU\";\"PT\"\nTRAIN_LANGUAGES = [\"ALL\"]\nTEST_LANGUAGES = [\"ALL\"]\n\n# Taxonomy Depth\nTAXONOMY_DEPTH = \"COARSE\" # \"COARSE\" OR \"FINE\"\n\n# Classifier Complexity\nCLASSIFIER_COMPLEXITY = \"FLAT\" # \"FLAT\" OR \"HIERARCHICAL\"\n\n# change the training hyperparameters here\nMODEL_NAME = \"roberta-base\" # OR \"deberta-v3-base\"\nMAX_LEN = 512\nBATCH_SIZE = 8\nEPOCHS = 3\nLEARNING_RATE = 2e-5\nMODEL_PATH = f\"{TASK}_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\" # -- to save the model later\n\ntokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n#tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n#tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n\n# debug mode -- reduced samples\nDEBUG_MODE = False",
            "block_group": "0b74c5d9b37b4460b5a91f83b34c80e1",
            "execution_count": 7,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "47fe07ac",
                "execution_start": 1745680535789,
                "execution_millis": 1,
                "execution_context_id": "6c640453-a614-43c5-bfd5-3d171fbd5481",
                "cell_id": "62726c5321f94778994bc298793e96f9",
                "deepnote_cell_type": "code"
            },
            "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
            "block_group": "0484d130f5604e759dca217a1dc26887",
            "execution_count": 10,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "a5495661d9f442768d73571169ad1f5c",
                "deepnote_cell_type": "markdown"
            },
            "source": "## UTILS Assemble Dataset",
            "block_group": "b50fd602af6c47548e80ec00857d49b9"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "b27f430e",
                "execution_start": 1745683139106,
                "execution_millis": 1149,
                "execution_context_id": "6c640453-a614-43c5-bfd5-3d171fbd5481",
                "cell_id": "e77f7dd60a1a430ebbd37089281db4ea",
                "deepnote_cell_type": "code"
            },
            "source": "if TASK != \"multi_task\":\n    if TAXONOMY_DEPTH == 'FINE':\n        if CLASSIFIER_COMPLEXITY == 'FLAT':\n            df_train, df_val, df_test, y_train, y_val, y_test, mlb, TEXT_COL, LABEL_COL = prepare_data_STL_fine(\n                TASK,\n                TRAIN_DOMAIN,\n                TEST_DOMAIN,\n            )\n        elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n            df_train, df_val, df_test, y_train, y_val, y_test, mlb, TEXT_COL, LABEL_COL, child_to_parent, label_to_index = prepare_data_STL_hierarchical(\n                TASK,\n                TRAIN_DOMAIN,\n                TEST_DOMAIN,\n            )\n\n\n    elif TAXONOMY_DEPTH == 'COARSE':\n        df_train, df_val, df_test, y_train, y_val, y_test, mlb, TEXT_COL, LABEL_COL = prepare_data_STL_coarse(\n                TASK,\n                TRAIN_DOMAIN,\n                TEST_DOMAIN,\n            )\n\n    train_dataset = MultiLabelDataset(df_train[TEXT_COL].tolist(), y_train, tokenizer, MAX_LEN)\n    val_dataset = MultiLabelDataset(df_val[TEXT_COL].tolist(), y_val, tokenizer, MAX_LEN)\n    test_dataset = MultiLabelDataset(df_test[TEXT_COL].tolist(), y_test, tokenizer, MAX_LEN)\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n    num_classes = len(mlb.classes_)\n\n\nelif TASK == \"multi_task\":\n\n    if TAXONOMY_DEPTH == 'FINE':\n        \n        if CLASSIFIER_COMPLEXITY == 'FLAT':\n            (\n                df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n                df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n                train_loader_s1, val_loader_s1, test_loader_s1,\n                train_loader_s2, val_loader_s2, test_loader_s2,\n                num_classes_dict\n            ) = prepare_data_MTL_fine_flat(\n                TASK,\n                train_domains=TRAIN_DOMAIN,\n                test_domains=TEST_DOMAIN,\n                train_languages=TRAIN_LANGUAGES,\n                model_name=MODEL_NAME,\n                max_len=MAX_LEN,\n                batch_size=BATCH_SIZE\n            )\n\n        elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n            (\n                df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n                df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n                train_loader_s1, val_loader_s1, test_loader_s1,\n                train_loader_s2, val_loader_s2, test_loader_s2,\n                num_classes_dict,\n                child_to_parent_map,\n                label_to_index_map\n            ) = prepare_data_MTL_hierarchical(\n                TASK,\n                train_domains=TRAIN_DOMAIN,\n                test_domains=TEST_DOMAIN,\n                train_languages=TRAIN_LANGUAGES,\n                model_name=MODEL_NAME,\n                max_len=MAX_LEN,\n                batch_size=BATCH_SIZE\n            )\n\n    elif TAXONOMY_DEPTH == 'COARSE':\n        (\n            df_train_s1, df_val_s1, df_test_s1, y_train_s1, y_val_s1, y_test_s1, mlb_s1,\n            df_train_s2, df_val_s2, df_test_s2, y_train_s2, y_val_s2, y_test_s2, mlb_s2,\n            train_loader_s1, val_loader_s1, test_loader_s1,\n            train_loader_s2, val_loader_s2, test_loader_s2,\n            num_classes_dict\n        ) = prepare_data_MTL_coarse(\n            TASK,\n            train_domains=TRAIN_DOMAIN,\n            test_domains=TEST_DOMAIN,\n            train_languages=TRAIN_LANGUAGES,\n            model_name=MODEL_NAME,\n            max_len=MAX_LEN,\n            batch_size=BATCH_SIZE\n        )\n\n\n",
            "block_group": "a8230a1f49a14ddc99725da2a7b03ade",
            "execution_count": 22,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "b4c55c78c616487cb9c73338c9886ab6",
                "deepnote_cell_type": "markdown"
            },
            "source": "## Training Loop",
            "block_group": "0eff6925517b43cf9ccf58f9edc98880"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "70c05727",
                "execution_start": 1745683200926,
                "execution_millis": 1186377,
                "execution_context_id": "6c640453-a614-43c5-bfd5-3d171fbd5481",
                "cell_id": "3c084ae072e14c75a89644ba77a26804",
                "deepnote_cell_type": "code"
            },
            "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nif TASK != \"multi_task\":\n    model = TransformerClassifier(MODEL_NAME, num_classes).to(device)\n\n    if CLASSIFIER_COMPLEXITY == 'FLAT':\n        trained_model = train_single_task_model(\n            model=model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            y_val=y_val,\n            MODEL_PATH=MODEL_PATH,\n            LEARNING_RATE=LEARNING_RATE,\n            EPOCHS=EPOCHS,\n            device=device,\n            predict_proba=eval_util.predict_proba,\n            evaluate_threshold_sweep=eval_util.evaluate_threshold_sweep\n        )\n        trained_model.load_state_dict(torch.load(MODEL_PATH))\n        trained_model.to(device)\n\n    elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n        trained_model = train_hierarchical_classifier(\n            model=model,\n            train_loader=train_loader,\n            val_loader=val_loader,\n            y_val=y_val,\n            MODEL_PATH=MODEL_PATH,\n            child_to_parent=child_to_parent,\n            label_to_index=label_to_index,\n            predict_proba=eval_util.predict_proba,\n            evaluate_threshold_sweep=eval_util.evaluate_threshold_sweep,\n            LEARNING_RATE=LEARNING_RATE,\n            EPOCHS=EPOCHS\n        )\n        trained_model.load_state_dict(torch.load(MODEL_PATH))\n        trained_model.to(device)\n\nelif TASK == \"multi_task\":\n    task_classes = {\n        \"narrative_classification\": y_train_s2.shape[1],\n        \"entity_framing\": y_train_s1.shape[1]\n    }\n    model = MultiTaskTransformer(MODEL_NAME, task_classes).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n    criterion = nn.BCEWithLogitsLoss()\n\n    if CLASSIFIER_COMPLEXITY == 'FLAT':\n        train_mtl_flat(\n            model=model,\n            loaders={\n                \"narrative_classification\": train_loader_s2,\n                \"entity_framing\": train_loader_s1\n            },\n            val_data={\n                \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2),\n                \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1)\n            },\n            mlbs={\n                \"narrative_classification\": mlb_s2,\n                \"entity_framing\": mlb_s1\n            },\n            optimizer=optimizer,\n            criterion=criterion,\n            device=device,\n            epochs=EPOCHS,\n            train_domain=TRAIN_DOMAIN,\n            test_domain=TEST_DOMAIN\n        )\n\n    elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n        train_mtl_hierarchical(\n            model=model,\n            loaders={\n                \"narrative_classification\": train_loader_s2,\n                \"entity_framing\": train_loader_s1\n            },\n            val_data={\n                \"narrative_classification\": (val_loader_s2, df_val_s2, y_val_s2, mlb_s2),\n                \"entity_framing\": (val_loader_s1, df_val_s1, y_val_s1, mlb_s1)\n            },\n            child_to_parent_map=child_to_parent_map,\n            label_to_index_map=label_to_index_map,\n            optimizer=optimizer,\n            criterion=criterion,\n            device=device,\n            epochs=EPOCHS,\n            train_domain=TRAIN_DOMAIN,\n            test_domain=TEST_DOMAIN\n        )\n\n\n    # Re-load best saved model per task\n    model.load_state_dict(torch.load(f\"entity_framing_MTL_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\"), strict=False)\n    model.load_state_dict(torch.load(f\"narrative_classification_MTL_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\"), strict=False)\n    trained_model = model\n",
            "block_group": "3fe48995adf346389329b91b32defed8",
            "execution_count": 28,
            "outputs": [
                {
                    "name": "stderr",
                    "text": "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nStarting Epoch 1/3...\n\nEpoch 1 - Average Loss: 0.8568\n\nValidating task: narrative_classification\n[narrative_classification] Macro F1: 0.8242\nBest model for task 'narrative_classification' saved to narrative_classification_MTL_UA-CC_to_UA-CC.pt\n\nValidating task: entity_framing\n[entity_framing] Macro F1: 0.5292\nBest model for task 'entity_framing' saved to entity_framing_MTL_UA-CC_to_UA-CC.pt\n\nStarting Epoch 2/3...\n\nEpoch 2 - Average Loss: 0.6425\n\nValidating task: narrative_classification\n[narrative_classification] Macro F1: 0.8253\nBest model for task 'narrative_classification' saved to narrative_classification_MTL_UA-CC_to_UA-CC.pt\n\nValidating task: entity_framing\n[entity_framing] Macro F1: 0.5198\n\nStarting Epoch 3/3...\n\nEpoch 3 - Average Loss: 0.5699\n\nValidating task: narrative_classification\n[narrative_classification] Macro F1: 0.8515\nBest model for task 'narrative_classification' saved to narrative_classification_MTL_UA-CC_to_UA-CC.pt\n\nValidating task: entity_framing\n[entity_framing] Macro F1: 0.5215\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "s3:deepnote-cell-outputs-production/ab987d18-2456-47a4-bf13-0cdf8026d637",
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "ef2657310edf4047b2b0d26f3e295f68",
                "deepnote_cell_type": "markdown"
            },
            "source": "## Evaluation",
            "block_group": "905c5b60c2504577b756d16cfa778da5"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "d9e0698e",
                "execution_start": 1745684558896,
                "execution_millis": 8385,
                "execution_context_id": "6c640453-a614-43c5-bfd5-3d171fbd5481",
                "cell_id": "e41fec67c22844bfaf5807a5c09279c4",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# EVALUATION (Single Task)\n# ==========================\nif TASK != \"multi_task\":\n    print(f\"\\nEvaluating Single-Task Model ({TASK})\")\n\n    if CLASSIFIER_COMPLEXITY == 'FLAT':\n        results_domain = eval_util.evaluate_per_domain_flat(\n            val_loader, df_val.reset_index(drop=True),\n            test_loader, df_test.reset_index(drop=True),\n            mlb\n        )\n\n        results_class = eval_util.evaluate_per_class_flat(\n            val_loader, df_val.reset_index(drop=True),\n            test_loader, df_test.reset_index(drop=True),\n            mlb\n        )\n\n    elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n        results_hierarchical = eval_util.evaluate_and_compare_hierarchical(\n            model=trained_model,\n            val_loader=val_loader,\n            val_df=df_val.reset_index(drop=True),\n            val_targets=y_val,\n            test_loader=test_loader,\n            test_df=df_test.reset_index(drop=True),\n            test_targets=y_test,\n            mlb=mlb,\n            child_to_parent=child_to_parent,\n            label_to_index=label_to_index\n        )\n\n# ==========================\n# EVALUATION (Multi-Task)\n# ==========================\nelif TASK == \"multi_task\":\n    if CLASSIFIER_COMPLEXITY == 'FLAT':\n        task_loaders = {\n            \"narrative_classification\": test_loader_s2,\n            \"entity_framing\": test_loader_s1,\n        }\n\n        task_dfs = {\n            \"narrative_classification\": df_test_s2,\n            \"entity_framing\": df_test_s1,\n        }\n\n        task_targets = {\n            \"narrative_classification\": y_test_s2,\n            \"entity_framing\": y_test_s1,\n        }\n\n        task_mlbs = {\n            \"narrative_classification\": mlb_s2,\n            \"entity_framing\": mlb_s1,\n        }\n\n        results_mtl = eval_util.evaluate_mtl_all_tasks(\n            model=trained_model,\n            task_loaders=task_loaders,\n            task_dfs=task_dfs,\n            task_targets=task_targets,\n            task_mlbs=task_mlbs,\n            domain_list=TRAIN_DOMAIN,\n            device=device,\n            load_from_disk=False\n        )\n\n\n    elif CLASSIFIER_COMPLEXITY == 'HIERARCHICAL':\n        eval_util.evaluate_mtl_hierarchical_all_tasks(\n            model=trained_model,\n            test_loaders={\n                \"narrative_classification\": test_loader_s2,\n                \"entity_framing\": test_loader_s1\n            },\n            df_tests={\n                \"narrative_classification\": df_test_s2,\n                \"entity_framing\": df_test_s1\n            },\n            y_tests={\n                \"narrative_classification\": y_test_s2,\n                \"entity_framing\": y_test_s1\n            },\n            mlbs={\n                \"narrative_classification\": mlb_s2,\n                \"entity_framing\": mlb_s1\n            },\n            child_to_parent_map=child_to_parent_map,\n            label_to_index_map=label_to_index_map,\n            device=device\n        )\n",
            "block_group": "1110bed05b404da6acf9f44db8f0d24b",
            "execution_count": 31,
            "outputs": [
                {
                    "name": "stdout",
                    "text": "\n--- Task: NARRATIVE_CLASSIFICATION ---\nEvaluating TEST [narrative_classification]: 100%|██████████| 23/23 [00:02<00:00,  9.68it/s]\n\nTEST (narrative_classification) [Threshold=0.25]\nMacro F1: 0.773\nMicro F1: 0.865\nExact Match: 0.843\n\n----------------------------\nPer-Domain Breakdown\n----------------------------\n\nDomain: CC\nMacro F1: 0.730\nMicro F1: 0.859\nExact Match: 0.836\n\nDomain: UA\nMacro F1: 0.409\nMicro F1: 0.869\nExact Match: 0.848\n\n--- Task: ENTITY_FRAMING ---\nEvaluating TEST [entity_framing]: 100%|██████████| 56/56 [00:06<00:00,  9.30it/s]\nTEST (entity_framing) [Threshold=0.25]\nMacro F1: 0.574\nMicro F1: 0.571\nExact Match: 0.272\n\n----------------------------\nPer-Domain Breakdown\n----------------------------\n\nDomain: CC\nMacro F1: 0.561\nMicro F1: 0.661\nExact Match: 0.371\n\nDomain: UA\nMacro F1: 0.422\nMicro F1: 0.546\nExact Match: 0.245\n\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "dbtable:cell_outputs/ff4672ab-f731-40cf-b48a-6fd0a67f88c3",
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "c96f8192e3544573b93df18fa90d7c87",
                "deepnote_cell_type": "markdown"
            },
            "source": "## Post-Hoc Analysis",
            "block_group": "6123291063444e2dbe03c506ade8ce68"
        },
        {
            "cell_type": "code",
            "metadata": {
                "cell_id": "d323aeb712e64ba1bd0286970c07a65a",
                "deepnote_cell_type": "code"
            },
            "source": "",
            "block_group": "8f5b08241e34497db157dbbd162bfa1b",
            "execution_count": null,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=49d39932-ba1f-4621-a036-ab99ade88496' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
            "metadata": {
                "created_in_deepnote_cell": true,
                "deepnote_cell_type": "markdown"
            }
        }
    ],
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "deepnote_notebook_id": "28c06ed7bf87409192461e9c1e8d8ba3"
    }
}