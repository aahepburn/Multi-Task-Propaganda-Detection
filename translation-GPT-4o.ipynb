{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import time\n",
    "import json"
   ],
   "id": "a5efd2e59230f9e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Machine Translation Pipeline using GPT-4o",
   "id": "93166bf6be610ee8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_raw_text_df = pd.read_csv(\"train_raw_text.csv\")\n",
    "train_raw_text_df = train_raw_text_df[train_raw_text_df[\"Language\"] != \"EN\"]\n",
    "\n",
    "annotations_files = [\"subtask1_RU.csv\", \"subtask1_PT.csv\", \"subtask1_HI.csv\", \"subtask1_BG.csv\"]\n",
    "annotations_df = pd.concat([pd.read_csv(f) for f in annotations_files], ignore_index=True)\n",
    "annotations_df.rename(columns={\"File\": \"Filename\"}, inplace=True)\n",
    "\n",
    "openai_client = openai.OpenAI(api_key=\"api-key\")\n"
   ],
   "id": "11e8e94db7813c98"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-07T18:56:00.656646Z",
     "start_time": "2025-04-07T18:56:00.651074Z"
    }
   },
   "source": [
    "### TRANSLATION UTILS\n",
    "\n",
    "\n",
    "# call OpenAI API\n",
    "def call_openai_api(prompt, role_desc, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"system\", \"content\": role_desc},\n",
    "                          {\"role\": \"user\", \"content\": prompt}],\n",
    "\n",
    "                temperature=0.2\n",
    "            )\n",
    "            result = response.choices[0].message.content.strip()\n",
    "            if result:\n",
    "                return result\n",
    "        except Exception as e:\n",
    "            print(f\"OpenAI API call failed (Attempt {attempt+1}): {e}\")\n",
    "            time.sleep(2)\n",
    "    return None\n",
    "\n",
    "# translate article\n",
    "def translate_text(article):\n",
    "    prompt = f\"\"\"\n",
    "    Translate the following news article into English:\n",
    "\n",
    "    ---\n",
    "    ARTICLE:\n",
    "    {article}\n",
    "    ---\n",
    "\n",
    "    Return only the translated text.\n",
    "    \"\"\"\n",
    "    return call_openai_api(prompt, \"You are a professional translator.\")\n",
    "\n",
    "# translate entities with position matching\n",
    "def translate_entities(article_non_en, article_en, entities):\n",
    "    prompt = f\"\"\"\n",
    "    Given the original non-English article and its English translation, translate the following entities while maintaining their exact occurrence in the English text:\n",
    "\n",
    "    ---\n",
    "    Original Non-English Article:\n",
    "    {article_non_en}\n",
    "\n",
    "    Translated English Article:\n",
    "    {article_en}\n",
    "\n",
    "    Entities:\n",
    "    {', '.join(entities)}\n",
    "    ---\n",
    "\n",
    "    Return a JSON object where each entity maps to its exact English translation found in the text.\n",
    "    \"\"\"\n",
    "    response = call_openai_api(prompt, \"You are a professional translator ensuring entity alignment.\")\n",
    "\n",
    "# debugging\n",
    "    if response:\n",
    "        try:\n",
    "            print(f\"Raw GPT-4o Response: {response}\")\n",
    "            cleaned_response = response.strip('```json').strip('```').strip()\n",
    "            return json.loads(cleaned_response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error parsing JSON response. Raw response:\\n{response}\")\n",
    "            return {}\n",
    "    return {}"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TRANSLATION LOOP\n",
    "\n",
    "translated_articles = []\n",
    "translated_entities = []\n",
    "start_time = time.time()\n",
    "\n",
    "for _, row in train_raw_text_df.iterrows():\n",
    "    filename = row[\"Filename\"]\n",
    "    article_non_en = row[\"Content\"]\n",
    "\n",
    "    entity_group = annotations_df[annotations_df[\"Filename\"] == filename]\n",
    "    if entity_group.empty:\n",
    "        continue  # skip if no matching entities found\n",
    "\n",
    "    entities_to_translate = entity_group[\"Entity\"].tolist()\n",
    "    entity_labels = entity_group[['Label1', 'Label2', 'Label3', 'Label4']].astype(str).agg(lambda x: ', '.join(x.dropna().unique()), axis=1).tolist()  # extract labels from annotation dataset\n",
    "\n",
    "    try:\n",
    "        article_en = translate_text(article_non_en)\n",
    "        if not article_en:\n",
    "            print(f\"Skipping {filename} due to empty translation response.\")\n",
    "            continue\n",
    "\n",
    "        entity_translations = translate_entities(article_non_en, article_en, entities_to_translate)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "        continue\n",
    "\n",
    "    translated_articles.append({\n",
    "        \"Filename\": filename,\n",
    "        \"Translated_Text\": article_en\n",
    "    })\n",
    "\n",
    "    for (original_entity, translated_entity), label in zip(entity_translations.items(), entity_labels):\n",
    "        match_start = article_en.find(translated_entity)\n",
    "        match_end = match_start + len(translated_entity) if match_start != -1 else -1\n",
    "\n",
    "        translated_entities.append({\n",
    "            \"Filename\": filename,\n",
    "            \"Original_Entity\": original_entity,\n",
    "            \"Translated_Entity\": translated_entity,\n",
    "            \"Start\": match_start,\n",
    "            \"End\": match_end,\n",
    "            \"Label\": label\n",
    "        })\n",
    "\n",
    "    print(f\"Processed {filename} in {round(time.time() - start_time, 2)}s\")\n"
   ],
   "id": "845b3d2a8b503da6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "translated_articles_df = pd.DataFrame(translated_articles)\n",
    "translated_articles_df.to_csv(\"translated_articles.csv\", index=False)\n",
    "\n",
    "\n",
    "translated_entities_df = pd.DataFrame(translated_entities)\n",
    "translated_entities_df.to_csv(\"translated_entities.csv\", index=False)\n",
    "\n",
    "print(f\"Processing completed in {round(time.time() - start_time, 2)}s. Output saved.\")"
   ],
   "id": "ef102709f1ed2fc7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
