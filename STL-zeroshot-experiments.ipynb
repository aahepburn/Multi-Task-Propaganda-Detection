{
    "cells": [
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "b618418e",
                "execution_start": 1744247818771,
                "execution_millis": 0,
                "execution_context_id": "11a785d9-7581-4f8f-8acb-f265eb8149b2",
                "cell_id": "22f4fb61c85f45f786bd656d6c268421",
                "deepnote_cell_type": "code"
            },
            "source": "import os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DebertaV2Tokenizer, AutoModel, RobertaTokenizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score, classification_report\nimport numpy as np\nimport shap\nfrom captum.attr import IntegratedGradients\nfrom transformers import AutoTokenizer\nimport torch.nn.functional as F\nimport hf_xet",
            "block_group": "05320c4fb99b445dba66cf205049fdf9",
            "execution_count": 9,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "12814d9b17f24237af546fe699f30ded",
                "deepnote_cell_type": "markdown"
            },
            "source": "# Single-Task Learning\n\nBelow is the current pipeline for single-task learning. The code chunk bellow allows to switch between the two tasks: \"narrative_classification\" and \"entity_framing\".\n\nWe can also specify the training and test domains.",
            "block_group": "b415a08f22c9471c815c4e27d7d8e2be"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "63411438",
                "execution_start": 1744247820099,
                "execution_millis": 1,
                "execution_context_id": "11a785d9-7581-4f8f-8acb-f265eb8149b2",
                "cell_id": "3765e5747cb34a90b5907bcd1335d9e4",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# CONTROL PANEL\n# ==========================\n\n# choose a task for the pipeline below: \"narrative_classification\" or \"entity_framing\"\nTASK = \"narrative_classification\"\n\n# select domains for training and testing: \"UA\"; \"CC\"; \"UA\", \"CC\";\nTRAIN_DOMAIN = [\"UA\"]\nTEST_DOMAIN = [\"UA\", \"CC\"] # The test data comes from a separate dataset. \n# The test data is always the same regardless of the domain we choose to train on. This is for consistency.\n\n\"\"\"\nNote that all articles are now in English, but if we wanted to control for e.g. certain cultural variations of a specific language,\nwe could exclude articles that were originally written in that language.\n\nNot to use the functionality, 'ALL' should be selected.\n\n\"\"\"\n# select languages for training and testing: \"ALL\";\"EN\";\"HI\";\"BG\";\"RU\";\"PT\"\nTRAIN_LANGUAGES = [\"ALL\"] \nTEST_LANGUAGES = [\"ALL\"]\n\n# debug mode -- reduced samples\nDEBUG_MODE = False\n\n# change the training hyperparameters here\nMODEL_NAME = \"roberta-base\" # OR \"deberta-v3-base\"\nMAX_LEN = 512\nBATCH_SIZE = 8\nEPOCHS = 3\nLEARNING_RATE = 2e-5\nMODEL_PATH = f\"{TASK}_STL_{'-'.join(TRAIN_DOMAIN)}_to_{'-'.join(TEST_DOMAIN)}.pt\" # -- to save the model later\n",
            "block_group": "dffc3ccb17bc445e96e81bb3c0e11639",
            "execution_count": 11,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "7bb4e15e",
                "execution_start": 1744247822353,
                "execution_millis": 695,
                "execution_context_id": "11a785d9-7581-4f8f-8acb-f265eb8149b2",
                "cell_id": "5c3ab7801dbd47d784eaf44091b0bb5c",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# LOAD AND MERGE DATA\n# ==========================\n\narticles = pd.read_csv(\"train-all-articles.csv\")\ns1 = pd.read_csv(\"train-S1-labels.csv\")\ns2 = pd.read_csv(\"train-S2-labels.csv\")\n\ntest_s1_articles = pd.read_csv(\"test-S1-articles.csv\")\ntest_s1_labels = pd.read_csv(\"test-S1-labels.csv\")\ntest_s2_articles = pd.read_csv(\"test-S2-articles.csv\")\ntest_s2_labels = pd.read_csv(\"test-S2-labels.csv\")\n\n# ==========================\n# STANDARDISE TEST SET COLUMNS\n# ==========================\n\nif TASK == \"entity_framing\":\n    test_s1_labels.rename(columns={\"Translated_Entity\": \"Entity\"}, inplace=True)\nelif TASK == \"narrative_classification\":\n    test_s2_labels.columns = [\"Filename\", \"Narrative\", \"Subnarrative\"]\n\n# ==========================\n# FILTER + SPLIT TRAIN/VAL\n# ==========================\n\n# filter domains/languages for train/val\nfiltered_articles = articles[articles[\"Domain\"].isin(TRAIN_DOMAIN)]\nif \"ALL\" not in TRAIN_LANGUAGES:\n    filtered_articles = filtered_articles[filtered_articles[\"Language\"].isin(TRAIN_LANGUAGES)]\n\n# 80/20 train/val split\nfiltered_articles = filtered_articles.sample(frac=1, random_state=42).reset_index(drop=True)\nsplit_idx = int(0.8 * len(filtered_articles))\ntrain_articles = filtered_articles.iloc[:split_idx].copy()\nval_articles = filtered_articles.iloc[split_idx:].copy()\n\n# debug subsampling if needed -- off by default\nif DEBUG_MODE:\n    train_articles = train_articles.sample(100)\n    val_articles = val_articles.sample(100)\n    test_s1_articles = test_s1_articles.sample(100)\n    test_s2_articles = test_s2_articles.sample(100)\n",
            "block_group": "f1dd3339e778493fa7f5cc455f638f45",
            "execution_count": 13,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "ed0f8f6c",
                "execution_start": 1744247823287,
                "execution_millis": 15,
                "execution_context_id": "11a785d9-7581-4f8f-8acb-f265eb8149b2",
                "cell_id": "08e86ab7f91041aab8b136908084c21b",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# TASK-SPECIFIC MERGE + PROCESSING\n# ==========================\n\nif TASK == \"narrative_classification\":\n    # Merge articles with S2 labels\n    df_train = pd.merge(train_articles, s2, on=\"Filename\")\n    df_val   = pd.merge(val_articles, s2, on=\"Filename\")\n    df_test  = pd.merge(test_s2_articles, test_s2_labels, on=\"Filename\")\n\n    TEXT_COL = \"Translated_Text\"\n    LABEL_COL = \"Narrative\"\n\n    for df in [df_train, df_val, df_test]:\n        df.dropna(subset=[TEXT_COL, LABEL_COL], inplace=True)\n        df[LABEL_COL] = df[LABEL_COL].apply(\n            lambda x: [s.strip() for s in str(x).split(\";\") if s.strip().lower() != \"nan\"]\n        )\n\n    # Create shared label space from all available narrative data (zero-shot setup)\n    full_set = pd.concat([df_train, df_val, df_test])\n    mlb = MultiLabelBinarizer()\n    mlb.fit(full_set[LABEL_COL])\n\n    y_train = mlb.transform(df_train[LABEL_COL])\n    y_val   = mlb.transform(df_val[LABEL_COL])\n    y_test  = mlb.transform(df_test[LABEL_COL])\n    num_classes = len(mlb.classes_)\n\nelif TASK == \"entity_framing\":\n    # Merge entity labels with articles\n    df_train = pd.merge(s1, train_articles, on=\"Filename\")\n    df_val   = pd.merge(s1, val_articles, on=\"Filename\")\n    df_test  = pd.merge(test_s1_labels, test_s1_articles, on=\"Filename\")\n\n    TEXT_COL = \"Translated_Text\"\n    LABEL_COL = \"Label\"\n\n    def insert_entity_marker(text, start, end):\n        try:\n            start, end = int(start), int(end)\n            return text[:start] + \"[ENTITY]\" + text[start:end] + \"[/ENTITY]\" + text[end:]\n        except:\n            return text\n\n    for df in [df_train, df_val, df_test]:\n        df.dropna(subset=[TEXT_COL, \"Entity\", LABEL_COL, \"Start\", \"End\"], inplace=True)\n        df[\"Start\"] = df[\"Start\"].astype(int)\n        df[\"End\"] = df[\"End\"].astype(int)\n        df[\"Input_Text\"] = df.apply(lambda row: insert_entity_marker(row[TEXT_COL], row[\"Start\"], row[\"End\"]), axis=1)\n        df[LABEL_COL] = df[LABEL_COL].apply(lambda x: [s.strip() for s in str(x).split(\",\") if s.strip().lower() != \"nan\"])\n\n    # For entity framing, create a separate label binarizer\n    mlb = MultiLabelBinarizer()\n    mlb.fit(df_train[LABEL_COL] + df_val[LABEL_COL] + df_test[LABEL_COL])  # full entity label space\n\n    y_train = mlb.transform(df_train[LABEL_COL])\n    y_val   = mlb.transform(df_val[LABEL_COL])\n    y_test  = mlb.transform(df_test[LABEL_COL])\n    num_classes = len(mlb.classes_)\n\nelse:\n    raise ValueError(\"Unknown TASK specified.\")\n",
            "block_group": "b35900c3aeb64991a6d1123c456e6404",
            "execution_count": 15,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "65d4f76c",
                "execution_start": 1744247824117,
                "execution_millis": 151,
                "execution_context_id": "11a785d9-7581-4f8f-8acb-f265eb8149b2",
                "cell_id": "26cff33da24441e482453874b1b4f04b",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# TOKENISATION and DATASET CLASS\n# ==========================\n\ntokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n#tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n\nclass MultiLabelDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float)\n        }\n\ntrain_dataset = MultiLabelDataset(df_train[TEXT_COL].tolist(), y_train, tokenizer, MAX_LEN)\nval_dataset   = MultiLabelDataset(df_val[TEXT_COL].tolist(), y_val, tokenizer, MAX_LEN)\ntest_dataset  = MultiLabelDataset(df_test[TEXT_COL].tolist(), y_test, tokenizer, MAX_LEN)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE)\ntest_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
            "block_group": "e393f42d1f384c04983bf5ead6b2dcfe",
            "execution_count": 17,
            "outputs": [
                {
                    "name": "stderr",
                    "text": "/root/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "dbtable:cell_outputs/b4ab1d64-b456-4262-bd48-1354d4c67b80",
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "23c1e26e",
                "execution_start": 1744247824939,
                "execution_millis": 1,
                "execution_context_id": "11a785d9-7581-4f8f-8acb-f265eb8149b2",
                "cell_id": "43bd3b945d6945dd88e65df63b2bc6b2",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# MODEL CLASS\n# ==========================\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, model_name, num_classes):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(0.3)\n        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0]  # CLS token\n        pooled_output = self.dropout(pooled_output)\n        return self.classifier(pooled_output)\n",
            "block_group": "6aaa0cfc62de4dca9e058c7a16dfa66a",
            "execution_count": 19,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "cdc23198",
                "execution_start": 1744247826580,
                "execution_millis": 1,
                "execution_context_id": "11a785d9-7581-4f8f-8acb-f265eb8149b2",
                "cell_id": "fdee3d1433eb4180abb0aee3cee13491",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# TRAINING UTILS\n# ==========================\n\ndef predict_proba(model, loader, device):\n    model.eval()\n    probs = []\n    with torch.no_grad():\n        for batch in loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            outputs = model(input_ids, attention_mask)\n            probs.extend(torch.sigmoid(outputs).cpu().numpy())\n    return np.array(probs)\n\ndef evaluate_threshold_sweep(y_true, y_pred, thresholds=np.arange(0.1, 0.9, 0.05)):\n    best_thresh = 0.5\n    best_f1 = 0\n    results = []\n\n    for thresh in thresholds:\n        y_pred_bin = (y_pred > thresh).astype(int)\n        macro = f1_score(y_true, y_pred_bin, average='macro', zero_division=0)\n        micro = f1_score(y_true, y_pred_bin, average='micro', zero_division=0)\n        exact = (y_pred_bin == y_true).all(axis=1).mean()\n\n        results.append((thresh, macro, micro, exact))\n        if macro > best_f1:\n            best_f1 = macro\n            best_thresh = thresh\n\n    print(\"Threshold sweep results:\")\n    for t, macro, micro, exact in results:\n        print(f\"Thresh {t:.2f} | Macro F1: {macro:.3f} | Micro F1: {micro:.3f} | Exact Match: {exact:.3f}\")\n\n    print(f\"\\n Best threshold = {best_thresh:.2f} with Macro F1 = {best_f1:.3f}\")\n    return best_thresh\n",
            "block_group": "2b4954ddc86847acba53c843cb1eeb6c",
            "execution_count": 21,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "eb4f1d5",
                "execution_start": 1744247829783,
                "execution_millis": 280983,
                "execution_context_id": "11a785d9-7581-4f8f-8acb-f265eb8149b2",
                "cell_id": "9580acf1af4348f28f21d4c881230184",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# TRAINING LOOP\n# ==========================\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = TransformerClassifier(MODEL_NAME, num_classes).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.BCEWithLogitsLoss()\nbest_macro_f1 = 0.0 \n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"\\nEpoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}\")\n\n    # validation\n    val_probs = predict_proba(model, val_loader, device)\n    threshold = evaluate_threshold_sweep(y_val, val_probs)\n    y_val_pred = (val_probs > threshold).astype(int)\n    macro_f1 = f1_score(y_val, y_val_pred, average=\"macro\", zero_division=0)\n    print(f\"Validation Macro F1 (Epoch {epoch+1}): {macro_f1:.4f}\")\n\n    if macro_f1 > best_macro_f1:\n        best_macro_f1 = macro_f1\n        torch.save(model.state_dict(), MODEL_PATH)\n        print(f\"Saved best model (Epoch {epoch+1}) to {MODEL_PATH}\")\n",
            "block_group": "6088e635ebcb4dd69a28c65254265f5a",
            "execution_count": 24,
            "outputs": [
                {
                    "name": "stderr",
                    "text": "/root/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/root/venv/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n/root/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 192/192 [01:18<00:00,  2.43it/s]\n\nEpoch 1: Loss = 0.2709\nThreshold sweep results:\nThresh 0.10 | Macro F1: 0.241 | Micro F1: 0.402 | Exact Match: 0.000\nThresh 0.15 | Macro F1: 0.237 | Micro F1: 0.456 | Exact Match: 0.008\nThresh 0.20 | Macro F1: 0.219 | Micro F1: 0.491 | Exact Match: 0.089\nThresh 0.25 | Macro F1: 0.225 | Micro F1: 0.505 | Exact Match: 0.189\nThresh 0.30 | Macro F1: 0.201 | Micro F1: 0.488 | Exact Match: 0.197\nThresh 0.35 | Macro F1: 0.191 | Micro F1: 0.456 | Exact Match: 0.194\nThresh 0.40 | Macro F1: 0.143 | Micro F1: 0.366 | Exact Match: 0.173\nThresh 0.45 | Macro F1: 0.099 | Micro F1: 0.287 | Exact Match: 0.121\nThresh 0.50 | Macro F1: 0.073 | Micro F1: 0.216 | Exact Match: 0.100\nThresh 0.55 | Macro F1: 0.052 | Micro F1: 0.153 | Exact Match: 0.089\nThresh 0.60 | Macro F1: 0.040 | Micro F1: 0.119 | Exact Match: 0.087\nThresh 0.65 | Macro F1: 0.036 | Micro F1: 0.107 | Exact Match: 0.087\nThresh 0.70 | Macro F1: 0.034 | Micro F1: 0.093 | Exact Match: 0.073\nThresh 0.75 | Macro F1: 0.035 | Micro F1: 0.091 | Exact Match: 0.071\nThresh 0.80 | Macro F1: 0.036 | Micro F1: 0.089 | Exact Match: 0.071\nThresh 0.85 | Macro F1: 0.026 | Micro F1: 0.056 | Exact Match: 0.047\n\n Best threshold = 0.10 with Macro F1 = 0.241\nValidation Macro F1 (Epoch 1): 0.2406\nSaved best model (Epoch 1) to narrative_classification_STL_UA-CC_to_UA-CC.pt\nEpoch 2: 100%|██████████| 192/192 [01:20<00:00,  2.38it/s]\n\nEpoch 2: Loss = 0.1895\nThreshold sweep results:\nThresh 0.10 | Macro F1: 0.343 | Micro F1: 0.463 | Exact Match: 0.047\nThresh 0.15 | Macro F1: 0.374 | Micro F1: 0.514 | Exact Match: 0.121\nThresh 0.20 | Macro F1: 0.356 | Micro F1: 0.547 | Exact Match: 0.181\nThresh 0.25 | Macro F1: 0.348 | Micro F1: 0.565 | Exact Match: 0.265\nThresh 0.30 | Macro F1: 0.306 | Micro F1: 0.539 | Exact Match: 0.265\nThresh 0.35 | Macro F1: 0.280 | Micro F1: 0.529 | Exact Match: 0.278\nThresh 0.40 | Macro F1: 0.260 | Micro F1: 0.514 | Exact Match: 0.262\nThresh 0.45 | Macro F1: 0.245 | Micro F1: 0.472 | Exact Match: 0.228\nThresh 0.50 | Macro F1: 0.217 | Micro F1: 0.423 | Exact Match: 0.205\nThresh 0.55 | Macro F1: 0.162 | Micro F1: 0.326 | Exact Match: 0.168\nThresh 0.60 | Macro F1: 0.127 | Micro F1: 0.260 | Exact Match: 0.131\nThresh 0.65 | Macro F1: 0.113 | Micro F1: 0.227 | Exact Match: 0.121\nThresh 0.70 | Macro F1: 0.077 | Micro F1: 0.148 | Exact Match: 0.084\nThresh 0.75 | Macro F1: 0.067 | Micro F1: 0.125 | Exact Match: 0.073\nThresh 0.80 | Macro F1: 0.027 | Micro F1: 0.043 | Exact Match: 0.018\nThresh 0.85 | Macro F1: 0.008 | Micro F1: 0.011 | Exact Match: 0.005\n\n Best threshold = 0.15 with Macro F1 = 0.374\nValidation Macro F1 (Epoch 2): 0.3736\nSaved best model (Epoch 2) to narrative_classification_STL_UA-CC_to_UA-CC.pt\nEpoch 3: 100%|██████████| 192/192 [01:24<00:00,  2.29it/s]\n\nEpoch 3: Loss = 0.1607\nThreshold sweep results:\nThresh 0.10 | Macro F1: 0.429 | Micro F1: 0.534 | Exact Match: 0.126\nThresh 0.15 | Macro F1: 0.390 | Micro F1: 0.564 | Exact Match: 0.213\nThresh 0.20 | Macro F1: 0.369 | Micro F1: 0.569 | Exact Match: 0.257\nThresh 0.25 | Macro F1: 0.355 | Micro F1: 0.578 | Exact Match: 0.299\nThresh 0.30 | Macro F1: 0.322 | Micro F1: 0.541 | Exact Match: 0.302\nThresh 0.35 | Macro F1: 0.300 | Micro F1: 0.525 | Exact Match: 0.323\nThresh 0.40 | Macro F1: 0.292 | Micro F1: 0.512 | Exact Match: 0.304\nThresh 0.45 | Macro F1: 0.259 | Micro F1: 0.484 | Exact Match: 0.278\nThresh 0.50 | Macro F1: 0.217 | Micro F1: 0.455 | Exact Match: 0.270\nThresh 0.55 | Macro F1: 0.209 | Micro F1: 0.436 | Exact Match: 0.262\nThresh 0.60 | Macro F1: 0.188 | Micro F1: 0.404 | Exact Match: 0.260\nThresh 0.65 | Macro F1: 0.166 | Micro F1: 0.369 | Exact Match: 0.244\nThresh 0.70 | Macro F1: 0.142 | Micro F1: 0.321 | Exact Match: 0.220\nThresh 0.75 | Macro F1: 0.120 | Micro F1: 0.288 | Exact Match: 0.199\nThresh 0.80 | Macro F1: 0.104 | Micro F1: 0.250 | Exact Match: 0.181\nThresh 0.85 | Macro F1: 0.085 | Micro F1: 0.193 | Exact Match: 0.152\n\n Best threshold = 0.10 with Macro F1 = 0.429\nValidation Macro F1 (Epoch 3): 0.4290\nSaved best model (Epoch 3) to narrative_classification_STL_UA-CC_to_UA-CC.pt\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "s3:deepnote-cell-outputs-production/02a479aa-880e-42ed-b0de-ec6f32b160c1",
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "is_code_hidden": false,
                "deepnote_to_be_reexecuted": true,
                "deepnote_app_is_code_hidden": true,
                "cell_id": "123035d3146b4b9a86ac1efbbc4fe67e",
                "deepnote_cell_type": "code"
            },
            "source": "torch.cuda.empty_cache()",
            "block_group": "5c86f24778cd445194873f12829216a4",
            "execution_count": null,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "a8c2963b",
                "execution_start": 1744248110821,
                "execution_millis": 1,
                "execution_context_id": "11a785d9-7581-4f8f-8acb-f265eb8149b2",
                "cell_id": "4f27e9bf7d50420b846b558699b80a36",
                "deepnote_cell_type": "code"
            },
            "source": "# ==========================\n# FIXED THRESHOLD EVALUATION\n# ==========================\n\ndef evaluate(loader, df_source, mlb, label=\"TEST\", threshold=0.25): \n\n    \"\"\"\n    Evaluates a multi-label classification model using a fixed probability threshold.\n\n    Args:\n        loader (DataLoader): A PyTorch DataLoader yielding batches of tokenised input data\n        df_source (pd.DataFrame): Source dataframe containing metadata for each example, including domain info.\n        mlb (MultiLabelBinarizer): The fitted multi-label binarizer used for encoding and decoding labels.\n        label (str, optional): Label for the dataset (e.g., 'TEST', 'VALIDATION'). Used for logging. Defaults to \"TEST\".\n        threshold (float, optional): Probability threshold to convert predicted probabilities into binary labels. Defaults to 0.25.\n\n    Returns:\n        dict: A dictionary containing overall macro F1, micro F1, exact match score, \n              the threshold used, and the list of labels used after filtering.\n              Also prints per-domain breakdowns of these metrics.\n\n    Notes:\n        - Filters out labels that are completely unseen in both predictions and ground truths \n          to avoid skewed metric calculations.\n        - Performs evaluation on the entire dataset as well as broken down by domain.\n    \"\"\"\n    model.eval()\n    y_true, y_pred, domains = [], [], []\n\n    with torch.no_grad():\n        for i, batch in enumerate(tqdm(loader, desc=f\"Evaluating {label}\")):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].cpu().numpy()\n\n            outputs = model(input_ids, attention_mask)\n            probs = torch.sigmoid(outputs).cpu().numpy()\n\n            y_pred.extend(probs)\n            y_true.extend(labels)\n\n            start = i * loader.batch_size\n            end = start + len(labels)\n            domains.extend(df_source[\"Domain\"].iloc[start:end].tolist())\n\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    domains = np.array(domains)\n\n    y_pred_bin = (y_pred > threshold).astype(int)\n\n    # filter columns where y_true or y_pred has no samples (i.e., unseen label)\n    mask = (y_true.sum(axis=0) + y_pred_bin.sum(axis=0)) > 0\n    y_true = y_true[:, mask]\n    y_pred_bin = y_pred_bin[:, mask]\n    filtered_labels = np.array(mlb.classes_)[mask]\n\n    macro = f1_score(y_true, y_pred_bin, average=\"macro\", zero_division=0)\n    micro = f1_score(y_true, y_pred_bin, average=\"micro\", zero_division=0)\n    exact = (y_pred_bin == y_true).all(axis=1).mean()\n\n    print(f\"\\n {label} (Fixed Threshold={threshold:.2f}):\")\n    print(f\"Macro F1: {macro:.3f}\")\n    print(f\"Micro F1: {micro:.3f}\")\n    print(f\"Exact Match: {exact:.3f}\")\n\n    print(\"\\n----------------------------\")\n    print(\"Per-Domain Breakdown\")\n    print(\"----------------------------\")\n    for domain in np.unique(domains):\n        idx = np.where(domains == domain)[0]\n        y_true_d = y_true[idx]\n        y_pred_d = y_pred_bin[idx]\n\n        macro_d = f1_score(y_true_d, y_pred_d, average=\"macro\", zero_division=0)\n        micro_d = f1_score(y_true_d, y_pred_d, average=\"micro\", zero_division=0)\n        exact_d = (y_pred_d == y_true_d).all(axis=1).mean()\n\n        print(f\"\\n Domain: {domain}\")\n        print(f\"Macro F1: {macro_d:.3f}\")\n        print(f\"Micro F1: {micro_d:.3f}\")\n        print(f\"Exact Match: {exact_d:.3f}\")\n\n    return {\n        \"macro\": macro,\n        \"micro\": micro,\n        \"exact\": exact,\n        \"threshold\": threshold,\n        \"labels_used\": filtered_labels.tolist()\n    }\n\n\ndef evaluate_and_compare_fixed_thresh(val_loader, df_val, test_loader, df_test, mlb, threshold=0.25):\n    print(\"\\n=========================\")\n    print(\"Validation (Fixed Threshold)\")\n    print(\"=========================\")\n    val_results = evaluate(val_loader, df_val.reset_index(drop=True), mlb, label=\"VALIDATION\", threshold=threshold)\n\n    print(\"\\n=========================\")\n    print(\"Test (Fixed Threshold)\")\n    print(\"=========================\")\n    test_results = evaluate(test_loader, df_test.reset_index(drop=True), mlb, label=\"TEST\", threshold=threshold)\n\n    print(\"\\n=========================\")\n    print(\"OOD Generalization (Fixed Threshold)\")\n    print(\"=========================\")\n    macro_drop = val_results[\"macro\"] - test_results[\"macro\"]\n    print(f\"Δ Macro F1 (val - test): {macro_drop:.3f}\")\n\n    return {\n        \"val\": val_results,\n        \"test\": test_results,\n        \"ood_gap_macro\": macro_drop\n    }\n\n",
            "block_group": "4534ee0a07354a7b9f2d3059ed80f04c",
            "execution_count": 25,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "cd92cb6",
                "execution_start": 1744248110871,
                "execution_millis": 10605,
                "execution_context_id": "11a785d9-7581-4f8f-8acb-f265eb8149b2",
                "cell_id": "b0eab2c12aa847a68fed5e3fbe55e419",
                "deepnote_cell_type": "code"
            },
            "source": "results = evaluate_and_compare_fixed_thresh(\n    val_loader, df_val,\n    test_loader, df_test,\n    mlb\n)",
            "block_group": "8da600d15a9a4975941b526e11598804",
            "execution_count": 26,
            "outputs": [
                {
                    "name": "stdout",
                    "text": "\n=========================\nValidation (Fixed Threshold)\n=========================\nEvaluating VALIDATION: 100%|██████████| 48/48 [00:07<00:00,  6.66it/s]\n\n VALIDATION (Fixed Threshold=0.25):\nMacro F1: 0.355\nMicro F1: 0.578\nExact Match: 0.299\n\n----------------------------\nPer-Domain Breakdown\n----------------------------\n\n Domain: CC\nMacro F1: 0.192\nMicro F1: 0.570\nExact Match: 0.460\n\n Domain: UA\nMacro F1: 0.189\nMicro F1: 0.581\nExact Match: 0.231\n\n=========================\nTest (Fixed Threshold)\n=========================\nEvaluating TEST: 100%|██████████| 23/23 [00:03<00:00,  6.74it/s]\n\n TEST (Fixed Threshold=0.25):\nMacro F1: 0.268\nMicro F1: 0.513\nExact Match: 0.247\n\n----------------------------\nPer-Domain Breakdown\n----------------------------\n\n Domain: CC\nMacro F1: 0.229\nMicro F1: 0.592\nExact Match: 0.452\n\n Domain: UA\nMacro F1: 0.150\nMicro F1: 0.473\nExact Match: 0.105\n\n=========================\nOOD Generalization (Fixed Threshold)\n=========================\nΔ Macro F1 (val - test): 0.087\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "s3:deepnote-cell-outputs-production/f29e9d8c-6de1-496d-ae36-2b005c49f53d",
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "9b5d3a0c634a4683ab1716b16a2ce23b",
                "deepnote_cell_type": "markdown"
            },
            "source": "## Zero-Shot Narrative Classification -- experimenting with label embeddings",
            "block_group": "f9d01bae54a9443993d613586e5091ab"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "e63793db",
                "execution_start": 1744248143825,
                "execution_millis": 1416,
                "execution_context_id": "11a785d9-7581-4f8f-8acb-f265eb8149b2",
                "cell_id": "3bdf7cc2bae94555bf18b656b25e0d42",
                "deepnote_cell_type": "code"
            },
            "source": "\n#Zero-Shot Narrative Classification\n\n\nzs_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# Load your fine-tuned model from checkpoint\nfine_tuned_model = AutoModel.from_pretrained(MODEL_NAME)\nstate_dict = torch.load(MODEL_PATH, map_location='cpu')\nfine_tuned_model.load_state_dict(state_dict, strict=False)\nfine_tuned_model.eval()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfine_tuned_model.to(device)\n\n# Use narrative labels from MultiLabelBinarizer\nzs_labels = list(mlb.classes_)\n\n# Encode label names into embeddings\nwith torch.no_grad():\n    zs_label_inputs = zs_tokenizer(zs_labels, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n    zs_label_outputs = fine_tuned_model(**zs_label_inputs)\n    zs_label_embeddings = zs_label_outputs.last_hidden_state[:, 0, :]\n    zs_label_embeddings = F.normalize(zs_label_embeddings, dim=1)\n",
            "block_group": "22b7a73fc24543cbb6b11957fa0414b8",
            "execution_count": 32,
            "outputs": [
                {
                    "name": "stderr",
                    "text": "/root/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "dbtable:cell_outputs/ecb8f85a-c255-47ef-8d2f-9686203e897e",
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "84f020b7",
                "execution_start": 1744248213321,
                "execution_millis": 0,
                "execution_context_id": "11a785d9-7581-4f8f-8acb-f265eb8149b2",
                "cell_id": "638251908bb74a3ab09bd36e9a5bf295",
                "deepnote_cell_type": "code"
            },
            "source": "\n# Zero-shot prediction function\ndef zero_shot_predict(text, top_k=3):\n    with torch.no_grad():\n        input_enc = zs_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n        output = fine_tuned_model(**input_enc)\n        text_embedding = output.last_hidden_state[:, 0, :]\n        text_embedding = F.normalize(text_embedding, dim=1)\n\n        sims = F.cosine_similarity(text_embedding, zs_label_embeddings)\n        topk_indices = torch.topk(sims, k=top_k).indices.tolist()\n        return [(zs_labels[i], sims[i].item()) for i in topk_indices]\n",
            "block_group": "f58fac71fbaf435f92a809c9144cb8eb",
            "execution_count": 38,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "3344d38b",
                "execution_start": 1744248214906,
                "execution_millis": 2460,
                "execution_context_id": "11a785d9-7581-4f8f-8acb-f265eb8149b2",
                "cell_id": "8dc835ee727a4a6faa26f6036ef82ddf",
                "deepnote_cell_type": "code"
            },
            "source": "\n# Run zero-shot predictions on validation or test set\n# You can switch df_val to df_test here\nprint(\"Running zero-shot predictions on test set...\")\n\nzs_results = []\nfor text in df_test[TEXT_COL].tolist():\n    zs_results.append(zero_shot_predict(text, top_k=3))\n\n# Example: print the first 3 predictions\nfor i in range(3):\n    print(f\"Text: {df_test[TEXT_COL].iloc[i][:120]}...\")\n    print(\"Predictions:\", zs_results[i])\n    print()\n",
            "block_group": "ec6f9e084ea54111b16f7dc2f15cf959",
            "execution_count": 40,
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Running zero-shot predictions on test set...\nText: General Milley: Russian military stocks rapidly depleting, soldiers demoralized \n\n“Russia remains isolated. Their milita...\nPredictions: [('URW: Speculating war outcomes', 0.995092511177063), ('URW: Discrediting Ukraine', 0.995059609413147), ('URW: Blaming the war on others rather than the invader', 0.9949306845664978)]\n\nText: Ukrainian nationalism, Ukrainian patriotism will be their downfall. \n\n Ukrainian nationalism, Ukrainian patriotism will ...\nPredictions: [('URW: Russia is the Victim', 0.9955462217330933), ('URW: Discrediting Ukraine', 0.9954404234886169), ('URW: Blaming the war on others rather than the invader', 0.9953905344009399)]\n\nText: Medvedev: Russia Seeks More in Ukraine, 'Probably Should Be Kyiv' \n\n Russian troops will go much further into Ukraine, t...\nPredictions: [('URW: Praise of Russia', 0.9958609342575073), ('URW: Speculating war outcomes', 0.9958573579788208), ('URW: Discrediting Ukraine', 0.9958275556564331)]\n\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "s3:deepnote-cell-outputs-production/e29fd22b-94d9-4d4c-9a96-de4e01087696",
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "cell_id": "7f093f113f0e422e8456f67b028c40d0",
                "deepnote_cell_type": "code"
            },
            "source": "",
            "block_group": "bfcdd1dc961541e1be48c9f2b3e2d725",
            "execution_count": null,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=49d39932-ba1f-4621-a036-ab99ade88496' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
            "metadata": {
                "created_in_deepnote_cell": true,
                "deepnote_cell_type": "markdown"
            }
        }
    ],
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "deepnote_notebook_id": "da639e09b0a54dbc97dbecd5b8777afe"
    }
}